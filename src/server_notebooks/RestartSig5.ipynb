{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import setGPU\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "160\n",
      "240\n",
      "320\n",
      "96\n",
      "192\n",
      "288\n",
      "384\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 64) 9408        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 64)   256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 64)   4096        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   9216        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64, 64, 80)   0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 80)   320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   5120        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   4096        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   9216        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 80)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 80)   320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 80)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 64)   5120        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 64)   4096        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 16)   9216        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 80)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 80)   320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 80)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   5120        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 64)     4096        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 16)     9216        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 80)     0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 80)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     5120        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 64)     0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4, 4, 64)     256         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4, 4, 64)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 4, 4, 64)     4096        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 64)     256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 16)     9216        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 80)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4, 4, 80)     0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     5120        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 80)     0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 144)    0           conv2d_16[0][0]                  \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 144)    576         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 144)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     9216        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 64)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 80)   320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 16)     9216        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 80)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 8, 160)    0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 64)   5120        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 160)  0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 224)  0           conv2d_12[0][0]                  \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 224)  896         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 224)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 64)   14336       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 64)   256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 80)   320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 16)   9216        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 80)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 240)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 64)   5120        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 240)  0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 304)  0           conv2d_8[0][0]                   \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 32, 304)  1216        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 32, 304)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 64)   19456       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 64)   256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 80)   320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 16)   9216        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 320)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   5120        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 320)  0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 64, 64, 384)  0           conv2d_4[0][0]                   \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 384)  1536        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 384)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 64)   24576       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 64)   256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 64)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 16)   9216        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 64, 64, 400)  0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64, 64, 464)  0           max_pooling2d[0][0]              \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 64, 64, 464)  1856        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 64, 64, 464)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 64)   29696       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 64, 64, 80)   0           conv2d_31[0][0]                  \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64, 64, 80)   320         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64, 64, 80)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 64, 64, 64)   5120        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 64, 64, 64)   256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 64, 64, 64)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 64, 64, 16)   9216        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 64, 64, 96)   0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 64, 64, 96)   384         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 64, 64, 96)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 64)   6144        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 32, 32, 80)   0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 32, 80)   320         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 80)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 64)   5120        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 64)   256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 64)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 16)   9216        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 32, 32, 96)   0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 96)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 64)   6144        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 80)   0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 80)   320         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16, 16, 80)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 64)   5120        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 64)   256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 64)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 16)   9216        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 96)   0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 96)   384         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 96)   0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 64)   6144        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 80)     0           max_pooling2d_7[0][0]            \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 80)     320         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 80)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 64)     5120        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 64)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 16)     9216        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 96)     0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 96)     384         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 96)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 64)     6144        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 64)     0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 4, 4, 80)     0           max_pooling2d_8[0][0]            \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 80)     320         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 80)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 64)     5120        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 64)     256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 64)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 96)     384         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 16)     9216        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 96)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 4, 4, 96)     0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 64)     6144        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 8, 8, 96)     0           concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 176)    0           conv2d_47[0][0]                  \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 8, 8, 176)    704         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 8, 8, 176)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 8, 64)     11264       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 64)     256         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 8, 8, 64)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 96)   384         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 8, 8, 16)     9216        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 96)   0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 192)    0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 64)   6144        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 192)  0           concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 16, 16, 272)  0           conv2d_43[0][0]                  \n",
      "                                                                 up_sampling2d_5[0][0]            \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16, 16, 272)  1088        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 16, 272)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 16, 16, 64)   17408       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 64)   256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 64)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 16)   9216        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 96)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 288)  0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 64)   6144        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 288)  0           concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 32, 32, 368)  0           conv2d_39[0][0]                  \n",
      "                                                                 up_sampling2d_6[0][0]            \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 32, 368)  1472        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 32, 32, 368)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 32, 64)   23552       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 64)   256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 64)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 64, 64, 96)   384         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 16)   9216        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 64, 64, 96)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 32, 32, 384)  0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 64, 64)   6144        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 384)  0           concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 64, 64, 464)  0           conv2d_35[0][0]                  \n",
      "                                                                 up_sampling2d_7[0][0]            \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 64, 64, 464)  1856        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 64, 64, 464)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 64, 64, 64)   29696       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 64, 64, 64)   256         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 64, 64, 64)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 64, 64, 16)   9216        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 64, 64, 480)  0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 64, 64, 480)  1920        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 64, 64, 480)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 64, 64, 16)   7680        activation_62[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 524,864\n",
      "Trainable params: 511,616\n",
      "Non-trainable params: 13,248\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Definizione Modello\n",
    "def getUnit1(layerPrec, filters, kernel_size = (1, 1), activation='relu', kernel_initializer='he_normal'):\n",
    "  l = BatchNormalization()(layerPrec)\n",
    "  l = Activation('relu')(l)\n",
    "  l = Conv2D(filters, kernel_size, kernel_initializer=kernel_initializer, padding='same',use_bias=False)(l)\n",
    "  return l\n",
    "\n",
    "def getUnit2(layerPrec, filters, kernel_size = (3, 3), activation='relu', kernel_initializer='he_normal'):\n",
    "  l = BatchNormalization()(layerPrec)\n",
    "  l = Activation('relu')(l)\n",
    "  l = Conv2D(filters, kernel_size, kernel_initializer=kernel_initializer, padding='same',use_bias=False)(l)\n",
    "  return l\n",
    "\n",
    "def getDownBlock(layerPrec,m,n,indexBlock):\n",
    "  l = getUnit1(layerPrec,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][f\"down{indexBlock}\"].append(l)\n",
    "\n",
    "  concat = Concatenate()([layerPrec,l])\n",
    "  l = getUnit1(concat,m)\n",
    "  maxPooling = MaxPool2D(padding='same')(l)\n",
    "  return maxPooling, getUnit1(concat,m)\n",
    "\n",
    "def getUpBlock(layerPrec,skipConn,m,n,indexBlock,upLayers=[]):\n",
    "  l = getUnit1(layerPrec,m)\n",
    "  l = UpSampling2D()(layerPrec)\n",
    "  concat = Concatenate()([skipConn,l]+upLayers)\n",
    "  l = getUnit1(concat,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][f\"up{indexBlock}\"].append(l)\n",
    "  concat = Concatenate()([concat,l])\n",
    "  return concat\n",
    "\n",
    "def getUNet(input,m,n,indexUNet,nUNet, nBlocks):\n",
    "  layerPrec = input\n",
    "  listSkipConn = []\n",
    "\n",
    "  if indexUNet != 0:\n",
    "    layerPrec = nets[f\"unet{indexUNet-1}\"]\n",
    "    #layerPrec = Concatenate()([input,layerPrec]) #l'abbiamo fatto giù con l'if dopo l'up\n",
    "    layerPrec = getUnit1(layerPrec,m)\n",
    "\n",
    "  # down\n",
    "  for i in range(nBlocks):\n",
    "    if nets[\"layers\"][f\"down{i}\"]:\n",
    "        layerPrec = Concatenate()([layerPrec]+nets[\"layers\"][f\"down{i}\"])\n",
    "    layerPrec, skipConn = getDownBlock(layerPrec,m,n,i)\n",
    "    listSkipConn.append(skipConn)\n",
    "\n",
    "  # bottle neck\n",
    "  if nets[\"layers\"][f\"bn\"]:\n",
    "      layerPrec = Concatenate()([layerPrec]+nets[\"layers\"][f\"bn\"])\n",
    "\n",
    "  l = getUnit1(layerPrec,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][\"bn\"].append(l)\n",
    "  concat = Concatenate()([layerPrec,l])\n",
    "\n",
    "  # up\n",
    "  layerPrec = concat\n",
    "  for i in range(nBlocks):\n",
    "    layerPrec = getUpBlock(layerPrec,listSkipConn[-(i+1)],m,n,i,upLayers=nets[\"layers\"][f\"up{i}\"])\n",
    "\n",
    "\n",
    "  if indexUNet != nUNet - 1:\n",
    "    l = Concatenate()([input,layerPrec])\n",
    "  else:\n",
    "    l = getUnit1(layerPrec,16,activation=\"sigmoid\") #era linear\n",
    "\n",
    "  return l\n",
    "\n",
    "def trasformationInput(x, filters):\n",
    "  #x = BatchNormalization()(x)\n",
    "  #x = Activation('relu')(x)\n",
    "  #x = Conv2D(filters, kernel_size=(7,7), strides=(2,2), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x)\n",
    "  #maxPooling = MaxPool2D(padding='same')(x)\n",
    "  \n",
    "  #x = Conv2D(filters*2, kernel_size=(5,5), strides=(1,1), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x)\n",
    "  #x = BatchNormalization()(x)\n",
    "  #x = Activation('relu')(x)\n",
    "  x = Conv2D(filters, kernel_size=(7,7), strides=(1,1), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x) #era 3x3\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPool2D(padding='same')(x)\n",
    "\n",
    "  return x#maxPooling\n",
    "\n",
    "def getCUNet(shape,m,n,nUNet,nBlocks):\n",
    "  for i in range(nUNet):\n",
    "    nets[f\"unet{i}\"] = None\n",
    "\n",
    "  for j in range(nBlocks):\n",
    "    nets[\"layers\"][f\"down{j}\"] = []\n",
    "    nets[\"layers\"][f\"up{j}\"] = []\n",
    "    \n",
    "  input = Input(shape=shape)\n",
    "  \n",
    "  t_input = trasformationInput(input,m) # per le heatmap da 64x64\n",
    "\n",
    "  for i in range(nUNet):\n",
    "    nets[f\"unet{i}\"] = getUNet(t_input,m,n,i,nUNet,nBlocks)\n",
    "\n",
    "  output = nets[f\"unet{nUNet-1}\"]\n",
    "  return Model(inputs=input, outputs=output)\n",
    "\n",
    "nets = {}\n",
    "nets[\"layers\"] = {}\n",
    "nets[\"layers\"][\"bn\"] = []\n",
    "shape = (128,128,3)\n",
    "m = 64\n",
    "n = 16\n",
    "nUNet = 2\n",
    "nBlocks = 4\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "net = getCUNet(shape,m,n,nUNet,nBlocks)\n",
    "\n",
    "layers = net.layers\n",
    "#print(layers[57].input)\n",
    "for i in range(len(layers)):\n",
    "    if isinstance(layers[i], tf.python.keras.layers.convolutional.Conv2D):\n",
    "        if isinstance(layers[i-1].input,list):\n",
    "            in_chan = 0\n",
    "            for k in layers[i-1].input:\n",
    "                in_chan = in_chan + k.shape[-1]\n",
    "            print(in_chan)\n",
    "        else:\n",
    "            in_chan = layers[i-1].input.shape[-1]\n",
    "            \n",
    "        n1 = layers[i].kernel_size[0] * layers[i].kernel_size[1] * in_chan\n",
    "        stdv = 1/math.sqrt(n1)\n",
    "        layers[i].kernel_initializer = tf.keras.initializers.RandomUniform(minval=-stdv, maxval=stdv)\n",
    "\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_preds(heatmaps):\n",
    "    \"\"\"Get keypoint predictions from score maps.\n",
    "    Note:\n",
    "        batch_size: N\n",
    "        num_keypoints: K\n",
    "        heatmap height: H\n",
    "        heatmap width: W\n",
    "    Args:\n",
    "        heatmaps (np.ndarray[N, K, H, W]): model predicted heatmaps.\n",
    "    Returns:\n",
    "        tuple: A tuple containing aggregated results.\n",
    "        - preds (np.ndarray[N, K, 2]): Predicted keypoint location.  \n",
    "        - maxvals (np.ndarray[N, K, 1]): Scores (confidence) of the keypoints. non lo restituiamo\n",
    "    \"\"\"\n",
    "    #assert isinstance(heatmaps, np.ndarray), ('heatmaps should be numpy.ndarray')\n",
    "    #assert heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n",
    "\n",
    "    N, K, _, W = heatmaps.shape\n",
    "    heatmaps_reshaped = heatmaps.reshape((N, K, -1))\n",
    "    idx = np.argmax(heatmaps_reshaped, 2).reshape((N, K, 1))\n",
    "    maxvals = np.amax(heatmaps_reshaped, 2).reshape((N, K, 1))\n",
    "\n",
    "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n",
    "    preds[:, :, 0] = preds[:, :, 0] % W\n",
    "    preds[:, :, 1] = preds[:, :, 1] // W\n",
    "\n",
    "    preds = np.where(np.tile(maxvals, (1, 1, 2)) > 0.0, preds, -1)\n",
    "    return preds#, maxvals\n",
    "\n",
    "def calc_dists(preds, target, normalize, use_zero=False):\n",
    "    preds = preds.astype(np.float32)\n",
    "    target = target.astype(np.float32)\n",
    "    normalize = normalize.astype(np.float32)\n",
    "    dists = np.zeros((preds.shape[1], preds.shape[0]))\n",
    "    if use_zero:\n",
    "        boundary = 0\n",
    "    else:\n",
    "        boundary = 1\n",
    "    for n in range(preds.shape[0]):\n",
    "        for c in range(preds.shape[1]):\n",
    "            if target[n,c,0] > boundary and target[n, c, 1] > boundary:\n",
    "                dists[c, n] = np.linalg.norm((preds[n,c,:]- target[n,c,:])/normalize[n]) # axis ricavato da solo\n",
    "            else:\n",
    "                dists[c, n] = -1\n",
    "    return dists\n",
    "\n",
    "def dist_acc(dists, thr=0.5):\n",
    "\n",
    "  #Return percentage below threshold while ignoring values with a -1\n",
    "\n",
    "  if (dists!=-1).sum() > 0:\n",
    "\n",
    "    return ((dists<=thr) == (dists!=-1)).sum().astype(np.float32) / (dists!=-1).sum().astype(np.float32)\n",
    "\n",
    "  else:\n",
    "\n",
    "    return -1\n",
    "\n",
    "def accuracy(output, target, thr=0.5):\n",
    "  ''' Calculate accuracy according to PCK, but uses ground truth heatmap rather than x,y locations\n",
    "        First value to be returned is average accuracy across 'idxs', followed by individual accuracies\n",
    "    '''\n",
    "  #output = output.numpy()\n",
    "  #print(output.__class__)\n",
    "  #target = target.numpy()\n",
    "\n",
    "  idkp = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "  preds = get_max_preds(output)\n",
    "  gts = get_max_preds(target)\n",
    "  norm = np.ones(preds.shape[0])*output.shape[3]/10\n",
    "\n",
    "  dists = calc_dists(preds, gts, norm)\n",
    "\n",
    "  acc = np.zeros(len(idkp)+1)\n",
    "  avg_acc = 0\n",
    "  cnt = 0\n",
    "\n",
    "  for i in range(len(idkp)):\n",
    "    acc[i+1] = dist_acc(dists[idkp[i]])\n",
    "    if acc[i+1] >= 0: \n",
    "      avg_acc = avg_acc + acc[i+1]\n",
    "      cnt += 1\n",
    "            \n",
    "  if cnt != 0:  \n",
    "    acc[0] = avg_acc / cnt\n",
    "\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import cv2\n",
    "\n",
    "# =============================================================================\n",
    "# General image processing functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_transform(center, scale, res, rot=0):\n",
    "    # Generate transformation matrix\n",
    "    h = 200 * scale\n",
    "    t = np.zeros((3, 3))\n",
    "    t[0, 0] = float(res[1]) / h\n",
    "    t[1, 1] = float(res[0]) / h\n",
    "    t[0, 2] = res[1] * (-float(center[0]) / h + .5)\n",
    "    t[1, 2] = res[0] * (-float(center[1]) / h + .5)\n",
    "    t[2, 2] = 1\n",
    "    if not rot == 0:\n",
    "        rot = -rot # To match direction of rotation from cropping\n",
    "        rot_mat = np.zeros((3,3))\n",
    "        rot_rad = rot * np.pi / 180\n",
    "        sn,cs = np.sin(rot_rad), np.cos(rot_rad)\n",
    "        rot_mat[0,:2] = [cs, -sn]\n",
    "        rot_mat[1,:2] = [sn, cs]\n",
    "        rot_mat[2,2] = 1\n",
    "        # Need to rotate around center\n",
    "        t_mat = np.eye(3)\n",
    "        t_mat[0,2] = -res[1]/2\n",
    "        t_mat[1,2] = -res[0]/2\n",
    "        t_inv = t_mat.copy()\n",
    "        t_inv[:2,2] *= -1\n",
    "        t = np.dot(t_inv,np.dot(rot_mat,np.dot(t_mat,t)))\n",
    "    return t\n",
    "\n",
    "def transform(pt, center, scale, res, invert=0, rot=0):\n",
    "    # Transform pixel location to different reference\n",
    "    t = get_transform(center, scale, res, rot=rot)\n",
    "    if invert:\n",
    "        t = np.linalg.inv(t)\n",
    "    new_pt = np.array([pt[0], pt[1], 1.]).T\n",
    "    new_pt = np.dot(t, new_pt)\n",
    "    return new_pt[:2].astype(int)\n",
    "\n",
    "def crop(img, center, scale, res, rot=0):\n",
    "    # Upper left point\n",
    "    ul = np.array(transform([0, 0], center, scale, res, invert=1))\n",
    "    # Bottom right point\n",
    "    br = np.array(transform(res, center, scale, res, invert=1))\n",
    "\n",
    "    new_shape = [br[1] - ul[1], br[0] - ul[0]]\n",
    "    if len(img.shape) > 2:\n",
    "        new_shape += [img.shape[2]]\n",
    "    new_img = np.zeros(new_shape)\n",
    "\n",
    "    # Range to fill new array\n",
    "    new_x = max(0, -ul[0]), min(br[0], len(img[0])) - ul[0]\n",
    "    new_y = max(0, -ul[1]), min(br[1], len(img)) - ul[1]\n",
    "    # Range to sample from original image\n",
    "    old_x = max(0, ul[0]), min(len(img[0]), br[0])\n",
    "    old_y = max(0, ul[1]), min(len(img), br[1])\n",
    "    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], old_x[0]:old_x[1]]\n",
    "\n",
    "    return cv2.resize(new_img, res)\n",
    "\n",
    "def inv_mat(mat):\n",
    "    ans = np.linalg.pinv(np.array(mat).tolist() + [[0,0,1]])\n",
    "    return ans[:2]\n",
    "\n",
    "def kpt_affine(kpt, mat):\n",
    "    kpt = np.array(kpt)\n",
    "    shape = kpt.shape\n",
    "    kpt = kpt.reshape(-1, 2)\n",
    "    return np.dot( np.concatenate((kpt, kpt[:, 0:1]*0+1), axis = 1), mat.T ).reshape(shape)\n",
    "\n",
    "\n",
    "def resize(im, res):\n",
    "    return np.array([cv2.resize(im[i],res) for i in range(im.shape[0])])\n",
    "\n",
    "def generateHeatmap(keypoints,output_res,num_parts):\n",
    "    #Init\n",
    "    sigma = output_res/64\n",
    "    size = 6*sigma+3\n",
    "    x = np.arange(0, size, 1, float) #crea un array composto da nove 1.0 vettore 1-D (9,)\n",
    "    y = x[:, np.newaxis] #Vettore 2-D (9,1)\n",
    "    x0, y0 = 3*sigma + 1, 3*sigma + 1 #vedi giù a tutto\n",
    "    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
    "    #Generation\n",
    "    hms = np.zeros(shape = (num_parts, output_res, output_res), dtype = np.float32) #crea vettore (16,64,64), cioè 16 heatmaps nere\n",
    "    for p in keypoints:\n",
    "        for idx, pt in enumerate(p): #ottiene id + [x,y] di ogni keypoint\n",
    "            if pt[0] > 0: \n",
    "                x, y = int(pt[0]), int(pt[1])\n",
    "                if x<0 or y<0 or x>=output_res or y>=output_res: #se succede questo, rimane heatmap idx-esima  tutta a 0\n",
    "                    continue\n",
    "                ul = int(x - 3*sigma - 1), int(y - 3*sigma - 1)\n",
    "                br = int(x + 3*sigma + 2), int(y + 3*sigma + 2)\n",
    "\n",
    "                c,d = max(0, -ul[0]), min(br[0], output_res) - ul[0]\n",
    "                a,b = max(0, -ul[1]), min(br[1], output_res) - ul[1]\n",
    "\n",
    "                cc,dd = max(0, ul[0]), min(br[0], output_res)\n",
    "                aa,bb = max(0, ul[1]), min(br[1], output_res)\n",
    "                hms[idx, aa:bb,cc:dd] = np.maximum(hms[idx, aa:bb,cc:dd], g[a:b,c:d])\n",
    "    return hms\n",
    "def getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    cropped = crop(img,c,s,inp_res)\n",
    "    orig_keypoints = []\n",
    "    for i in keypoints:\n",
    "        orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "    orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "    kptmp = np.copy(orig_keypoints)\n",
    "    for i in range(orig_keypoints.shape[1]):\n",
    "        if orig_keypoints[0,i,0] > 0:\n",
    "            orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "    keypoints = np.copy(orig_keypoints)\n",
    "    h, w = cropped.shape[0:2]\n",
    "    center = np.array((w/2,h/2))\n",
    "    scale = max(h,w)/200\n",
    "    aug_rot = (np.random.random()*2-1)*30\n",
    "    aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "    scale *= aug_scale\n",
    "\n",
    "    mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "    mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "    inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "    keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "    \n",
    "    #print('\\n')\n",
    "\n",
    "\n",
    "    #Flip 50% probability\n",
    "    if np.random.randint(2) == 0:\n",
    "        inp = inp[:, ::-1]\n",
    "        keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "        keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "        orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "        orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "    ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "        for i in range(np.shape(orig_keypoints)[1]):\n",
    "            if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                keypoints[0,i,0] = 0\n",
    "                keypoints[0,i,1] = 0\n",
    "                orig_keypoints[0,i,0] = 0\n",
    "                orig_keypoints[0,i,1] = 0\n",
    "\n",
    "    heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "    return inp,heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=2.5e-3, rho=0.99, momentum=0.0, epsilon=1e-08\n",
    ")\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=6.7e-3)\n",
    "\n",
    "\n",
    "def heatmapLoss(y_true,y_pred):\n",
    "    l = tf.math.square((y_pred - y_true))\n",
    "    l = tf.reduce_mean(l,axis=3)\n",
    "    l = tf.reduce_mean(l,axis=2)\n",
    "    l = tf.reduce_mean(l,axis=1)\n",
    "    return l\n",
    "\n",
    "def euclidean_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_true - y_pred)))\n",
    "\n",
    "def dice(y_true,y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice(y_true, y_pred)\n",
    "\n",
    "def weighted_mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    apply weights on heatmap mse loss to only pick valid keypoint heatmap\n",
    "    since y_true would be gt_heatmap with shape\n",
    "    (batch_size, heatmap_size[0], heatmap_size[1], num_keypoints)\n",
    "    we sum up the heatmap for each keypoints and check. Sum for invalid\n",
    "    keypoint would be 0, so we can get a keypoint weights tensor with shape\n",
    "    (batch_size, 1, 1, num_keypoints)\n",
    "    and multiply to loss\n",
    "    \"\"\"\n",
    "    heatmap_sum = K.sum(K.sum(y_true, axis=1, keepdims=True), axis=2, keepdims=True)\n",
    "\n",
    "    # keypoint_weights shape: (batch_size, 1, 1, num_keypoints), with\n",
    "    # valid_keypoint = 1.0, invalid_keypoint = 0.0\n",
    "    keypoint_weights = 1.0 - K.cast(K.equal(heatmap_sum, 0.0), 'float32')\n",
    "\n",
    "    return K.sqrt(K.mean(K.square((y_true - y_pred) * keypoint_weights)))\n",
    "\n",
    "def smooth_l1_loss(y_true, y_pred):\n",
    "    diff = K.abs(y_true - y_pred)\n",
    "    less_than_one = K.cast(K.less(diff, 1.0), 'float32')\n",
    "    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "parts = {'mpii':['rank', 'rkne', 'rhip',\n",
    "                 'lhip', 'lkne', 'lank',\n",
    "                 'pelv', 'thrx', 'neck', 'head',\n",
    "                 'rwri', 'relb', 'rsho',\n",
    "                 'lsho', 'lelb', 'lwri']}\n",
    "\n",
    "flipped_parts = {'mpii':[5, 4, 3, 2, 1, 0, 6, 7, 8, 9, 15, 14, 13, 12, 11, 10]}\n",
    "\n",
    "part_pairs = {'mpii':[[0, 5], [1, 4], [2, 3], [6], [7], [8], [9], [10, 15], [11, 14], [12, 13]]}\n",
    "\n",
    "pair_names = {'mpii':['ankle', 'knee', 'hip', 'pelvis', 'thorax', 'neck', 'head', 'wrist', 'elbow', 'shoulder']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imageio\n",
    "\n",
    "class Dataset(tf.keras.utils.Sequence):\n",
    "    #Costruttore\n",
    "    def __init__(self,json_path,batch_size,input_shape=(256,256,3),output_shape=(64,64,16),train=True,dataset_name=\"MPII\"):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.batch_size = batch_size\n",
    "        f = open(json_path)\n",
    "        self.dataset = json.load(f)\n",
    "        if dataset_name == \"MPII\":\n",
    "            self.dataset = self.dataset[\"dataset\"][\"MPII\"][\"people\"]\n",
    "        if train:\n",
    "            self.dataset = self.dataset[:-7221]\n",
    "        self.batch_images = np.zeros(shape=((self.batch_size,)+(self.input_shape)), dtype=np.float32)\n",
    "        self.batch_heatmaps = np.zeros(shape=((self.batch_size,)+(self.output_shape)), dtype=np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        for n, annotation in enumerate(self.dataset[i*self.batch_size:(i+1)*self.batch_size]):\n",
    "            #sample_index = i*self.batch_size + n\n",
    "            image, gt_heatmap = self.extract_imgs_hms(annotation)\n",
    "            gt_heatmap = np.transpose(gt_heatmap,(1,2,0))\n",
    "            self.batch_images[n,:,:,:] = image\n",
    "            self.batch_heatmaps[n, :, :, :] = gt_heatmap#gt_heatmap.reshape((self.output_shape))\n",
    "            \n",
    "        return self.batch_images, self.batch_heatmaps\n",
    "#getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    \n",
    "    def extract_imgs_hms(self,annotation):\n",
    "        inp_res = self.input_shape[0:2]\n",
    "        out_res = self.output_shape[0:2]\n",
    "        img = imageio.imread(f\"../images_mpii/{annotation['filepath']}\")\n",
    "        c = [annotation[\"objpos\"][\"x\"],annotation[\"objpos\"][\"y\"]]\n",
    "        s = annotation[\"scale\"]\n",
    "        keypoints = annotation[\"keypoints\"]\n",
    "        cropped = crop(img,c,s,inp_res)\n",
    "        \n",
    "        orig_keypoints = []\n",
    "        for i in keypoints:\n",
    "            orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "        orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "        kptmp = np.copy(orig_keypoints)\n",
    "        for i in range(orig_keypoints.shape[1]):\n",
    "            if orig_keypoints[0,i,0] > 0:\n",
    "                orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "        keypoints = np.copy(orig_keypoints)\n",
    "        h, w = cropped.shape[0:2]\n",
    "        center = np.array((w/2,h/2))\n",
    "        scale = max(h,w)/200\n",
    "        aug_rot = (np.random.random()*2-1)*30\n",
    "        aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "        scale *= aug_scale\n",
    "\n",
    "        mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "        mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "        inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "        keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "\n",
    "        #print('\\n')\n",
    "\n",
    "\n",
    "        #Flip 50% probability\n",
    "        if np.random.randint(2) == 0:\n",
    "            inp = inp[:, ::-1]\n",
    "            keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "            keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "            orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "            orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "        ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "            for i in range(np.shape(orig_keypoints)[1]):\n",
    "                if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                    keypoints[0,i,0] = 0\n",
    "                    keypoints[0,i,1] = 0\n",
    "                    orig_keypoints[0,i,0] = 0\n",
    "                    orig_keypoints[0,i,1] = 0\n",
    "\n",
    "        heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "        return inp,heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imageio\n",
    "\n",
    "class Dataset2(tf.keras.utils.Sequence):\n",
    "    #Costruttore\n",
    "    def __init__(self,json_path,batch_size,input_shape=(256,256,3),output_shape=(64,64,16),train=True,dataset_name=\"MPII\"):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.images = np.load(\"../imgs_mpii.npz\")['arr_0']\n",
    "        self.hms = np.load(\"../hms_mpii.npz\")['arr_0']\n",
    "        self.hms = self.hms.reshape((self.hms.shape[0],64,64,16))\n",
    "        if dataset_name == \"MPII\":\n",
    "            self.images = self.images[\"dataset\"][\"MPII\"][\"people\"]\n",
    "        if train:\n",
    "            self.images = self.images[:-8665]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.images[i*self.batch_size:(i+1)*self.batch_size], self.hms[i*self.batch_size:(i+1)*self.batch_size]\n",
    "#getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    \n",
    "    def extract_imgs_hms(self,annotation):\n",
    "        inp_res = self.input_shape[0:2]\n",
    "        out_res = self.output_shape[0:2]\n",
    "        img = imageio.imread(f\"../images_mpii/{annotation['filepath']}\")\n",
    "        c = [annotation[\"objpos\"][\"x\"],annotation[\"objpos\"][\"y\"]]\n",
    "        s = annotation[\"scale\"]\n",
    "        keypoints = annotation[\"keypoints\"]\n",
    "        cropped = crop(img,c,s,inp_res)\n",
    "        \n",
    "        orig_keypoints = []\n",
    "        for i in keypoints:\n",
    "            orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "        orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "        kptmp = np.copy(orig_keypoints)\n",
    "        for i in range(orig_keypoints.shape[1]):\n",
    "            if orig_keypoints[0,i,0] > 0:\n",
    "                orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "        keypoints = np.copy(orig_keypoints)\n",
    "        h, w = cropped.shape[0:2]\n",
    "        center = np.array((w/2,h/2))\n",
    "        scale = max(h,w)/200\n",
    "        aug_rot = (np.random.random()*2-1)*30\n",
    "        aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "        scale *= aug_scale\n",
    "\n",
    "        mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "        mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "        inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "        keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "\n",
    "        #print('\\n')\n",
    "\n",
    "\n",
    "        #Flip 50% probability\n",
    "        if np.random.randint(2) == 0:\n",
    "            inp = inp[:, ::-1]\n",
    "            keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "            keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "            orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "            orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "        ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "            for i in range(np.shape(orig_keypoints)[1]):\n",
    "                if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                    keypoints[0,i,0] = 0\n",
    "                    keypoints[0,i,1] = 0\n",
    "                    orig_keypoints[0,i,0] = 0\n",
    "                    orig_keypoints[0,i,1] = 0\n",
    "\n",
    "        heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "        return inp,heatmaps\n",
    "####################################################################àà\n",
    "\n",
    "\n",
    "def get_normalize(input_shape):\n",
    "    \"\"\"\n",
    "    rescale keypoint distance normalize coefficient\n",
    "    based on input shape, used for PCK evaluation\n",
    "    NOTE: 6.4 is standard normalize coefficient under\n",
    "          input shape (256,256)\n",
    "    # Arguments\n",
    "        input_shape: input image shape as (height, width)\n",
    "    # Returns\n",
    "        scale: normalize coefficient\n",
    "    \"\"\"\n",
    "    #assert input_shape[0] == input_shape[1], 'only support square input shape.'\n",
    "\n",
    "    # use averaged scale factor for non square input shape\n",
    "    scale = float((input_shape[0] + input_shape[1]) / 2) / 256.0\n",
    "\n",
    "    return 6.4*scale\n",
    "\n",
    "global_best_acc = 0.0\n",
    "\n",
    "class EvalCallBack(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model_input_shape, fileName, imgs,hms):\n",
    "        self.normalize = get_normalize(model_input_shape)\n",
    "        self.model_input_shape = model_input_shape\n",
    "        self.best_acc = 0.0\n",
    "        self.fileName = fileName\n",
    "        self.eval_images = imgs#np.load(\"../imgs_val_mpii128.npz\")['arr_0']\n",
    "        self.eval_hms = hms#np.load(\"../hms_val_mpii128.npz\")['arr_0']\n",
    "        self.listAcc = []\n",
    "        #self.eval_hms = np.reshape(self.eval_hms,(self.eval_hms.shape[0],64,64,16))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global global_best_acc\n",
    "        output = self.model.predict(self.eval_images)\n",
    "        #output = output.reshape( (output.shape[0],)+(16,64,64) )\n",
    "        output = np.transpose(output,(0,3,1,2))\n",
    "        val_acc = accuracy(output,self.eval_hms)\n",
    "        self.listAcc.append(val_acc[0])\n",
    "        print('\\nvalidate accuracy:\\n', val_acc, '@epoch', epoch)\n",
    "        f = open(self.fileName, \"a\")\n",
    "        if val_acc[0] > self.best_acc:\n",
    "            # Save best accuray value and model checkpoint\n",
    "            #checkpoint_dir = os.path.join(self.log_dir, 'ep{epoch:03d}-loss{loss:.3f}-val_acc{val_acc:.3f}.h5'.format(epoch=(epoch+1), loss=logs.get('loss'), val_acc=val_acc))\n",
    "            #self.model.save(f\"../modelsave/ep{epoch}_acc{val_acc[0]}.h5\")\n",
    "            self.model.save_weights(f\"../cunet{nUNet}_{m}{n}_best_sigmoidAdamQuasiDef3sx12_restartsig5.h5\")\n",
    "            print('Epoch {epoch:03d}: val_acc improved from {best_acc:.3f} to {val_acc:.3f}'.format(epoch=epoch+1, best_acc=self.best_acc, val_acc=val_acc[0]))# checkpoint_dir=checkpoint_dir))\n",
    "            message = 'Epoch {epoch:03d}: val_acc improved from {best_acc:.3f} to {val_acc:.3f}. Train loss is {loss:.3f}\\n'.format(epoch=epoch+1, best_acc=self.best_acc, val_acc=val_acc[0],loss=logs.get('loss'))\n",
    "            self.best_acc = val_acc[0]\n",
    "            global_best_acc = val_acc[0]\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            message = 'Epoch {epoch:03d}: val_acc did not improve from {best_acc:.3f}. Train loss is {loss:.3f}\\n'.format(epoch=epoch+1, best_acc=self.best_acc,loss=logs.get('loss'))\n",
    "            print('Epoch {epoch:03d}: val_acc did not improve from {best_acc:.3f}'.format(epoch=epoch+1, best_acc=self.best_acc))\n",
    "        f.write(message)\n",
    "        f.close()\n",
    "        if epoch == 150:\n",
    "            np.savez_compressed(f'../cunet{nUNet}_{m}{n}_best_sigmoidAdamQuasiDef3sx12_restartsig5_accs', self.listAcc)\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "class CustomLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience,factor,fileName=None):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.fileName = fileName\n",
    "        self.factor = factor\n",
    "        self.best_acc = 0.0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        global global_best_acc\n",
    "        print(f\"Counter: {self.counter}, Global: {global_best_acc}, MyBest: {self.best_acc}\\n\")\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "    \n",
    "    def schedule(self,epoch,lr):\n",
    "        global global_best_acc\n",
    "        if self.counter == self.patience:\n",
    "            self.counter = 0 \n",
    "            print(\"Epoch %03d: Updating Learning rate.. New value is %f\" % (epoch, lr*self.factor))\n",
    "            message = \"Epoch %03d: Updating Learning rate.. New value is %f\" % (epoch, lr*self.factor)\n",
    "            f = open(self.fileName, \"a\")\n",
    "            f.write(message)\n",
    "            f.close()\n",
    "            return lr*self.factor\n",
    "        if self.best_acc == global_best_acc:\n",
    "            self.counter = self.counter + 1\n",
    "        elif self.best_acc < global_best_acc:\n",
    "            self.counter = 0\n",
    "            self.best_acc = global_best_acc\n",
    "        return lr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finito 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndataset = Dataset(\"datasets.json\",32)\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "with open('../imgs_train_mpii128sx12_genhm.pickle', 'rb') as handle:\n",
    "    train_images = pickle.load(handle)\n",
    "with open('../hms_train_mpii128sx12_genhm.pickle', 'rb') as handle:\n",
    "    train_hms = pickle.load(handle)\n",
    "\n",
    "eval_images = np.load(\"../imgs_val_mpii128sx12_genhm.npz\")['arr_0']\n",
    "eval_hms = np.load(\"../hms_val_mpii128sx12_genhm.npz\")['arr_0']\n",
    "\n",
    "#train_images = np.load(\"../imgs_train_mpii128.npz\")['arr_0']\n",
    "#train_hms = np.load(\"../hms_train_mpii128.npz\")['arr_0']\n",
    "train_hms = np.transpose(train_hms,(0,2,3,1))#np.reshape(train_hms,(train_hms.shape[0],64,64,16))\n",
    "eval_hms2 = np.transpose(eval_hms,(0,2,3,1))\n",
    "print(\"Finito 1\")\n",
    "\"\"\"\n",
    "dataset = Dataset(\"datasets.json\",32)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finito 2\n"
     ]
    }
   ],
   "source": [
    "evalcallback = EvalCallBack((128,128),\"../sigmoidLayersAdamSchedulingQuasiDef3sx12_restartsig5.txt\",eval_images,eval_hms)\n",
    "updateLR = CustomLearningRateScheduler(7,0.2,fileName=\"../sigmoidLayersAdamSchedulingQuasiDef3sx12_restartsig5.txt\")\n",
    "print(\"Finito 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 0, Global: 0.0, MyBest: 0.0\n",
      "\n",
      "Epoch 1/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.1007WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0076s vs `on_test_batch_end` time: 0.0369s). Check your callbacks.\n",
      "\n",
      "validate accuracy:\n",
      " [0.08182707 0.04886821 0.01693494 0.14498195 0.04054249 0.094615\n",
      " 0.03335444 0.         0.22063957 0.08994413 0.09117179 0.03082143\n",
      " 0.10182584 0.08262505 0.15721254 0.09268361 0.06301215] @epoch 0\n",
      "Epoch 001: val_acc improved from 0.000 to 0.082\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.1007 - val_loss: 0.0567\n",
      "Counter: 1, Global: 0.08182707254309207, MyBest: 0.0\n",
      "\n",
      "Epoch 2/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0584\n",
      "validate accuracy:\n",
      " [0.23757552 0.16902898 0.17940979 0.11162455 0.17198096 0.16926689\n",
      " 0.10491873 0.33695024 0.4010613  0.60726255 0.47955498 0.18280786\n",
      " 0.11713483 0.15563606 0.32139373 0.20685297 0.08632382] @epoch 1\n",
      "Epoch 002: val_acc improved from 0.082 to 0.238\n",
      "576/576 [==============================] - 122s 211ms/step - loss: 0.0584 - val_loss: 0.0544\n",
      "Counter: 0, Global: 0.2375755151733756, MyBest: 0.08182707254309207\n",
      "\n",
      "Epoch 3/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0547\n",
      "validate accuracy:\n",
      " [0.36696498 0.2371483  0.21076459 0.37400723 0.31424037 0.22999497\n",
      " 0.22947013 0.46889469 0.68412232 0.69999999 0.5806964  0.23017107\n",
      " 0.20084269 0.44517207 0.44027874 0.29181296 0.23382312] @epoch 2\n",
      "Epoch 003: val_acc improved from 0.238 to 0.367\n",
      "576/576 [==============================] - 122s 212ms/step - loss: 0.0547 - val_loss: 0.0516\n",
      "Counter: 0, Global: 0.366964977234602, MyBest: 0.2375755151733756\n",
      "\n",
      "Epoch 4/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0512\n",
      "validate accuracy:\n",
      " [0.40809114 0.26189974 0.24597585 0.39292419 0.37945461 0.25582957\n",
      " 0.28900146 0.48813656 0.73229998 0.75055867 0.66088718 0.25929591\n",
      " 0.31432584 0.4776369  0.47804877 0.3104901  0.23269285] @epoch 3\n",
      "Epoch 004: val_acc improved from 0.367 to 0.408\n",
      "576/576 [==============================] - 123s 213ms/step - loss: 0.0512 - val_loss: 0.0543\n",
      "Counter: 0, Global: 0.40809113811701536, MyBest: 0.366964977234602\n",
      "\n",
      "Epoch 5/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0484\n",
      "validate accuracy:\n",
      " [0.46205908 0.3044214  0.30063716 0.43162456 0.44005194 0.28652909\n",
      " 0.31602281 0.56119794 0.79290605 0.79944134 0.72229445 0.28658277\n",
      " 0.36348316 0.57377732 0.57351917 0.368628   0.2718282 ] @epoch 4\n",
      "Epoch 005: val_acc improved from 0.408 to 0.462\n",
      "576/576 [==============================] - 122s 212ms/step - loss: 0.0484 - val_loss: 0.0484\n",
      "Counter: 0, Global: 0.4620590843260288, MyBest: 0.40809113811701536\n",
      "\n",
      "Epoch 6/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0468\n",
      "validate accuracy:\n",
      " [0.46769112 0.30251744 0.3138833  0.45602888 0.44798732 0.31337023\n",
      " 0.32362255 0.57595485 0.78019828 0.78254187 0.70177722 0.30750743\n",
      " 0.38623595 0.58367008 0.55693382 0.35893837 0.29189035] @epoch 5\n",
      "Epoch 006: val_acc improved from 0.462 to 0.468\n",
      "576/576 [==============================] - 121s 210ms/step - loss: 0.0468 - val_loss: 0.0483\n",
      "Counter: 0, Global: 0.46769112162292004, MyBest: 0.4620590843260288\n",
      "\n",
      "Epoch 7/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0457\n",
      "validate accuracy:\n",
      " [0.50530467 0.32113391 0.34875923 0.46064982 0.48809695 0.33098474\n",
      " 0.35233271 0.59403938 0.81650609 0.82122904 0.76332897 0.3510533\n",
      " 0.42415729 0.62142956 0.62843204 0.42592332 0.33681831] @epoch 6\n",
      "Epoch 007: val_acc improved from 0.468 to 0.505\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0457 - val_loss: 0.0469\n",
      "Counter: 0, Global: 0.505304666236043, MyBest: 0.46769112162292004\n",
      "\n",
      "Epoch 8/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0449\n",
      "validate accuracy:\n",
      " [0.52472381 0.34969324 0.36049631 0.5039711  0.50887316 0.3682268\n",
      " 0.35908803 0.60011572 0.82851559 0.83673185 0.78269035 0.32800791\n",
      " 0.44241574 0.65445173 0.65491289 0.45372841 0.36366206] @epoch 7\n",
      "Epoch 008: val_acc improved from 0.505 to 0.525\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0449 - val_loss: 0.0463\n",
      "Counter: 0, Global: 0.5247238054871559, MyBest: 0.505304666236043\n",
      "\n",
      "Epoch 9/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0441\n",
      "validate accuracy:\n",
      " [0.55115647 0.37465623 0.3832998  0.53487366 0.54350024 0.38399598\n",
      " 0.3787207  0.63715279 0.84694874 0.85405028 0.80566394 0.37961262\n",
      " 0.4863764  0.67312247 0.68125433 0.47352901 0.38174626] @epoch 8\n",
      "Epoch 009: val_acc improved from 0.525 to 0.551\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0441 - val_loss: 0.0455\n",
      "Counter: 0, Global: 0.5511564668267965, MyBest: 0.5247238054871559\n",
      "\n",
      "Epoch 10/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0435\n",
      "validate accuracy:\n",
      " [0.55719352 0.38354135 0.38849765 0.54209387 0.53700763 0.39540344\n",
      " 0.38674265 0.64380789 0.85434997 0.86466479 0.81028754 0.39530608\n",
      " 0.50182587 0.67312247 0.68306619 0.4797079  0.37567109] @epoch 9\n",
      "Epoch 010: val_acc improved from 0.551 to 0.557\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0435 - val_loss: 0.0454\n",
      "Counter: 0, Global: 0.557193523272872, MyBest: 0.5511564668267965\n",
      "\n",
      "Epoch 11/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0429\n",
      "validate accuracy:\n",
      " [0.5788216  0.39179182 0.42203221 0.56274366 0.56225652 0.42627075\n",
      " 0.4101752  0.66116899 0.86426479 0.87094975 0.82184654 0.42542061\n",
      " 0.51896065 0.69694859 0.68836236 0.51551747 0.42243573] @epoch 10\n",
      "Epoch 011: val_acc improved from 0.557 to 0.579\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0429 - val_loss: 0.0446\n",
      "Counter: 0, Global: 0.5788216032087803, MyBest: 0.557193523272872\n",
      "\n",
      "Epoch 12/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0424\n",
      "validate accuracy:\n",
      " [0.56244399 0.3824836  0.40023473 0.54931408 0.54465443 0.40395907\n",
      " 0.39202026 0.64554399 0.85253453 0.86075419 0.81173241 0.40619257\n",
      " 0.49592698 0.68677723 0.67665505 0.49557647 0.39474428] @epoch 11\n",
      "Epoch 012: val_acc did not improve from 0.579\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0424 - val_loss: 0.0451\n",
      "Counter: 0, Global: 0.5788216032087803, MyBest: 0.5788216032087803\n",
      "\n",
      "Epoch 13/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0419\n",
      "validate accuracy:\n",
      " [0.57901253 0.39792681 0.42354125 0.5689531  0.56139088 0.43147123\n",
      " 0.40637535 0.65219909 0.84792626 0.85544693 0.80277419 0.45030397\n",
      " 0.51910114 0.70544797 0.69602787 0.51313019 0.43218422] @epoch 12\n",
      "Epoch 013: val_acc improved from 0.579 to 0.579\n",
      "576/576 [==============================] - 121s 210ms/step - loss: 0.0419 - val_loss: 0.0449\n",
      "Counter: 1, Global: 0.5790125280618668, MyBest: 0.5788216032087803\n",
      "\n",
      "Epoch 14/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0414\n",
      "validate accuracy:\n",
      " [0.53431267 0.35815528 0.38598257 0.52794224 0.52560955 0.38231841\n",
      " 0.37069875 0.63266784 0.83926827 0.83868712 0.7822569  0.36674678\n",
      " 0.44859549 0.61794621 0.62606269 0.47802275 0.36804181] @epoch 13\n",
      "Epoch 014: val_acc did not improve from 0.579\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0414 - val_loss: 0.0462\n",
      "Counter: 0, Global: 0.5790125280618668, MyBest: 0.5790125280618668\n",
      "\n",
      "Epoch 15/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0411\n",
      "validate accuracy:\n",
      " [0.56485747 0.38481066 0.40459424 0.56028879 0.56167942 0.40731421\n",
      " 0.3983534  0.64525461 0.83061028 0.84511173 0.79251552 0.41863424\n",
      " 0.50688201 0.67214715 0.6832056  0.52380282 0.40251485] @epoch 14\n",
      "Epoch 015: val_acc did not improve from 0.579\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0411 - val_loss: 0.0455\n",
      "Counter: 1, Global: 0.5790125280618668, MyBest: 0.5790125280618668\n",
      "\n",
      "Epoch 16/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0407\n",
      "validate accuracy:\n",
      " [0.59870664 0.41781256 0.44668007 0.58974731 0.59111238 0.4464016\n",
      " 0.42284146 0.68561924 0.86817485 0.8698324  0.82256901 0.45016259\n",
      " 0.55660111 0.71534067 0.71456444 0.54627156 0.43557501] @epoch 15\n",
      "Epoch 016: val_acc improved from 0.579 to 0.599\n",
      "576/576 [==============================] - 121s 210ms/step - loss: 0.0407 - val_loss: 0.0443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 2, Global: 0.5987066403031349, MyBest: 0.5790125280618668\n",
      "\n",
      "Epoch 17/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0403\n",
      "validate accuracy:\n",
      " [0.60019379 0.42860165 0.44584173 0.59465706 0.59803778 0.45898339\n",
      " 0.42600802 0.67751735 0.86510265 0.87472069 0.8318162  0.44535556\n",
      " 0.54269665 0.71492267 0.71344948 0.54444599 0.44094378] @epoch 16\n",
      "Epoch 017: val_acc improved from 0.599 to 0.600\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0403 - val_loss: 0.0442\n",
      "Counter: 0, Global: 0.6001937910914421, MyBest: 0.5987066403031349\n",
      "\n",
      "Epoch 18/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0400\n",
      "validate accuracy:\n",
      " [0.60767911 0.42373598 0.46177062 0.58642602 0.58043575 0.45848012\n",
      " 0.43170783 0.67100692 0.8741796  0.88310057 0.84395319 0.47660115\n",
      " 0.56853932 0.7296921  0.72766548 0.54809719 0.45747387] @epoch 17\n",
      "Epoch 018: val_acc improved from 0.600 to 0.608\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0400 - val_loss: 0.0441\n",
      "Counter: 0, Global: 0.6076791081577539, MyBest: 0.6001937910914421\n",
      "\n",
      "Epoch 19/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0397\n",
      "validate accuracy:\n",
      " [0.50738125 0.36492491 0.33584842 0.47783393 0.44726589 0.3264553\n",
      " 0.32784462 0.65277779 0.85239494 0.86675978 0.82704812 0.36292946\n",
      " 0.40786517 0.56137663 0.55512196 0.39067546 0.36097768] @epoch 18\n",
      "Epoch 019: val_acc did not improve from 0.608\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0397 - val_loss: 0.0469\n",
      "Counter: 0, Global: 0.6076791081577539, MyBest: 0.6076791081577539\n",
      "\n",
      "Epoch 20/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0394\n",
      "validate accuracy:\n",
      " [0.53220251 0.36048234 0.38346747 0.51234657 0.50757468 0.38483477\n",
      " 0.35908803 0.62919563 0.82949311 0.83729053 0.79381591 0.36137423\n",
      " 0.46783707 0.62937158 0.62662023 0.4635585  0.36888951] @epoch 19\n",
      "Epoch 020: val_acc did not improve from 0.608\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0394 - val_loss: 0.0465\n",
      "Counter: 1, Global: 0.6076791081577539, MyBest: 0.6076791081577539\n",
      "\n",
      "Epoch 21/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0391\n",
      "validate accuracy:\n",
      " [0.61723908 0.43621749 0.46730381 0.61689532 0.60597318 0.46200302\n",
      " 0.45387375 0.69979745 0.88213938 0.88533521 0.84987718 0.4730666\n",
      " 0.56699437 0.72202873 0.71916378 0.56199974 0.47315627] @epoch 20\n",
      "Epoch 021: val_acc improved from 0.608 to 0.617\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0391 - val_loss: 0.0440\n",
      "Counter: 2, Global: 0.6172390803694725, MyBest: 0.6076791081577539\n",
      "\n",
      "Epoch 22/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0389\n",
      "validate accuracy:\n",
      " [0.60654373 0.43219802 0.44701543 0.58209383 0.58173424 0.45965442\n",
      " 0.44564071 0.6652199  0.87990505 0.88687152 0.8497327  0.46995616\n",
      " 0.5558989  0.72021735 0.71623695 0.55273134 0.45959312] @epoch 21\n",
      "Epoch 022: val_acc did not improve from 0.617\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0389 - val_loss: 0.0440\n",
      "Counter: 0, Global: 0.6172390803694725, MyBest: 0.6172390803694725\n",
      "\n",
      "Epoch 23/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0386\n",
      "validate accuracy:\n",
      " [0.60361859 0.41908187 0.46780685 0.58685923 0.59039098 0.46468714\n",
      " 0.4399409  0.67274308 0.87655354 0.88128489 0.84698743 0.43941751\n",
      " 0.5400281  0.72244668 0.70564461 0.54683334 0.45719129] @epoch 22\n",
      "Epoch 023: val_acc did not improve from 0.617\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0386 - val_loss: 0.0442\n",
      "Counter: 1, Global: 0.6172390803694725, MyBest: 0.6172390803694725\n",
      "\n",
      "Epoch 24/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0384\n",
      "validate accuracy:\n",
      " [0.61482973 0.43960229 0.47082496 0.60043323 0.59760499 0.4695521\n",
      " 0.44416296 0.6802662  0.87138671 0.88100559 0.83528394 0.47532871\n",
      " 0.56390452 0.73178208 0.73128921 0.57126808 0.47358012] @epoch 23\n",
      "Epoch 024: val_acc did not improve from 0.617\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0384 - val_loss: 0.0439\n",
      "Counter: 2, Global: 0.6172390803694725, MyBest: 0.6172390803694725\n",
      "\n",
      "Epoch 25/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0382\n",
      "validate accuracy:\n",
      " [0.61477006 0.4266977  0.47032192 0.60823107 0.61102295 0.4760946\n",
      " 0.4452185  0.69791669 0.87529677 0.88519555 0.84568703 0.48183233\n",
      " 0.55070227 0.70335793 0.71679443 0.55975282 0.48219836] @epoch 24\n",
      "Epoch 025: val_acc did not improve from 0.617\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0382 - val_loss: 0.0437\n",
      "Counter: 3, Global: 0.6172390803694725, MyBest: 0.6172390803694725\n",
      "\n",
      "Epoch 26/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0379\n",
      "validate accuracy:\n",
      " [0.62689566 0.4480643  0.47954392 0.60866427 0.61722696 0.49823856\n",
      " 0.45682922 0.69270831 0.88828373 0.88994414 0.85392284 0.48777041\n",
      " 0.56896067 0.73819143 0.74160278 0.57295322 0.48742583] @epoch 25\n",
      "Epoch 026: val_acc improved from 0.617 to 0.627\n",
      "576/576 [==============================] - 121s 210ms/step - loss: 0.0379 - val_loss: 0.0436\n",
      "Counter: 4, Global: 0.6268956623971462, MyBest: 0.6172390803694725\n",
      "\n",
      "Epoch 27/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0378\n",
      "validate accuracy:\n",
      " [0.60712812 0.39009944 0.43661973 0.59768951 0.58115715 0.45176983\n",
      " 0.43128562 0.69241899 0.8861891  0.89050281 0.85796851 0.47490457\n",
      " 0.55688202 0.71255398 0.71094078 0.56199974 0.4810681 ] @epoch 26\n",
      "Epoch 027: val_acc did not improve from 0.627\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0378 - val_loss: 0.0441\n",
      "Counter: 0, Global: 0.6268956623971462, MyBest: 0.6268956623971462\n",
      "\n",
      "Epoch 28/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0375\n",
      "validate accuracy:\n",
      " [0.62287401 0.42923629 0.48054996 0.60490972 0.60481894 0.47861096\n",
      " 0.44648513 0.68648726 0.88437366 0.89329606 0.85016614 0.48494273\n",
      " 0.56896067 0.73930609 0.74076653 0.57632357 0.4967505 ] @epoch 27\n",
      "Epoch 028: val_acc did not improve from 0.627\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0375 - val_loss: 0.0436\n",
      "Counter: 1, Global: 0.6268956623971462, MyBest: 0.6268956623971462\n",
      "\n",
      "Epoch 29/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0374\n",
      "validate accuracy:\n",
      " [0.63274789 0.44785276 0.48541248 0.63003612 0.62314242 0.49370912\n",
      " 0.45366266 0.7120949  0.88842338 0.89455307 0.86519289 0.49879825\n",
      " 0.58511233 0.7330361  0.73351914 0.57814914 0.50127155] @epoch 28\n",
      "Epoch 029: val_acc improved from 0.627 to 0.633\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0374 - val_loss: 0.0434\n",
      "Counter: 2, Global: 0.6327478941529989, MyBest: 0.6268956623971462\n",
      "\n",
      "Epoch 30/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0372\n",
      "validate accuracy:\n",
      " [0.62796744 0.44108313 0.47803488 0.60332131 0.62631655 0.48733434\n",
      " 0.46358454 0.69212961 0.88395476 0.88980448 0.86201417 0.49766719\n",
      " 0.572191   0.72425807 0.74118465 0.5860132  0.49858716] @epoch 29\n",
      "Epoch 030: val_acc did not improve from 0.633\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0372 - val_loss: 0.0439\n",
      "Counter: 0, Global: 0.6327478941529989, MyBest: 0.6327478941529989\n",
      "\n",
      "Epoch 31/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0369\n",
      "validate accuracy:\n",
      " [0.62464955 0.44341019 0.45389    0.58425993 0.61650556 0.48263714\n",
      " 0.46062908 0.69068289 0.89023882 0.89357543 0.86071378 0.49441537\n",
      " 0.57865167 0.73220009 0.73658538 0.58348548 0.49251202] @epoch 30\n",
      "Epoch 031: val_acc did not improve from 0.633\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0369 - val_loss: 0.0441\n",
      "Counter: 1, Global: 0.6327478941529989, MyBest: 0.6327478941529989\n",
      "\n",
      "Epoch 32/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0368\n",
      "validate accuracy:\n",
      " [0.61046495 0.42394754 0.45975855 0.58888084 0.60655028 0.47257173\n",
      " 0.4513405  0.6863426  0.87082809 0.87639666 0.83268315 0.48013574\n",
      " 0.55842698 0.71004599 0.7071777  0.56340402 0.47894886] @epoch 31\n",
      "Epoch 032: val_acc did not improve from 0.633\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0368 - val_loss: 0.0441\n",
      "Counter: 2, Global: 0.6327478941529989, MyBest: 0.6327478941529989\n",
      "\n",
      "Epoch 33/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0367\n",
      "validate accuracy:\n",
      " [0.63275454 0.45039138 0.48524481 0.61342961 0.60352039 0.49320585\n",
      " 0.45704031 0.68460649 0.89079738 0.89608938 0.86331457 0.51406759\n",
      " 0.58946627 0.74989551 0.75094074 0.58517063 0.49689177] @epoch 32\n",
      "Epoch 033: val_acc improved from 0.633 to 0.633\n",
      "576/576 [==============================] - 121s 210ms/step - loss: 0.0367 - val_loss: 0.0435\n",
      "Counter: 3, Global: 0.6327545419335365, MyBest: 0.6327478941529989\n",
      "\n",
      "Epoch 34/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0365\n",
      "validate accuracy:\n",
      " [0.62972429 0.42458218 0.4808853  0.61501807 0.62184387 0.49521893\n",
      " 0.45915136 0.69574654 0.88227904 0.88631284 0.85493428 0.50756395\n",
      " 0.58469099 0.74028146 0.73867595 0.58783877 0.50056511] @epoch 33\n",
      "Epoch 034: val_acc did not improve from 0.633\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0365 - val_loss: 0.0435\n",
      "Counter: 0, Global: 0.6327545419335365, MyBest: 0.6327545419335365\n",
      "\n",
      "Epoch 35/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0363\n",
      "validate accuracy:\n",
      " [0.63515174 0.4489105  0.49111333 0.60548735 0.61939114 0.50025165\n",
      " 0.47350645 0.69545716 0.88535118 0.89301676 0.85854644 0.51534003\n",
      " 0.58960676 0.74683017 0.74229968 0.58545148 0.51186776] @epoch 34\n",
      "Epoch 035: val_acc improved from 0.633 to 0.635\n",
      "576/576 [==============================] - 121s 210ms/step - loss: 0.0363 - val_loss: 0.0435\n",
      "Counter: 1, Global: 0.6351517401635647, MyBest: 0.6327545419335365\n",
      "\n",
      "Epoch 36/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0362\n",
      "validate accuracy:\n",
      " [0.60813811 0.41696635 0.46545944 0.58888084 0.58880395 0.47290725\n",
      " 0.45091832 0.67259836 0.87362099 0.87974858 0.84279728 0.45369714\n",
      " 0.55435395 0.71896338 0.72891986 0.56508917 0.45648488] @epoch 35\n",
      "Epoch 036: val_acc did not improve from 0.635\n",
      "576/576 [==============================] - 121s 210ms/step - loss: 0.0362 - val_loss: 0.0445\n",
      "Counter: 0, Global: 0.6351517401635647, MyBest: 0.6351517401635647\n",
      "\n",
      "Epoch 37/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0360\n",
      "validate accuracy:\n",
      " [0.62479451 0.4480643  0.47334003 0.61386281 0.58187854 0.48179835\n",
      " 0.45725143 0.68648726 0.87990505 0.88631284 0.85276693 0.50685704\n",
      " 0.58679777 0.73749477 0.71428573 0.575481   0.51412827] @epoch 36\n",
      "Epoch 037: val_acc did not improve from 0.635\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0360 - val_loss: 0.0440\n",
      "Counter: 1, Global: 0.6351517401635647, MyBest: 0.6351517401635647\n",
      "\n",
      "Epoch 38/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0359\n",
      "validate accuracy:\n",
      " [0.62275203 0.43790987 0.4741784  0.60851985 0.60150051 0.4707264\n",
      " 0.44500738 0.6857639  0.87795001 0.88463688 0.8464095  0.50516045\n",
      " 0.57598317 0.73011005 0.72989547 0.58123857 0.50904208] @epoch 37\n",
      "Epoch 038: val_acc did not improve from 0.635\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0359 - val_loss: 0.0438\n",
      "Counter: 2, Global: 0.6351517401635647, MyBest: 0.6351517401635647\n",
      "\n",
      "Epoch 39/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0358\n",
      "validate accuracy:\n",
      " [0.62356312 0.43198645 0.48021463 0.60736459 0.60669458 0.48314041\n",
      " 0.45450708 0.68243635 0.88088256 0.89050281 0.84770989 0.49611199\n",
      " 0.57935393 0.73247874 0.73491287 0.57323408 0.49547896] @epoch 38\n",
      "Epoch 039: val_acc did not improve from 0.635\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0358 - val_loss: 0.0438\n",
      "Counter: 3, Global: 0.6351517401635647, MyBest: 0.6351517401635647\n",
      "\n",
      "Epoch 40/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0357\n",
      "validate accuracy:\n",
      " [0.62473338 0.43981382 0.47686118 0.60606498 0.6029433  0.4863278\n",
      " 0.4667511  0.70066553 0.88102221 0.88351953 0.84944373 0.49978793\n",
      " 0.5695225  0.73122478 0.72069687 0.58264291 0.4984459 ] @epoch 39\n",
      "Epoch 040: val_acc did not improve from 0.635\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0357 - val_loss: 0.0442\n",
      "Counter: 4, Global: 0.6351517401635647, MyBest: 0.6351517401635647\n",
      "\n",
      "Epoch 41/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0355\n",
      "validate accuracy:\n",
      " [0.63326429 0.45271844 0.4919517  0.61371839 0.60784882 0.50411004\n",
      " 0.47308424 0.69126159 0.88088256 0.89050281 0.8534894  0.50615013\n",
      " 0.58932585 0.74474013 0.73965156 0.58629405 0.50649899] @epoch 40\n",
      "Epoch 041: val_acc did not improve from 0.635\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0355 - val_loss: 0.0436\n",
      "Counter: 5, Global: 0.6351517401635647, MyBest: 0.6351517401635647\n",
      "\n",
      "Epoch 42/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0353\n",
      "validate accuracy:\n",
      " [0.62568382 0.4592765  0.47116029 0.5956679  0.59962487 0.4886764\n",
      " 0.47392866 0.68041086 0.88465297 0.88840783 0.85796851 0.49300155\n",
      " 0.57092696 0.73554409 0.73128921 0.57899171 0.50141281] @epoch 41\n",
      "Epoch 042: val_acc did not improve from 0.635\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0353 - val_loss: 0.0437\n",
      "Counter: 6, Global: 0.6351517401635647, MyBest: 0.6351517401635647\n",
      "\n",
      "Epoch 43/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0353\n",
      "validate accuracy:\n",
      " [0.61370862 0.43812144 0.46931589 0.59638989 0.58635116 0.48465022\n",
      " 0.45281824 0.6657986  0.88227904 0.8837989  0.8484323  0.48154956\n",
      " 0.55842698 0.72620875 0.71972126 0.55076534 0.47471038] @epoch 42\n",
      "Epoch 043: val_acc did not improve from 0.635\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0353 - val_loss: 0.0441\n",
      "Counter: 7, Global: 0.6351517401635647, MyBest: 0.6351517401635647\n",
      "\n",
      "Epoch 043: Updating Learning rate.. New value is 0.000500\n",
      "Epoch 44/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0337\n",
      "validate accuracy:\n",
      " [0.65106864 0.46562302 0.50117373 0.64447653 0.63295341 0.51820165\n",
      " 0.48575047 0.71585649 0.89750034 0.89874303 0.86403698 0.53386116\n",
      " 0.60505617 0.76146024 0.75581884 0.61030751 0.52627861] @epoch 43\n",
      "Epoch 044: val_acc improved from 0.635 to 0.651\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0337 - val_loss: 0.0430\n",
      "Counter: 0, Global: 0.6510686371475458, MyBest: 0.6351517401635647\n",
      "\n",
      "Epoch 45/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0332\n",
      "validate accuracy:\n",
      " [0.6461581  0.45229533 0.49849096 0.63638991 0.62920213 0.51048481\n",
      " 0.48026177 0.70891201 0.8948471  0.89734638 0.86562634 0.52495402\n",
      " 0.6005618  0.75783753 0.75289196 0.60440952 0.52401811] @epoch 44\n",
      "Epoch 045: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0332 - val_loss: 0.0432\n",
      "Counter: 0, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 46/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0329\n",
      "validate accuracy:\n",
      " [0.6500323  0.45758408 0.50469482 0.63985562 0.63684893 0.51518202\n",
      " 0.47899514 0.71455437 0.89694178 0.89790505 0.86519289 0.52919555\n",
      " 0.60786515 0.75867355 0.75944251 0.60862237 0.52896297] @epoch 45\n",
      "Epoch 046: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0329 - val_loss: 0.0432\n",
      "Counter: 1, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 47/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0327\n",
      "validate accuracy:\n",
      " [0.6498404  0.46329597 0.50234741 0.64303249 0.63569468 0.51904041\n",
      " 0.4821617  0.71238428 0.89414889 0.89818436 0.86591536 0.52764034\n",
      " 0.60646069 0.75909156 0.75581884 0.60651594 0.5257135 ] @epoch 46\n",
      "Epoch 047: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0327 - val_loss: 0.0435\n",
      "Counter: 2, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 48/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0326\n",
      "validate accuracy:\n",
      " [0.64856068 0.45906496 0.5072099  0.63999999 0.63035637 0.51568526\n",
      " 0.47836185 0.71252894 0.89568496 0.900419   0.86403698 0.52339882\n",
      " 0.60519665 0.76034552 0.75442511 0.60567337 0.52458322] @epoch 47\n",
      "Epoch 048: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0326 - val_loss: 0.0436\n",
      "Counter: 3, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 49/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0324\n",
      "validate accuracy:\n",
      " [0.6450245  0.45292997 0.49279007 0.6329242  0.63093346 0.50645864\n",
      " 0.47582859 0.70659721 0.89219385 0.8965084  0.86345905 0.52735758\n",
      " 0.60519665 0.75909156 0.75010455 0.60202217 0.52599603] @epoch 48\n",
      "Epoch 049: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0324 - val_loss: 0.0436\n",
      "Counter: 4, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 50/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0323\n",
      "validate accuracy:\n",
      " [0.64678582 0.45462239 0.49832326 0.63263535 0.63295341 0.51048481\n",
      " 0.47646189 0.7103588  0.89470744 0.89776534 0.86475945 0.52820587\n",
      " 0.6082865  0.76104224 0.74996513 0.6048308  0.52317041] @epoch 49\n",
      "Epoch 050: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0323 - val_loss: 0.0436\n",
      "Counter: 5, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 51/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0322\n",
      "validate accuracy:\n",
      " [0.6461191  0.4558917  0.49916163 0.63234657 0.63093346 0.50729746\n",
      " 0.47772852 0.70471644 0.89191455 0.89594972 0.86071378 0.52933693\n",
      " 0.61207867 0.76201755 0.7502439  0.60525209 0.52232271] @epoch 50\n",
      "Epoch 051: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0322 - val_loss: 0.0437\n",
      "Counter: 6, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 52/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0321\n",
      "validate accuracy:\n",
      " [0.64647788 0.45779565 0.49916163 0.63162452 0.63237625 0.50528437\n",
      " 0.47582859 0.70355904 0.89456779 0.89818436 0.86692673 0.52806449\n",
      " 0.60786515 0.75964886 0.75010455 0.60637552 0.52627861] @epoch 51\n",
      "Epoch 052: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0321 - val_loss: 0.0437\n",
      "Counter: 7, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 052: Updating Learning rate.. New value is 0.000100\n",
      "Epoch 53/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0317\n",
      "validate accuracy:\n",
      " [0.64694106 0.45779565 0.49798793 0.63552344 0.63064492 0.51132363\n",
      " 0.47751743 0.70616317 0.89456779 0.89958102 0.86461496 0.53046799\n",
      " 0.60856742 0.7621569  0.75177699 0.60188174 0.520486  ] @epoch 52\n",
      "Epoch 053: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0317 - val_loss: 0.0438\n",
      "Counter: 0, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 54/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0316\n",
      "validate accuracy:\n",
      " [0.64618146 0.45462239 0.49714956 0.63379061 0.62963498 0.51165915\n",
      " 0.47603968 0.7048611  0.89414889 0.89762568 0.8638925  0.53018522\n",
      " 0.6082865  0.76382887 0.75094074 0.60005617 0.52218139] @epoch 53\n",
      "Epoch 054: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0316 - val_loss: 0.0438\n",
      "Counter: 1, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 55/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0316\n",
      "validate accuracy:\n",
      " [0.64521776 0.45356461 0.49547285 0.6265704  0.62977928 0.50612313\n",
      " 0.47540638 0.70688659 0.89400923 0.89720672 0.86548185 0.53244734\n",
      " 0.60870785 0.75909156 0.75052267 0.60398823 0.51822549] @epoch 54\n",
      "Epoch 055: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 120s 209ms/step - loss: 0.0316 - val_loss: 0.0438\n",
      "Counter: 2, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 56/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0316\n",
      "validate accuracy:\n",
      " [0.64558782 0.45462239 0.49731722 0.62888086 0.62949067 0.50981379\n",
      " 0.47413975 0.70818865 0.89289206 0.89818436 0.86418146 0.52749896\n",
      " 0.60926968 0.76132089 0.74996513 0.60188174 0.52175754] @epoch 55\n",
      "Epoch 056: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0316 - val_loss: 0.0439\n",
      "Counter: 3, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 57/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0315\n",
      "validate accuracy:\n",
      " [0.64524613 0.45187223 0.49362844 0.62902528 0.63035637 0.50847173\n",
      " 0.47456196 0.70833331 0.89191455 0.89804471 0.86490393 0.52848864\n",
      " 0.61011237 0.75978822 0.74968642 0.60511166 0.5196383 ] @epoch 56\n",
      "Epoch 057: val_acc did not improve from 0.651\n",
      "576/576 [==============================] - 121s 209ms/step - loss: 0.0315 - val_loss: 0.0439\n",
      "Counter: 4, Global: 0.6510686371475458, MyBest: 0.6510686371475458\n",
      "\n",
      "Epoch 58/151\n",
      "490/576 [========================>.....] - ETA: 14s - loss: 0.0315"
     ]
    }
   ],
   "source": [
    "\n",
    "net.compile(optimizer=optimizer, loss=weighted_mse_loss, metrics=[])\n",
    "#checkpointer = ModelCheckpoint(filepath=f'../cunet{nUNet}_{m}{n}_best.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True)\n",
    "history = net.fit(train_images,train_hms,validation_data=(eval_images,eval_hms2),epochs=151, batch_size=32,shuffle=True,verbose=1, callbacks=[evalcallback,updateLR])#,use_multiprocessing=True,workers=20)#[checkpointer,updateLR])\n",
    "#np.savez_compressed(\"../history\",history)\n",
    "net.save_weights(f'../cunet{nUNet}_{m}{n}_last_sigmoidAdamQuasiDef3sx12_restartsig5.h5', overwrite=True)\n",
    "np.save('../history_sigmoidAdamQuasiDef3sx12_restartsig5.npy',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "net.load_weights(\"../cunet2_6416_last_sigmoidAdamQuasiDef3sx12.h5\")\n",
    "output = net.predict(train_images)\n",
    "output = np.transpose(output,(0,3,1,2))\n",
    "print(accuracy(output,np.transpose(train_hms,(0,3,1,2))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ho aumentato solo patience \n",
    "#Provato con dropout ma andato male"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
