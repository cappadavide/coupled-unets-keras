{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import setGPU\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "160\n",
      "240\n",
      "320\n",
      "96\n",
      "192\n",
      "288\n",
      "384\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 128 9600        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 128 512         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 128 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 73728       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 64)   4096        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   9216        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64, 64, 80)   0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 80)   320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   5120        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 64)   4096        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   9216        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 80)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 80)   320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 80)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 64)   5120        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   4096        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 16)   9216        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 80)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 80)   320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 80)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 64)   5120        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 64)     4096        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 16)     9216        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 80)     0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 80)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     5120        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 64)     0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 64)     256         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 64)     4096        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 4, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 4, 4, 64)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 4, 4, 16)     9216        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 80)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4, 4, 80)     0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     5120        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 80)     0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 144)    0           conv2d_17[0][0]                  \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 144)    576         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 144)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     9216        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 64)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 80)   320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 16)     9216        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 80)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 8, 160)    0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 64)   5120        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 160)  0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 224)  0           conv2d_13[0][0]                  \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 224)  896         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 224)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 64)   14336       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 64)   256         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 80)   320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 16)   9216        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 80)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 240)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 64)   5120        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 240)  0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 304)  0           conv2d_9[0][0]                   \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 304)  1216        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 304)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 64)   19456       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 64)   256         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 80)   320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 16)   9216        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 80)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 320)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 64)   5120        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 320)  0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 64, 64, 384)  0           conv2d_5[0][0]                   \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 384)  1536        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 384)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 64)   24576       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 64, 64, 64)   256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 64, 64, 64)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 16)   9216        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 64, 64, 400)  0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64, 64, 464)  0           max_pooling2d[0][0]              \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64, 64, 464)  1856        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64, 64, 464)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 64, 64, 64)   29696       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 64, 64, 80)   0           conv2d_32[0][0]                  \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 64, 64, 80)   320         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 64, 64, 80)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 64, 64, 64)   5120        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 64, 64, 64)   256         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 64, 64, 64)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 16)   9216        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 64, 64, 96)   0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 64, 64, 96)   384         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 64, 64, 96)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 64, 64)   6144        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 32, 32, 80)   0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 80)   320         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 80)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 64)   5120        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 64)   256         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 64)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 16)   9216        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 32, 32, 96)   0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 96)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 64)   6144        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 80)   0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 80)   320         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 80)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 64)   5120        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 64)   256         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 64)   0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 16)   9216        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 96)   0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 96)   384         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 96)   0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 64)   6144        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 80)     0           max_pooling2d_7[0][0]            \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 80)     320         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 80)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 64)     5120        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 64)     256         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 64)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 16)     9216        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 96)     0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 96)     384         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 96)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 64)     6144        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 64)     0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 4, 4, 80)     0           max_pooling2d_8[0][0]            \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 80)     320         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 80)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 64)     5120        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 64)     256         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 64)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 8, 96)     384         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 16)     9216        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 8, 8, 96)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 4, 4, 96)     0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 8, 8, 64)     6144        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 8, 8, 96)     0           concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 176)    0           conv2d_48[0][0]                  \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 176)    704         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 8, 8, 176)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 8, 8, 64)     11264       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 8, 8, 64)     256         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 8, 8, 64)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 16, 16, 96)   384         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 8, 8, 16)     9216        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 16, 16, 96)   0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 192)    0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 16, 16, 64)   6144        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 192)  0           concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 16, 16, 272)  0           conv2d_44[0][0]                  \n",
      "                                                                 up_sampling2d_5[0][0]            \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 272)  1088        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 272)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 64)   17408       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 64)   256         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 16, 64)   0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 16, 16, 16)   9216        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 32, 96)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 288)  0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 32, 32, 64)   6144        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 288)  0           concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 32, 32, 368)  0           conv2d_40[0][0]                  \n",
      "                                                                 up_sampling2d_6[0][0]            \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 368)  1472        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 368)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 64)   23552       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 64)   256         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 32, 32, 64)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 64, 64, 96)   384         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 32, 32, 16)   9216        activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 64, 64, 96)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 32, 32, 384)  0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 64, 64, 64)   6144        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 384)  0           concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 64, 64, 464)  0           conv2d_36[0][0]                  \n",
      "                                                                 up_sampling2d_7[0][0]            \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 64, 64, 464)  1856        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 64, 64, 464)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 64, 64, 64)   29696       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 64, 64, 64)   256         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 64, 64, 64)   0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 64, 64, 16)   9216        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 64, 64, 480)  0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 64, 64, 480)  1920        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 64, 64, 480)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 64, 64, 16)   7680        activation_63[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 599,296\n",
      "Trainable params: 585,792\n",
      "Non-trainable params: 13,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Definizione Modello\n",
    "def getUnit1(layerPrec, filters, kernel_size = (1, 1), activation='relu', kernel_initializer='he_normal'):\n",
    "  l = BatchNormalization()(layerPrec)\n",
    "  l = Activation('relu')(l)\n",
    "  l = Conv2D(filters, kernel_size, kernel_initializer=kernel_initializer, padding='same',use_bias=False)(l)\n",
    "  return l\n",
    "\n",
    "def getUnit2(layerPrec, filters, kernel_size = (3, 3), activation='relu', kernel_initializer='he_normal'):\n",
    "  l = BatchNormalization()(layerPrec)\n",
    "  l = Activation('relu')(l)\n",
    "  l = Conv2D(filters, kernel_size, kernel_initializer=kernel_initializer, padding='same',use_bias=False)(l)\n",
    "  return l\n",
    "\n",
    "def getDownBlock(layerPrec,m,n,indexBlock):\n",
    "  l = getUnit1(layerPrec,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][f\"down{indexBlock}\"].append(l)\n",
    "\n",
    "  concat = Concatenate()([layerPrec,l])\n",
    "  l = getUnit1(concat,m)\n",
    "  maxPooling = MaxPool2D(padding='same')(l)\n",
    "  return maxPooling, getUnit1(concat,m)\n",
    "\n",
    "def getUpBlock(layerPrec,skipConn,m,n,indexBlock,upLayers=[]):\n",
    "  l = getUnit1(layerPrec,m)\n",
    "  l = UpSampling2D()(layerPrec)\n",
    "  concat = Concatenate()([skipConn,l]+upLayers)\n",
    "  l = getUnit1(concat,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][f\"up{indexBlock}\"].append(l)\n",
    "  concat = Concatenate()([concat,l])\n",
    "  return concat\n",
    "\n",
    "def getUNet(input,m,n,indexUNet,nUNet, nBlocks):\n",
    "  layerPrec = input\n",
    "  listSkipConn = []\n",
    "\n",
    "  if indexUNet != 0:\n",
    "    layerPrec = nets[f\"unet{indexUNet-1}\"]\n",
    "    #layerPrec = Concatenate()([input,layerPrec]) #l'abbiamo fatto gi con l'if dopo l'up\n",
    "    layerPrec = getUnit1(layerPrec,m)\n",
    "\n",
    "  # down\n",
    "  for i in range(nBlocks):\n",
    "    if nets[\"layers\"][f\"down{i}\"]:\n",
    "        layerPrec = Concatenate()([layerPrec]+nets[\"layers\"][f\"down{i}\"])\n",
    "    layerPrec, skipConn = getDownBlock(layerPrec,m,n,i)\n",
    "    listSkipConn.append(skipConn)\n",
    "\n",
    "  # bottle neck\n",
    "  if nets[\"layers\"][f\"bn\"]:\n",
    "      layerPrec = Concatenate()([layerPrec]+nets[\"layers\"][f\"bn\"])\n",
    "\n",
    "  l = getUnit1(layerPrec,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][\"bn\"].append(l)\n",
    "  concat = Concatenate()([layerPrec,l])\n",
    "\n",
    "  # up\n",
    "  layerPrec = concat\n",
    "  for i in range(nBlocks):\n",
    "    layerPrec = getUpBlock(layerPrec,listSkipConn[-(i+1)],m,n,i,upLayers=nets[\"layers\"][f\"up{i}\"])\n",
    "\n",
    "\n",
    "  if indexUNet != nUNet - 1:\n",
    "    l = Concatenate()([input,layerPrec])\n",
    "  else:\n",
    "    l = getUnit1(layerPrec,16,activation=\"sigmoid\") #era linear\n",
    "\n",
    "  return l\n",
    "\n",
    "def trasformationInput(x, filters):\n",
    "  #x = BatchNormalization()(x)\n",
    "  #x = Activation('relu')(x)\n",
    "  #x = Conv2D(filters, kernel_size=(7,7), strides=(2,2), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x)\n",
    "  #maxPooling = MaxPool2D(padding='same')(x)\n",
    "  \n",
    "  x = Conv2D(filters*2, kernel_size=(5,5), strides=(1,1), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Conv2D(filters, kernel_size=(3,3), strides=(1,1), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x) #era 3x3\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPool2D(padding='same')(x)\n",
    "\n",
    "  return x#maxPooling\n",
    "\n",
    "def getCUNet(shape,m,n,nUNet,nBlocks):\n",
    "  for i in range(nUNet):\n",
    "    nets[f\"unet{i}\"] = None\n",
    "\n",
    "  for j in range(nBlocks):\n",
    "    nets[\"layers\"][f\"down{j}\"] = []\n",
    "    nets[\"layers\"][f\"up{j}\"] = []\n",
    "    \n",
    "  input = Input(shape=shape)\n",
    "  \n",
    "  t_input = trasformationInput(input,m) # per le heatmap da 64x64\n",
    "\n",
    "  for i in range(nUNet):\n",
    "    nets[f\"unet{i}\"] = getUNet(t_input,m,n,i,nUNet,nBlocks)\n",
    "\n",
    "  output = nets[f\"unet{nUNet-1}\"]\n",
    "  return Model(inputs=input, outputs=output)\n",
    "\n",
    "nets = {}\n",
    "nets[\"layers\"] = {}\n",
    "nets[\"layers\"][\"bn\"] = []\n",
    "shape = (128,128,3)\n",
    "m = 64\n",
    "n = 16\n",
    "nUNet = 2\n",
    "nBlocks = 4\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "net = getCUNet(shape,m,n,nUNet,nBlocks)\n",
    "\n",
    "layers = net.layers\n",
    "#print(layers[57].input)\n",
    "for i in range(len(layers)):\n",
    "    if isinstance(layers[i], tf.python.keras.layers.convolutional.Conv2D):\n",
    "        if isinstance(layers[i-1].input,list):\n",
    "            in_chan = 0\n",
    "            for k in layers[i-1].input:\n",
    "                in_chan = in_chan + k.shape[-1]\n",
    "            print(in_chan)\n",
    "        else:\n",
    "            in_chan = layers[i-1].input.shape[-1]\n",
    "            \n",
    "        n1 = layers[i].kernel_size[0] * layers[i].kernel_size[1] * in_chan\n",
    "        stdv = 1/math.sqrt(n1)\n",
    "        layers[i].kernel_initializer = tf.keras.initializers.RandomUniform(minval=-stdv, maxval=stdv)\n",
    "\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_preds(heatmaps):\n",
    "    \"\"\"Get keypoint predictions from score maps.\n",
    "    Note:\n",
    "        batch_size: N\n",
    "        num_keypoints: K\n",
    "        heatmap height: H\n",
    "        heatmap width: W\n",
    "    Args:\n",
    "        heatmaps (np.ndarray[N, K, H, W]): model predicted heatmaps.\n",
    "    Returns:\n",
    "        tuple: A tuple containing aggregated results.\n",
    "        - preds (np.ndarray[N, K, 2]): Predicted keypoint location.  \n",
    "        - maxvals (np.ndarray[N, K, 1]): Scores (confidence) of the keypoints. non lo restituiamo\n",
    "    \"\"\"\n",
    "    #assert isinstance(heatmaps, np.ndarray), ('heatmaps should be numpy.ndarray')\n",
    "    #assert heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n",
    "\n",
    "    N, K, _, W = heatmaps.shape\n",
    "    heatmaps_reshaped = heatmaps.reshape((N, K, -1))\n",
    "    idx = np.argmax(heatmaps_reshaped, 2).reshape((N, K, 1))\n",
    "    maxvals = np.amax(heatmaps_reshaped, 2).reshape((N, K, 1))\n",
    "\n",
    "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n",
    "    preds[:, :, 0] = preds[:, :, 0] % W\n",
    "    preds[:, :, 1] = preds[:, :, 1] // W\n",
    "\n",
    "    preds = np.where(np.tile(maxvals, (1, 1, 2)) > 0.0, preds, -1)\n",
    "    return preds#, maxvals\n",
    "\n",
    "def calc_dists(preds, target, normalize, use_zero=False):\n",
    "    preds = preds.astype(np.float32)\n",
    "    target = target.astype(np.float32)\n",
    "    normalize = normalize.astype(np.float32)\n",
    "    dists = np.zeros((preds.shape[1], preds.shape[0]))\n",
    "    if use_zero:\n",
    "        boundary = 0\n",
    "    else:\n",
    "        boundary = 1\n",
    "    for n in range(preds.shape[0]):\n",
    "        for c in range(preds.shape[1]):\n",
    "            if target[n,c,0] > boundary and target[n, c, 1] > boundary:\n",
    "                dists[c, n] = np.linalg.norm((preds[n,c,:]- target[n,c,:])/normalize[n]) # axis ricavato da solo\n",
    "            else:\n",
    "                dists[c, n] = -1\n",
    "    return dists\n",
    "\n",
    "def dist_acc(dists, thr=0.5):\n",
    "\n",
    "  #Return percentage below threshold while ignoring values with a -1\n",
    "\n",
    "  if (dists!=-1).sum() > 0:\n",
    "\n",
    "    return ((dists<=thr) == (dists!=-1)).sum().astype(np.float32) / (dists!=-1).sum().astype(np.float32)\n",
    "\n",
    "  else:\n",
    "\n",
    "    return -1\n",
    "\n",
    "def accuracy(output, target, thr=0.5):\n",
    "  ''' Calculate accuracy according to PCK, but uses ground truth heatmap rather than x,y locations\n",
    "        First value to be returned is average accuracy across 'idxs', followed by individual accuracies\n",
    "    '''\n",
    "  #output = output.numpy()\n",
    "  #print(output.__class__)\n",
    "  #target = target.numpy()\n",
    "\n",
    "  idkp = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "  preds = get_max_preds(output)\n",
    "  gts = get_max_preds(target)\n",
    "  norm = np.ones(preds.shape[0])*output.shape[3]/10\n",
    "\n",
    "  dists = calc_dists(preds, gts, norm)\n",
    "\n",
    "  acc = np.zeros(len(idkp)+1)\n",
    "  avg_acc = 0\n",
    "  cnt = 0\n",
    "\n",
    "  for i in range(len(idkp)):\n",
    "    acc[i+1] = dist_acc(dists[idkp[i]])\n",
    "    if acc[i+1] >= 0: \n",
    "      avg_acc = avg_acc + acc[i+1]\n",
    "      cnt += 1\n",
    "            \n",
    "  if cnt != 0:  \n",
    "    acc[0] = avg_acc / cnt\n",
    "\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import cv2\n",
    "\n",
    "# =============================================================================\n",
    "# General image processing functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_transform(center, scale, res, rot=0):\n",
    "    # Generate transformation matrix\n",
    "    h = 200 * scale\n",
    "    t = np.zeros((3, 3))\n",
    "    t[0, 0] = float(res[1]) / h\n",
    "    t[1, 1] = float(res[0]) / h\n",
    "    t[0, 2] = res[1] * (-float(center[0]) / h + .5)\n",
    "    t[1, 2] = res[0] * (-float(center[1]) / h + .5)\n",
    "    t[2, 2] = 1\n",
    "    if not rot == 0:\n",
    "        rot = -rot # To match direction of rotation from cropping\n",
    "        rot_mat = np.zeros((3,3))\n",
    "        rot_rad = rot * np.pi / 180\n",
    "        sn,cs = np.sin(rot_rad), np.cos(rot_rad)\n",
    "        rot_mat[0,:2] = [cs, -sn]\n",
    "        rot_mat[1,:2] = [sn, cs]\n",
    "        rot_mat[2,2] = 1\n",
    "        # Need to rotate around center\n",
    "        t_mat = np.eye(3)\n",
    "        t_mat[0,2] = -res[1]/2\n",
    "        t_mat[1,2] = -res[0]/2\n",
    "        t_inv = t_mat.copy()\n",
    "        t_inv[:2,2] *= -1\n",
    "        t = np.dot(t_inv,np.dot(rot_mat,np.dot(t_mat,t)))\n",
    "    return t\n",
    "\n",
    "def transform(pt, center, scale, res, invert=0, rot=0):\n",
    "    # Transform pixel location to different reference\n",
    "    t = get_transform(center, scale, res, rot=rot)\n",
    "    if invert:\n",
    "        t = np.linalg.inv(t)\n",
    "    new_pt = np.array([pt[0], pt[1], 1.]).T\n",
    "    new_pt = np.dot(t, new_pt)\n",
    "    return new_pt[:2].astype(int)\n",
    "\n",
    "def crop(img, center, scale, res, rot=0):\n",
    "    # Upper left point\n",
    "    ul = np.array(transform([0, 0], center, scale, res, invert=1))\n",
    "    # Bottom right point\n",
    "    br = np.array(transform(res, center, scale, res, invert=1))\n",
    "\n",
    "    new_shape = [br[1] - ul[1], br[0] - ul[0]]\n",
    "    if len(img.shape) > 2:\n",
    "        new_shape += [img.shape[2]]\n",
    "    new_img = np.zeros(new_shape)\n",
    "\n",
    "    # Range to fill new array\n",
    "    new_x = max(0, -ul[0]), min(br[0], len(img[0])) - ul[0]\n",
    "    new_y = max(0, -ul[1]), min(br[1], len(img)) - ul[1]\n",
    "    # Range to sample from original image\n",
    "    old_x = max(0, ul[0]), min(len(img[0]), br[0])\n",
    "    old_y = max(0, ul[1]), min(len(img), br[1])\n",
    "    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], old_x[0]:old_x[1]]\n",
    "\n",
    "    return cv2.resize(new_img, res)\n",
    "\n",
    "def inv_mat(mat):\n",
    "    ans = np.linalg.pinv(np.array(mat).tolist() + [[0,0,1]])\n",
    "    return ans[:2]\n",
    "\n",
    "def kpt_affine(kpt, mat):\n",
    "    kpt = np.array(kpt)\n",
    "    shape = kpt.shape\n",
    "    kpt = kpt.reshape(-1, 2)\n",
    "    return np.dot( np.concatenate((kpt, kpt[:, 0:1]*0+1), axis = 1), mat.T ).reshape(shape)\n",
    "\n",
    "\n",
    "def resize(im, res):\n",
    "    return np.array([cv2.resize(im[i],res) for i in range(im.shape[0])])\n",
    "\n",
    "def generateHeatmap(keypoints,output_res,num_parts):\n",
    "    #Init\n",
    "    sigma = output_res/64\n",
    "    size = 6*sigma+3\n",
    "    x = np.arange(0, size, 1, float) #crea un array composto da nove 1.0 vettore 1-D (9,)\n",
    "    y = x[:, np.newaxis] #Vettore 2-D (9,1)\n",
    "    x0, y0 = 3*sigma + 1, 3*sigma + 1 #vedi gi a tutto\n",
    "    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
    "    #Generation\n",
    "    hms = np.zeros(shape = (num_parts, output_res, output_res), dtype = np.float32) #crea vettore (16,64,64), cio 16 heatmaps nere\n",
    "    for p in keypoints:\n",
    "        for idx, pt in enumerate(p): #ottiene id + [x,y] di ogni keypoint\n",
    "            if pt[0] > 0: \n",
    "                x, y = int(pt[0]), int(pt[1])\n",
    "                if x<0 or y<0 or x>=output_res or y>=output_res: #se succede questo, rimane heatmap idx-esima  tutta a 0\n",
    "                    continue\n",
    "                ul = int(x - 3*sigma - 1), int(y - 3*sigma - 1)\n",
    "                br = int(x + 3*sigma + 2), int(y + 3*sigma + 2)\n",
    "\n",
    "                c,d = max(0, -ul[0]), min(br[0], output_res) - ul[0]\n",
    "                a,b = max(0, -ul[1]), min(br[1], output_res) - ul[1]\n",
    "\n",
    "                cc,dd = max(0, ul[0]), min(br[0], output_res)\n",
    "                aa,bb = max(0, ul[1]), min(br[1], output_res)\n",
    "                hms[idx, aa:bb,cc:dd] = np.maximum(hms[idx, aa:bb,cc:dd], g[a:b,c:d])\n",
    "    return hms\n",
    "def getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    cropped = crop(img,c,s,inp_res)\n",
    "    orig_keypoints = []\n",
    "    for i in keypoints:\n",
    "        orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "    orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "    kptmp = np.copy(orig_keypoints)\n",
    "    for i in range(orig_keypoints.shape[1]):\n",
    "        if orig_keypoints[0,i,0] > 0:\n",
    "            orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "    keypoints = np.copy(orig_keypoints)\n",
    "    h, w = cropped.shape[0:2]\n",
    "    center = np.array((w/2,h/2))\n",
    "    scale = max(h,w)/200\n",
    "    aug_rot = (np.random.random()*2-1)*30\n",
    "    aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "    scale *= aug_scale\n",
    "\n",
    "    mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "    mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "    inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "    keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "    \n",
    "    #print('\\n')\n",
    "\n",
    "\n",
    "    #Flip 50% probability\n",
    "    if np.random.randint(2) == 0:\n",
    "        inp = inp[:, ::-1]\n",
    "        keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "        keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "        orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "        orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "    ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "        for i in range(np.shape(orig_keypoints)[1]):\n",
    "            if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                keypoints[0,i,0] = 0\n",
    "                keypoints[0,i,1] = 0\n",
    "                orig_keypoints[0,i,0] = 0\n",
    "                orig_keypoints[0,i,1] = 0\n",
    "\n",
    "    heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "    return inp,heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.keras.optimizers.RMSprop(\n",
    "#    learning_rate=6.7e-3, rho=0.99, momentum=0.0, epsilon=1e-08\n",
    "#)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=6.7e-3)\n",
    "\n",
    "\n",
    "def heatmapLoss(y_true,y_pred):\n",
    "    l = tf.math.square((y_pred - y_true))\n",
    "    l = tf.reduce_mean(l,axis=3)\n",
    "    l = tf.reduce_mean(l,axis=2)\n",
    "    l = tf.reduce_mean(l,axis=1)\n",
    "    return l\n",
    "\n",
    "def euclidean_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_true - y_pred)))\n",
    "\n",
    "def dice(y_true,y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice(y_true, y_pred)\n",
    "\n",
    "def weighted_mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    apply weights on heatmap mse loss to only pick valid keypoint heatmap\n",
    "    since y_true would be gt_heatmap with shape\n",
    "    (batch_size, heatmap_size[0], heatmap_size[1], num_keypoints)\n",
    "    we sum up the heatmap for each keypoints and check. Sum for invalid\n",
    "    keypoint would be 0, so we can get a keypoint weights tensor with shape\n",
    "    (batch_size, 1, 1, num_keypoints)\n",
    "    and multiply to loss\n",
    "    \"\"\"\n",
    "    heatmap_sum = K.sum(K.sum(y_true, axis=1, keepdims=True), axis=2, keepdims=True)\n",
    "\n",
    "    # keypoint_weights shape: (batch_size, 1, 1, num_keypoints), with\n",
    "    # valid_keypoint = 1.0, invalid_keypoint = 0.0\n",
    "    keypoint_weights = 1.0 - K.cast(K.equal(heatmap_sum, 0.0), 'float32')\n",
    "\n",
    "    return K.sqrt(K.mean(K.square((y_true - y_pred) * keypoint_weights)))\n",
    "\n",
    "def smooth_l1_loss(y_true, y_pred):\n",
    "    diff = K.abs(y_true - y_pred)\n",
    "    less_than_one = K.cast(K.less(diff, 1.0), 'float32')\n",
    "    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "parts = {'mpii':['rank', 'rkne', 'rhip',\n",
    "                 'lhip', 'lkne', 'lank',\n",
    "                 'pelv', 'thrx', 'neck', 'head',\n",
    "                 'rwri', 'relb', 'rsho',\n",
    "                 'lsho', 'lelb', 'lwri']}\n",
    "\n",
    "flipped_parts = {'mpii':[5, 4, 3, 2, 1, 0, 6, 7, 8, 9, 15, 14, 13, 12, 11, 10]}\n",
    "\n",
    "part_pairs = {'mpii':[[0, 5], [1, 4], [2, 3], [6], [7], [8], [9], [10, 15], [11, 14], [12, 13]]}\n",
    "\n",
    "pair_names = {'mpii':['ankle', 'knee', 'hip', 'pelvis', 'thorax', 'neck', 'head', 'wrist', 'elbow', 'shoulder']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imageio\n",
    "\n",
    "class Dataset(tf.keras.utils.Sequence):\n",
    "    #Costruttore\n",
    "    def __init__(self,json_path,batch_size,input_shape=(256,256,3),output_shape=(64,64,16),train=True,dataset_name=\"MPII\"):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.batch_size = batch_size\n",
    "        f = open(json_path)\n",
    "        self.dataset = json.load(f)\n",
    "        if dataset_name == \"MPII\":\n",
    "            self.dataset = self.dataset[\"dataset\"][\"MPII\"][\"people\"]\n",
    "        if train:\n",
    "            self.dataset = self.dataset[:-7221]\n",
    "        self.batch_images = np.zeros(shape=((self.batch_size,)+(self.input_shape)), dtype=np.float32)\n",
    "        self.batch_heatmaps = np.zeros(shape=((self.batch_size,)+(self.output_shape)), dtype=np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        for n, annotation in enumerate(self.dataset[i*self.batch_size:(i+1)*self.batch_size]):\n",
    "            #sample_index = i*self.batch_size + n\n",
    "            image, gt_heatmap = self.extract_imgs_hms(annotation)\n",
    "            gt_heatmap = np.transpose(gt_heatmap,(1,2,0))\n",
    "            self.batch_images[n,:,:,:] = image\n",
    "            self.batch_heatmaps[n, :, :, :] = gt_heatmap#gt_heatmap.reshape((self.output_shape))\n",
    "            \n",
    "        return self.batch_images, self.batch_heatmaps\n",
    "#getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    \n",
    "    def extract_imgs_hms(self,annotation):\n",
    "        inp_res = self.input_shape[0:2]\n",
    "        out_res = self.output_shape[0:2]\n",
    "        img = imageio.imread(f\"../images_mpii/{annotation['filepath']}\")\n",
    "        c = [annotation[\"objpos\"][\"x\"],annotation[\"objpos\"][\"y\"]]\n",
    "        s = annotation[\"scale\"]\n",
    "        keypoints = annotation[\"keypoints\"]\n",
    "        cropped = crop(img,c,s,inp_res)\n",
    "        \n",
    "        orig_keypoints = []\n",
    "        for i in keypoints:\n",
    "            orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "        orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "        kptmp = np.copy(orig_keypoints)\n",
    "        for i in range(orig_keypoints.shape[1]):\n",
    "            if orig_keypoints[0,i,0] > 0:\n",
    "                orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "        keypoints = np.copy(orig_keypoints)\n",
    "        h, w = cropped.shape[0:2]\n",
    "        center = np.array((w/2,h/2))\n",
    "        scale = max(h,w)/200\n",
    "        aug_rot = (np.random.random()*2-1)*30\n",
    "        aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "        scale *= aug_scale\n",
    "\n",
    "        mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "        mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "        inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "        keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "\n",
    "        #print('\\n')\n",
    "\n",
    "\n",
    "        #Flip 50% probability\n",
    "        if np.random.randint(2) == 0:\n",
    "            inp = inp[:, ::-1]\n",
    "            keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "            keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "            orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "            orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "        ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "            for i in range(np.shape(orig_keypoints)[1]):\n",
    "                if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                    keypoints[0,i,0] = 0\n",
    "                    keypoints[0,i,1] = 0\n",
    "                    orig_keypoints[0,i,0] = 0\n",
    "                    orig_keypoints[0,i,1] = 0\n",
    "\n",
    "        heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "        return inp,heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imageio\n",
    "\n",
    "class Dataset2(tf.keras.utils.Sequence):\n",
    "    #Costruttore\n",
    "    def __init__(self,json_path,batch_size,input_shape=(256,256,3),output_shape=(64,64,16),train=True,dataset_name=\"MPII\"):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.images = np.load(\"../imgs_mpii.npz\")['arr_0']\n",
    "        self.hms = np.load(\"../hms_mpii.npz\")['arr_0']\n",
    "        self.hms = self.hms.reshape((self.hms.shape[0],64,64,16))\n",
    "        if dataset_name == \"MPII\":\n",
    "            self.images = self.images[\"dataset\"][\"MPII\"][\"people\"]\n",
    "        if train:\n",
    "            self.images = self.images[:-8665]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.images[i*self.batch_size:(i+1)*self.batch_size], self.hms[i*self.batch_size:(i+1)*self.batch_size]\n",
    "#getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    \n",
    "    def extract_imgs_hms(self,annotation):\n",
    "        inp_res = self.input_shape[0:2]\n",
    "        out_res = self.output_shape[0:2]\n",
    "        img = imageio.imread(f\"../images_mpii/{annotation['filepath']}\")\n",
    "        c = [annotation[\"objpos\"][\"x\"],annotation[\"objpos\"][\"y\"]]\n",
    "        s = annotation[\"scale\"]\n",
    "        keypoints = annotation[\"keypoints\"]\n",
    "        cropped = crop(img,c,s,inp_res)\n",
    "        \n",
    "        orig_keypoints = []\n",
    "        for i in keypoints:\n",
    "            orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "        orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "        kptmp = np.copy(orig_keypoints)\n",
    "        for i in range(orig_keypoints.shape[1]):\n",
    "            if orig_keypoints[0,i,0] > 0:\n",
    "                orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "        keypoints = np.copy(orig_keypoints)\n",
    "        h, w = cropped.shape[0:2]\n",
    "        center = np.array((w/2,h/2))\n",
    "        scale = max(h,w)/200\n",
    "        aug_rot = (np.random.random()*2-1)*30\n",
    "        aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "        scale *= aug_scale\n",
    "\n",
    "        mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "        mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "        inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "        keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "\n",
    "        #print('\\n')\n",
    "\n",
    "\n",
    "        #Flip 50% probability\n",
    "        if np.random.randint(2) == 0:\n",
    "            inp = inp[:, ::-1]\n",
    "            keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "            keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "            orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "            orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "        ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "            for i in range(np.shape(orig_keypoints)[1]):\n",
    "                if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                    keypoints[0,i,0] = 0\n",
    "                    keypoints[0,i,1] = 0\n",
    "                    orig_keypoints[0,i,0] = 0\n",
    "                    orig_keypoints[0,i,1] = 0\n",
    "\n",
    "        heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "        return inp,heatmaps\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def get_normalize(input_shape):\n",
    "    \"\"\"\n",
    "    rescale keypoint distance normalize coefficient\n",
    "    based on input shape, used for PCK evaluation\n",
    "    NOTE: 6.4 is standard normalize coefficient under\n",
    "          input shape (256,256)\n",
    "    # Arguments\n",
    "        input_shape: input image shape as (height, width)\n",
    "    # Returns\n",
    "        scale: normalize coefficient\n",
    "    \"\"\"\n",
    "    #assert input_shape[0] == input_shape[1], 'only support square input shape.'\n",
    "\n",
    "    # use averaged scale factor for non square input shape\n",
    "    scale = float((input_shape[0] + input_shape[1]) / 2) / 256.0\n",
    "\n",
    "    return 6.4*scale\n",
    "\n",
    "global_best_acc = 0.0\n",
    "\n",
    "class EvalCallBack(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model_input_shape, fileName, imgs,hms):\n",
    "        self.normalize = get_normalize(model_input_shape)\n",
    "        self.model_input_shape = model_input_shape\n",
    "        self.best_acc = 0.0\n",
    "        self.fileName = fileName\n",
    "        self.eval_images = imgs#np.load(\"../imgs_val_mpii128.npz\")['arr_0']\n",
    "        self.eval_hms = hms#np.load(\"../hms_val_mpii128.npz\")['arr_0']\n",
    "        self.listAcc = []\n",
    "        #self.eval_hms = np.reshape(self.eval_hms,(self.eval_hms.shape[0],64,64,16))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global global_best_acc\n",
    "        output = self.model.predict(self.eval_images)\n",
    "        #output = output.reshape( (output.shape[0],)+(16,64,64) )\n",
    "        output = np.transpose(output,(0,3,1,2))\n",
    "        val_acc = accuracy(output,self.eval_hms)\n",
    "        self.listAcc.append(val_acc[0])\n",
    "        print('\\nvalidate accuracy:\\n', val_acc, '@epoch', epoch)\n",
    "        f = open(self.fileName, \"a\")\n",
    "        if val_acc[0] > self.best_acc:\n",
    "            # Save best accuray value and model checkpoint\n",
    "            #checkpoint_dir = os.path.join(self.log_dir, 'ep{epoch:03d}-loss{loss:.3f}-val_acc{val_acc:.3f}.h5'.format(epoch=(epoch+1), loss=logs.get('loss'), val_acc=val_acc))\n",
    "            #self.model.save(f\"../modelsave/ep{epoch}_acc{val_acc[0]}.h5\")\n",
    "            self.model.save_weights(f\"../cunet{nUNet}_{m}{n}_best_sigmoidAdamQuasiDef3sx12_restartsig8.h5\")\n",
    "            print('Epoch {epoch:03d}: val_acc improved from {best_acc:.3f} to {val_acc:.3f}'.format(epoch=epoch+1, best_acc=self.best_acc, val_acc=val_acc[0]))# checkpoint_dir=checkpoint_dir))\n",
    "            message = 'Epoch {epoch:03d}: val_acc improved from {best_acc:.3f} to {val_acc:.3f}. Train loss is {loss:.3f}\\n'.format(epoch=epoch+1, best_acc=self.best_acc, val_acc=val_acc[0],loss=logs.get('loss'))\n",
    "            self.best_acc = val_acc[0]\n",
    "            global_best_acc = val_acc[0]\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            message = 'Epoch {epoch:03d}: val_acc did not improve from {best_acc:.3f}. Train loss is {loss:.3f}\\n'.format(epoch=epoch+1, best_acc=self.best_acc,loss=logs.get('loss'))\n",
    "            print('Epoch {epoch:03d}: val_acc did not improve from {best_acc:.3f}'.format(epoch=epoch+1, best_acc=self.best_acc))\n",
    "        f.write(message)\n",
    "        f.close()\n",
    "        if epoch == 150:\n",
    "            np.savez_compressed(f'../cunet{nUNet}_{m}{n}_best_sigmoidAdamQuasiDef3sx12_restartsig8_accs', self.listAcc)\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "class CustomLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience,factor,fileName=None):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.fileName = fileName\n",
    "        self.factor = factor\n",
    "        self.best_acc = 0.0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        global global_best_acc\n",
    "        print(f\"Counter: {self.counter}, Global: {global_best_acc}, MyBest: {self.best_acc}\\n\")\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "    \n",
    "    def schedule(self,epoch,lr):\n",
    "        global global_best_acc\n",
    "        if self.counter == self.patience:\n",
    "            self.counter = 0 \n",
    "            print(\"Epoch %03d: Updating Learning rate.. New value is %f\" % (epoch, lr*self.factor))\n",
    "            message = \"Epoch %03d: Updating Learning rate.. New value is %f\" % (epoch, lr*self.factor)\n",
    "            f = open(self.fileName, \"a\")\n",
    "            f.write(message)\n",
    "            f.close()\n",
    "            return lr*self.factor\n",
    "        if self.best_acc == global_best_acc:\n",
    "            self.counter = self.counter + 1\n",
    "        elif self.best_acc < global_best_acc:\n",
    "            self.counter = 0\n",
    "            self.best_acc = global_best_acc\n",
    "        return lr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finito 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndataset = Dataset(\"datasets.json\",32)\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "with open('../imgs_train_mpii128sx12_genhm.pickle', 'rb') as handle:\n",
    "    train_images = pickle.load(handle)\n",
    "with open('../hms_train_mpii128sx12_genhm.pickle', 'rb') as handle:\n",
    "    train_hms = pickle.load(handle)\n",
    "\n",
    "eval_images = np.load(\"../imgs_val_mpii128sx12_genhm.npz\")['arr_0']\n",
    "eval_hms = np.load(\"../hms_val_mpii128sx12_genhm.npz\")['arr_0']\n",
    "\n",
    "#train_images = np.load(\"../imgs_train_mpii128.npz\")['arr_0']\n",
    "#train_hms = np.load(\"../hms_train_mpii128.npz\")['arr_0']\n",
    "train_hms = np.transpose(train_hms,(0,2,3,1))#np.reshape(train_hms,(train_hms.shape[0],64,64,16))\n",
    "eval_hms2 = np.transpose(eval_hms,(0,2,3,1))\n",
    "print(\"Finito 1\")\n",
    "\"\"\"\n",
    "dataset = Dataset(\"datasets.json\",32)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finito 2\n"
     ]
    }
   ],
   "source": [
    "evalcallback = EvalCallBack((128,128),\"../sigmoidLayersAdamSchedulingQuasiDef3sx12_restartsig8.txt\",eval_images,eval_hms)\n",
    "updateLR = CustomLearningRateScheduler(7,0.2,fileName=\"../sigmoidLayersAdamSchedulingQuasiDef3sx12_restartsig8.txt\")\n",
    "print(\"Finito 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 0, Global: 0.0, MyBest: 0.0\n",
      "\n",
      "Epoch 1/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0789WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0077s vs `on_test_batch_end` time: 0.0431s). Check your callbacks.\n",
      "\n",
      "validate accuracy:\n",
      " [0.16828759 0.09223609 0.12910798 0.26671481 0.26605108 0.12850192\n",
      " 0.08127507 0.29412615 0.2225946  0.18421787 0.15792516 0.1341722\n",
      " 0.15702248 0.16274209 0.1487108  0.13340823 0.13379486] @epoch 0\n",
      "Epoch 001: val_acc improved from 0.000 to 0.168\n",
      "576/576 [==============================] - 141s 245ms/step - loss: 0.0789 - val_loss: 0.0545\n",
      "Counter: 1, Global: 0.1682875882834196, MyBest: 0.0\n",
      "\n",
      "Epoch 2/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0616\n",
      "validate accuracy:\n",
      " [0.20315201 0.13010366 0.13497652 0.27740073 0.28249893 0.11994632\n",
      " 0.11885159 0.32826969 0.28822792 0.28393856 0.22756827 0.15806589\n",
      " 0.17668539 0.20537829 0.18508711 0.18508637 0.14834699] @epoch 1\n",
      "Epoch 002: val_acc improved from 0.168 to 0.203\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0616 - val_loss: 0.0531\n",
      "Counter: 0, Global: 0.20315201440826058, MyBest: 0.1682875882834196\n",
      "\n",
      "Epoch 3/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0542\n",
      "validate accuracy:\n",
      " [0.30885706 0.1724138  0.18125419 0.29906136 0.31611601 0.18168093\n",
      " 0.17204982 0.39250579 0.57924873 0.61801678 0.47912151 0.18719073\n",
      " 0.23890449 0.3296642  0.35818815 0.25333521 0.18296129] @epoch 2\n",
      "Epoch 003: val_acc improved from 0.203 to 0.309\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0542 - val_loss: 0.0512\n",
      "Counter: 0, Global: 0.3088570609688759, MyBest: 0.20315201440826058\n",
      "\n",
      "Epoch 4/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0524\n",
      "validate accuracy:\n",
      " [0.33198279 0.14237359 0.19701542 0.3361733  0.33487231 0.19308841\n",
      " 0.18197171 0.4266493  0.6075967  0.65628493 0.49862736 0.23328149\n",
      " 0.28188202 0.37898844 0.37658536 0.26246312 0.20387115] @epoch 3\n",
      "Epoch 004: val_acc improved from 0.309 to 0.332\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0524 - val_loss: 0.0540\n",
      "Counter: 0, Global: 0.3319827886298299, MyBest: 0.3088570609688759\n",
      "\n",
      "Epoch 5/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0505\n",
      "validate accuracy:\n",
      " [0.38491624 0.22382061 0.20221327 0.36115524 0.35362864 0.21254823\n",
      " 0.23200338 0.44965279 0.7066052  0.72527933 0.6269325  0.2601442\n",
      " 0.30828652 0.48237425 0.46062717 0.30501333 0.24837525] @epoch 4\n",
      "Epoch 005: val_acc improved from 0.332 to 0.385\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0505 - val_loss: 0.0498\n",
      "Counter: 0, Global: 0.38491624407470226, MyBest: 0.3319827886298299\n",
      "\n",
      "Epoch 6/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0492\n",
      "validate accuracy:\n",
      " [0.42261131 0.24645653 0.24463448 0.39003611 0.39849949 0.25515854\n",
      " 0.26831329 0.51244211 0.74752128 0.76201117 0.68487215 0.27866533\n",
      " 0.33750001 0.51079839 0.51317072 0.33436316 0.27733824] @epoch 5\n",
      "Epoch 006: val_acc improved from 0.385 to 0.423\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0492 - val_loss: 0.0491\n",
      "Counter: 0, Global: 0.42261131294071674, MyBest: 0.38491624407470226\n",
      "\n",
      "Epoch 7/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0494\n",
      "validate accuracy:\n",
      " [0.02688139 0.02496298 0.02481556 0.04953068 0.01688068 0.0152659\n",
      " 0.02554359 0.07407407 0.03826281 0.02094972 0.02456293 0.01583486\n",
      " 0.01179775 0.03302215 0.0189547  0.01657071 0.01907318] @epoch 6\n",
      "Epoch 007: val_acc did not improve from 0.423\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0494 - val_loss: 0.9027\n",
      "Counter: 0, Global: 0.42261131294071674, MyBest: 0.42261131294071674\n",
      "\n",
      "Epoch 8/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0473\n",
      "validate accuracy:\n",
      " [0.4819211  0.28432411 0.26441985 0.44332129 0.46688789 0.31219593\n",
      " 0.33291113 0.56568289 0.80784804 0.82150841 0.75205892 0.30538669\n",
      " 0.39058989 0.61683154 0.61644602 0.4130038  0.31732127] @epoch 7\n",
      "Epoch 008: val_acc improved from 0.423 to 0.482\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0473 - val_loss: 0.0478\n",
      "Counter: 1, Global: 0.4819211047142744, MyBest: 0.42261131294071674\n",
      "\n",
      "Epoch 9/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0460\n",
      "validate accuracy:\n",
      " [0.5245577  0.33953881 0.3568075  0.5091697  0.50483334 0.34876698\n",
      " 0.34958833 0.61414933 0.83479959 0.84664804 0.79121512 0.35529479\n",
      " 0.43651685 0.63522363 0.65045297 0.45625615 0.36366206] @epoch 8\n",
      "Epoch 009: val_acc improved from 0.482 to 0.525\n",
      "576/576 [==============================] - 140s 244ms/step - loss: 0.0460 - val_loss: 0.0464\n",
      "Counter: 0, Global: 0.524557700380683, MyBest: 0.4819211047142744\n",
      "\n",
      "Epoch 10/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0444\n",
      "validate accuracy:\n",
      " [0.53846382 0.37042522 0.37240106 0.51133573 0.50872892 0.35296091\n",
      " 0.37935403 0.59331596 0.83368242 0.84301674 0.78514665 0.3806023\n",
      " 0.49171349 0.67172915 0.66606271 0.48125264 0.37369314] @epoch 10\n",
      "Epoch 011: val_acc improved from 0.525 to 0.538\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0444 - val_loss: 0.0462\n",
      "Counter: 1, Global: 0.5384638179093599, MyBest: 0.524557700380683\n",
      "\n",
      "Epoch 12/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0437\n",
      "validate accuracy:\n",
      " [0.55096238 0.37931034 0.40308517 0.5302527  0.51623142 0.39724877\n",
      " 0.39814228 0.63744211 0.85546714 0.85907823 0.79468286 0.39488193\n",
      " 0.47233146 0.65459108 0.64055747 0.47746104 0.40463409] @epoch 11\n",
      "Epoch 012: val_acc improved from 0.538 to 0.551\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0437 - val_loss: 0.0458\n",
      "Counter: 0, Global: 0.5509623810648918, MyBest: 0.5384638179093599\n",
      "\n",
      "Epoch 13/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0432\n",
      "validate accuracy:\n",
      " [0.58286643 0.38904166 0.43695506 0.55480146 0.56874907 0.43449086\n",
      " 0.42178595 0.65813076 0.86943161 0.87332404 0.83268315 0.42301711\n",
      " 0.51811796 0.70572662 0.69672471 0.51804525 0.42483753] @epoch 12\n",
      "Epoch 013: val_acc improved from 0.551 to 0.583\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0432 - val_loss: 0.0448\n",
      "Counter: 0, Global: 0.5828664265573025, MyBest: 0.5509623810648918\n",
      "\n",
      "Epoch 14/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0427\n",
      "validate accuracy:\n",
      " [0.58400906 0.40681192 0.44064388 0.58339351 0.56889337 0.42694178\n",
      " 0.41819718 0.6779514  0.87781036 0.88254189 0.83571738 0.42966211\n",
      " 0.50814605 0.68886721 0.665784   0.5061087  0.42667419] @epoch 13\n",
      "Epoch 014: val_acc improved from 0.583 to 0.584\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0427 - val_loss: 0.0448\n",
      "Counter: 0, Global: 0.5840090587735176, MyBest: 0.5828664265573025\n",
      "\n",
      "Epoch 15/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0421\n",
      "validate accuracy:\n",
      " [0.57317907 0.40363866 0.39805499 0.55451262 0.54191315 0.39523569\n",
      " 0.39750898 0.68590856 0.88130152 0.88659215 0.84019649 0.44295207\n",
      " 0.49971911 0.65082902 0.6565854  0.50189579 0.43402091] @epoch 14\n",
      "Epoch 015: val_acc did not improve from 0.584\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0421 - val_loss: 0.0450\n",
      "Counter: 0, Global: 0.5840090587735176, MyBest: 0.5840090587735176\n",
      "\n",
      "Epoch 16/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0417\n",
      "validate accuracy:\n",
      " [0.62629907 0.45166066 0.47971159 0.61617327 0.62617224 0.46636471\n",
      " 0.45302933 0.7037037  0.88995951 0.89231843 0.85609019 0.48055989\n",
      " 0.57205057 0.7429288  0.74731708 0.56859994 0.47414523] @epoch 15\n",
      "Epoch 016: val_acc improved from 0.584 to 0.626\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0417 - val_loss: 0.0433\n",
      "Counter: 1, Global: 0.6262990720570087, MyBest: 0.5840090587735176\n",
      "\n",
      "Epoch 17/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0413\n",
      "validate accuracy:\n",
      " [0.62425681 0.43240955 0.48054996 0.61588448 0.61419708 0.47542357\n",
      " 0.44500738 0.6958912  0.8951264  0.89622903 0.85710156 0.47843912\n",
      " 0.56685394 0.74557614 0.7452265  0.56284231 0.48135066] @epoch 16\n",
      "Epoch 017: val_acc did not improve from 0.626\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0413 - val_loss: 0.0433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 0, Global: 0.6262990720570087, MyBest: 0.6262990720570087\n",
      "\n",
      "Epoch 18/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0409\n",
      "validate accuracy:\n",
      " [0.62709082 0.44552571 0.47551978 0.60722023 0.60453039 0.4695521\n",
      " 0.46210682 0.69458914 0.89233345 0.89427376 0.85811299 0.49582919\n",
      " 0.57626402 0.74724817 0.74216026 0.56677431 0.50141281] @epoch 17\n",
      "Epoch 018: val_acc improved from 0.626 to 0.627\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0409 - val_loss: 0.0433\n",
      "Counter: 1, Global: 0.6270908210426569, MyBest: 0.6262990720570087\n",
      "\n",
      "Epoch 19/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0405\n",
      "validate accuracy:\n",
      " [0.63313786 0.45377618 0.48407111 0.61516243 0.61679411 0.47793993\n",
      " 0.46147349 0.71339697 0.89372993 0.89706701 0.86244762 0.49936378\n",
      " 0.57963485 0.74613351 0.74313587 0.57520014 0.5108788 ] @epoch 18\n",
      "Epoch 019: val_acc improved from 0.627 to 0.633\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0405 - val_loss: 0.0431\n",
      "Counter: 0, Global: 0.6331378575414419, MyBest: 0.6270908210426569\n",
      "\n",
      "Epoch 20/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0401\n",
      "validate accuracy:\n",
      " [0.63691808 0.45017982 0.500503   0.61689532 0.61462992 0.49303809\n",
      " 0.47181761 0.7037037  0.89763999 0.9006983  0.86129171 0.50756395\n",
      " 0.58707863 0.74850214 0.74759585 0.5850302  0.50452107] @epoch 19\n",
      "Epoch 020: val_acc improved from 0.633 to 0.637\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0401 - val_loss: 0.0428\n",
      "Counter: 0, Global: 0.636918080970645, MyBest: 0.6331378575414419\n",
      "\n",
      "Epoch 21/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0399\n",
      "validate accuracy:\n",
      " [0.63235513 0.43769833 0.48054996 0.62310469 0.63021207 0.48297265\n",
      " 0.44711843 0.71354169 0.89317137 0.89371508 0.85609019 0.49950516\n",
      " 0.58595508 0.74892014 0.74968642 0.58208114 0.49335971] @epoch 20\n",
      "Epoch 021: val_acc did not improve from 0.637\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0399 - val_loss: 0.0429\n",
      "Counter: 0, Global: 0.636918080970645, MyBest: 0.636918080970645\n",
      "\n",
      "Epoch 22/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0396\n",
      "validate accuracy:\n",
      " [0.61041462 0.43515971 0.45606974 0.58223826 0.58649546 0.45143431\n",
      " 0.44458517 0.68663192 0.87976539 0.88519555 0.8464095  0.4648664\n",
      " 0.55337077 0.72175002 0.72097564 0.56199974 0.48968634] @epoch 21\n",
      "Epoch 022: val_acc did not improve from 0.637\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0396 - val_loss: 0.0436\n",
      "Counter: 1, Global: 0.636918080970645, MyBest: 0.636918080970645\n",
      "\n",
      "Epoch 23/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0393\n",
      "validate accuracy:\n",
      " [0.62840428 0.44108313 0.49463448 0.61732852 0.61506277 0.47710115\n",
      " 0.46295124 0.70413774 0.88199973 0.88784915 0.85811299 0.50063622\n",
      " 0.57261235 0.72913474 0.73519164 0.57197022 0.50466233] @epoch 22\n",
      "Epoch 023: val_acc did not improve from 0.637\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0393 - val_loss: 0.0434\n",
      "Counter: 2, Global: 0.636918080970645, MyBest: 0.636918080970645\n",
      "\n",
      "Epoch 24/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0391\n",
      "validate accuracy:\n",
      " [0.64634509 0.44764122 0.49849096 0.63566786 0.63295341 0.5106526\n",
      " 0.46822885 0.71296299 0.89652282 0.90055865 0.8663488  0.52679205\n",
      " 0.60266852 0.7583949  0.7616725  0.59837103 0.52359426] @epoch 23\n",
      "Epoch 024: val_acc improved from 0.637 to 0.646\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0391 - val_loss: 0.0425\n",
      "Counter: 3, Global: 0.6463450882583857, MyBest: 0.636918080970645\n",
      "\n",
      "Epoch 25/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0387\n",
      "validate accuracy:\n",
      " [0.652795   0.46710387 0.51559359 0.63754511 0.64377433 0.52122128\n",
      " 0.47350645 0.72337961 0.89987433 0.89832401 0.86866057 0.53470945\n",
      " 0.60744381 0.76132089 0.74968642 0.60103917 0.54153717] @epoch 24\n",
      "Epoch 025: val_acc improved from 0.646 to 0.653\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0387 - val_loss: 0.0423\n",
      "Counter: 0, Global: 0.6527950037270784, MyBest: 0.6463450882583857\n",
      "\n",
      "Epoch 26/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0386\n",
      "validate accuracy:\n",
      " [0.65777218 0.47281575 0.51928234 0.64693141 0.65041119 0.52122128\n",
      " 0.48448384 0.72685188 0.89889681 0.90055865 0.87097239 0.54078889\n",
      " 0.61264044 0.76773024 0.76627177 0.61030751 0.53419048] @epoch 25\n",
      "Epoch 026: val_acc improved from 0.653 to 0.658\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0386 - val_loss: 0.0421\n",
      "Counter: 0, Global: 0.6577721796929836, MyBest: 0.6527950037270784\n",
      "\n",
      "Epoch 27/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0384\n",
      "validate accuracy:\n",
      " [0.65963904 0.47196954 0.52548623 0.64592057 0.64738131 0.5143432\n",
      " 0.48870593 0.73234951 0.90504122 0.90698326 0.87877476 0.53485084\n",
      " 0.61278093 0.77177095 0.76682925 0.60370737 0.54732978] @epoch 26\n",
      "Epoch 027: val_acc improved from 0.658 to 0.660\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0384 - val_loss: 0.0422\n",
      "Counter: 0, Global: 0.6596390418708324, MyBest: 0.6577721796929836\n",
      "\n",
      "Epoch 28/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0381\n",
      "validate accuracy:\n",
      " [0.65689021 0.47514281 0.5197854  0.64635378 0.64478433 0.51082033\n",
      " 0.48511717 0.72236687 0.9040637  0.90405029 0.86851609 0.52990246\n",
      " 0.61544943 0.76522225 0.76655054 0.61185229 0.54026562] @epoch 27\n",
      "Epoch 028: val_acc did not improve from 0.660\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0381 - val_loss: 0.0422\n",
      "Counter: 0, Global: 0.6596390418708324, MyBest: 0.6596390418708324\n",
      "\n",
      "Epoch 29/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0379\n",
      "validate accuracy:\n",
      " [0.66054868 0.47154644 0.50318581 0.64332128 0.64723706 0.5143432\n",
      " 0.50094998 0.72511572 0.90587908 0.91033518 0.87935269 0.54319245\n",
      " 0.62162924 0.77121359 0.76905924 0.61409914 0.54831874] @epoch 28\n",
      "Epoch 029: val_acc improved from 0.660 to 0.661\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0379 - val_loss: 0.0420\n",
      "Counter: 1, Global: 0.6605486776679754, MyBest: 0.6596390418708324\n",
      "\n",
      "Epoch 30/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0377\n",
      "validate accuracy:\n",
      " [0.66525554 0.48360482 0.52414489 0.6566065  0.64781415 0.52105349\n",
      " 0.49799451 0.74204284 0.90797377 0.90712291 0.8767519  0.54220277\n",
      " 0.62401688 0.7781803  0.76822299 0.61704814 0.5493077 ] @epoch 29\n",
      "Epoch 030: val_acc improved from 0.661 to 0.665\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0377 - val_loss: 0.0420\n",
      "Counter: 0, Global: 0.6652555353939533, MyBest: 0.6605486776679754\n",
      "\n",
      "Epoch 31/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0375\n",
      "validate accuracy:\n",
      " [0.65765894 0.48000845 0.51693493 0.65342963 0.63915741 0.51870489\n",
      " 0.48363945 0.73668981 0.9042033  0.90572625 0.87241727 0.5375371\n",
      " 0.60786515 0.7557475  0.75442511 0.61466086 0.5413959 ] @epoch 30\n",
      "Epoch 031: val_acc did not improve from 0.665\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0375 - val_loss: 0.0421\n",
      "Counter: 0, Global: 0.6652555353939533, MyBest: 0.6652555353939533\n",
      "\n",
      "Epoch 32/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0373\n",
      "validate accuracy:\n",
      " [0.66368104 0.48254707 0.52045608 0.65848374 0.65041119 0.52726054\n",
      " 0.48912814 0.7277199  0.90364474 0.90418994 0.87227279 0.54333383\n",
      " 0.62036514 0.7781803  0.76891989 0.6209802  0.5510031 ] @epoch 31\n",
      "Epoch 032: val_acc did not improve from 0.665\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0373 - val_loss: 0.0420\n",
      "Counter: 1, Global: 0.6652555353939533, MyBest: 0.6652555353939533\n",
      "\n",
      "Epoch 33/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0371\n",
      "validate accuracy:\n",
      " [0.66458069 0.48635498 0.52330649 0.64563179 0.64348578 0.51769835\n",
      " 0.49609458 0.72569442 0.90699625 0.90586591 0.8771854  0.54898912\n",
      " 0.62879211 0.77595097 0.77686411 0.62168235 0.55269849] @epoch 32\n",
      "Epoch 033: val_acc did not improve from 0.665\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0371 - val_loss: 0.0421\n",
      "Counter: 2, Global: 0.6652555353939533, MyBest: 0.6652555353939533\n",
      "\n",
      "Epoch 34/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0369\n",
      "validate accuracy:\n",
      " [0.66147012 0.47006559 0.517941   0.65487367 0.64391863 0.52541518\n",
      " 0.48828372 0.72873265 0.90392405 0.90223461 0.86736023 0.54503042\n",
      " 0.6230337  0.76926291 0.77003485 0.61859292 0.55481774] @epoch 33\n",
      "Epoch 034: val_acc did not improve from 0.665\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0369 - val_loss: 0.0422\n",
      "Counter: 3, Global: 0.6652555353939533, MyBest: 0.6652555353939533\n",
      "\n",
      "Epoch 35/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0367\n",
      "validate accuracy:\n",
      " [0.67007617 0.48275861 0.52816904 0.65833932 0.65950078 0.53715819\n",
      " 0.50960523 0.74508101 0.90601873 0.90991622 0.87964165 0.54715115\n",
      " 0.63132024 0.77748364 0.7691986  0.62406963 0.5558067 ] @epoch 34\n",
      "Epoch 035: val_acc improved from 0.665 to 0.670\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0367 - val_loss: 0.0418\n",
      "Counter: 4, Global: 0.6700761709362268, MyBest: 0.6652555353939533\n",
      "\n",
      "Epoch 36/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0366\n",
      "validate accuracy:\n",
      " [0.65647791 0.47175798 0.51207244 0.63783395 0.63497329 0.51165915\n",
      " 0.48300612 0.73394096 0.90629798 0.90642458 0.87920821 0.53386116\n",
      " 0.61685395 0.75282151 0.75261325 0.615363   0.554959  ] @epoch 35\n",
      "Epoch 036: val_acc did not improve from 0.670\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0366 - val_loss: 0.0424\n",
      "Counter: 0, Global: 0.6700761709362268, MyBest: 0.6700761709362268\n",
      "\n",
      "Epoch 37/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0366\n",
      "validate accuracy:\n",
      " [0.66524157 0.48360482 0.51810867 0.65068591 0.64290869 0.52323437\n",
      " 0.4922947  0.72931135 0.90797377 0.90740222 0.88152003 0.55803758\n",
      " 0.62092698 0.7781803  0.76878047 0.61915463 0.56174058] @epoch 36\n",
      "Epoch 037: val_acc did not improve from 0.670\n",
      "576/576 [==============================] - 139s 241ms/step - loss: 0.0366 - val_loss: 0.0421\n",
      "Counter: 1, Global: 0.6700761709362268, MyBest: 0.6700761709362268\n",
      "\n",
      "Epoch 38/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0363\n",
      "validate accuracy:\n",
      " [0.6694883  0.49566323 0.53286386 0.64967507 0.65127689 0.53212547\n",
      " 0.51002747 0.72699654 0.90490156 0.90879887 0.87964165 0.55252367\n",
      " 0.6327247  0.77441829 0.77519166 0.62055892 0.56442499] @epoch 37\n",
      "Epoch 038: val_acc did not improve from 0.670\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0363 - val_loss: 0.0418\n",
      "Counter: 2, Global: 0.6700761709362268, MyBest: 0.6700761709362268\n",
      "\n",
      "Epoch 39/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0362\n",
      "validate accuracy:\n",
      " [0.66438893 0.48656654 0.52246815 0.65241879 0.64262009 0.53078341\n",
      " 0.49947223 0.72858799 0.90113115 0.90516758 0.87140584 0.54361659\n",
      " 0.62542135 0.77177095 0.7717073  0.61845249 0.55863237] @epoch 38\n",
      "Epoch 039: val_acc did not improve from 0.670\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0362 - val_loss: 0.0421\n",
      "Counter: 3, Global: 0.6700761709362268, MyBest: 0.6700761709362268\n",
      "\n",
      "Epoch 40/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0360\n",
      "validate accuracy:\n",
      " [0.66769215 0.48656654 0.52883971 0.65848374 0.65733659 0.53296429\n",
      " 0.49672788 0.73292822 0.90308619 0.90544695 0.87227279 0.54785806\n",
      " 0.62879211 0.77232826 0.77365851 0.62828255 0.55750209] @epoch 39\n",
      "Epoch 040: val_acc did not improve from 0.670\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0360 - val_loss: 0.0421\n",
      "Counter: 4, Global: 0.6700761709362268, MyBest: 0.6700761709362268\n",
      "\n",
      "Epoch 41/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0359\n",
      "validate accuracy:\n",
      " [0.66210852 0.47725829 0.52431256 0.64548737 0.64507288 0.51451099\n",
      " 0.49123919 0.73292822 0.90504122 0.90530723 0.87487358 0.55337197\n",
      " 0.62092698 0.76954156 0.75484324 0.62533355 0.55368745] @epoch 40\n",
      "Epoch 041: val_acc did not improve from 0.670\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0359 - val_loss: 0.0422\n",
      "Counter: 5, Global: 0.6700761709362268, MyBest: 0.6700761709362268\n",
      "\n",
      "Epoch 42/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0357\n",
      "validate accuracy:\n",
      " [0.67294129 0.48825893 0.52951038 0.65444046 0.66036648 0.52306658\n",
      " 0.49862781 0.73220485 0.91048735 0.9131285  0.88412076 0.56227911\n",
      " 0.63820225 0.78695834 0.78369337 0.63389975 0.56781578] @epoch 41\n",
      "Epoch 042: val_acc improved from 0.670 to 0.673\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0357 - val_loss: 0.0420\n",
      "Counter: 6, Global: 0.672941293567419, MyBest: 0.6700761709362268\n",
      "\n",
      "Epoch 43/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0356\n",
      "validate accuracy:\n",
      " [0.66700579 0.48445103 0.52900738 0.65516245 0.64940125 0.52759606\n",
      " 0.49715009 0.73148149 0.90350509 0.90921789 0.88021964 0.55238229\n",
      " 0.63244385 0.77135295 0.76445991 0.62322706 0.5610342 ] @epoch 42\n",
      "Epoch 043: val_acc did not improve from 0.673\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0356 - val_loss: 0.0420\n",
      "Counter: 0, Global: 0.672941293567419, MyBest: 0.672941293567419\n",
      "\n",
      "Epoch 44/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0355\n",
      "validate accuracy:\n",
      " [0.66481658 0.48275861 0.51425219 0.63148016 0.63179916 0.51518202\n",
      " 0.50284988 0.71469909 0.90657729 0.91243017 0.88542116 0.56100667\n",
      " 0.62837076 0.78068829 0.77895468 0.62715912 0.56343597] @epoch 43\n",
      "Epoch 044: val_acc did not improve from 0.673\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0355 - val_loss: 0.0421\n",
      "Counter: 1, Global: 0.672941293567419, MyBest: 0.672941293567419\n",
      "\n",
      "Epoch 45/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0354\n",
      "validate accuracy:\n",
      " [0.6608848  0.47027713 0.50821596 0.64707583 0.6489684  0.52558297\n",
      " 0.4870171  0.72424769 0.89442813 0.89972067 0.87082791 0.54715115\n",
      " 0.62598312 0.76912361 0.76027876 0.62701869 0.56823963] @epoch 44\n",
      "Epoch 045: val_acc did not improve from 0.673\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0354 - val_loss: 0.0424\n",
      "Counter: 2, Global: 0.672941293567419, MyBest: 0.672941293567419\n",
      "\n",
      "Epoch 46/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0353\n",
      "validate accuracy:\n",
      " [0.6692004  0.49079755 0.518444   0.652852   0.6498341  0.52591848\n",
      " 0.50327212 0.72164351 0.90434295 0.90642458 0.87805229 0.55775481\n",
      " 0.63679773 0.78263897 0.7790941  0.62968683 0.56965244] @epoch 45\n",
      "Epoch 046: val_acc did not improve from 0.673\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0353 - val_loss: 0.0420\n",
      "Counter: 3, Global: 0.672941293567419, MyBest: 0.672941293567419\n",
      "\n",
      "Epoch 47/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0351\n",
      "validate accuracy:\n",
      " [0.67736418 0.49100909 0.54040915 0.66166067 0.66397345 0.54135209\n",
      " 0.50791639 0.73480904 0.90755481 0.91019553 0.881809   0.5646826\n",
      " 0.64199436 0.78654033 0.77951217 0.64274681 0.58166146] @epoch 46\n",
      "Epoch 047: val_acc improved from 0.673 to 0.677\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0351 - val_loss: 0.0420\n",
      "Counter: 4, Global: 0.6773641835898161, MyBest: 0.672941293567419\n",
      "\n",
      "Epoch 48/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0351\n",
      "validate accuracy:\n",
      " [0.67053364 0.4819124  0.5350436  0.65155232 0.65459532 0.53715819\n",
      " 0.50643867 0.7404514  0.9044826  0.9068436  0.87834126 0.55973423\n",
      " 0.6289326  0.76800889 0.76724738 0.63305718 0.57473862] @epoch 47\n",
      "Epoch 048: val_acc did not improve from 0.677\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0351 - val_loss: 0.0419\n",
      "Counter: 0, Global: 0.6773641835898161, MyBest: 0.6773641835898161\n",
      "\n",
      "Epoch 49/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0348\n",
      "validate accuracy:\n",
      " [0.67482749 0.4893167  0.52917504 0.66223824 0.66195357 0.53195775\n",
      " 0.50664979 0.73885995 0.91230273 0.9131285  0.8825314  0.56581366\n",
      " 0.64522469 0.77971298 0.78006971 0.6371296  0.56117547] @epoch 48\n",
      "Epoch 049: val_acc did not improve from 0.677\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0348 - val_loss: 0.0421\n",
      "Counter: 1, Global: 0.6773641835898161, MyBest: 0.6773641835898161\n",
      "\n",
      "Epoch 50/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0348\n",
      "validate accuracy:\n",
      " [0.67784979 0.49693251 0.53839707 0.6675812  0.66469485 0.54806238\n",
      " 0.51319402 0.74131942 0.91188383 0.90893853 0.88195348 0.55365473\n",
      " 0.6417135  0.78445035 0.77644598 0.63951695 0.57685786] @epoch 49\n",
      "Epoch 050: val_acc improved from 0.677 to 0.678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0348 - val_loss: 0.0419\n",
      "Counter: 2, Global: 0.677849791944027, MyBest: 0.6773641835898161\n",
      "\n",
      "Epoch 51/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0346\n",
      "validate accuracy:\n",
      " [0.67337005 0.48868203 0.53085178 0.65198559 0.65560526 0.54034561\n",
      " 0.50791639 0.74088544 0.90573943 0.90977651 0.87805229 0.56298602\n",
      " 0.63623595 0.77581161 0.77365851 0.6390956  0.57629275] @epoch 50\n",
      "Epoch 051: val_acc did not improve from 0.678\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0346 - val_loss: 0.0421\n",
      "Counter: 0, Global: 0.677849791944027, MyBest: 0.677849791944027\n",
      "\n",
      "Epoch 52/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0345\n",
      "validate accuracy:\n",
      " [0.66741578 0.47831607 0.53403753 0.64476532 0.64651567 0.52189231\n",
      " 0.49292803 0.72410303 0.90895128 0.91047484 0.88123101 0.55931008\n",
      " 0.63356739 0.77901632 0.77198607 0.6315124  0.56004518] @epoch 51\n",
      "Epoch 052: val_acc did not improve from 0.678\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0345 - val_loss: 0.0423\n",
      "Counter: 1, Global: 0.677849791944027, MyBest: 0.677849791944027\n",
      "\n",
      "Epoch 53/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0345\n",
      "validate accuracy:\n",
      " [0.67137503 0.4888936  0.53118712 0.64967507 0.65243113 0.53229326\n",
      " 0.51087189 0.72974539 0.90783411 0.9107542  0.88238692 0.55832034\n",
      " 0.62935394 0.78194231 0.77644598 0.63614661 0.56371856] @epoch 52\n",
      "Epoch 053: val_acc did not improve from 0.678\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0345 - val_loss: 0.0420\n",
      "Counter: 2, Global: 0.677849791944027, MyBest: 0.677849791944027\n",
      "\n",
      "Epoch 54/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0344\n",
      "validate accuracy:\n",
      " [0.66627253 0.47662365 0.52330649 0.65617329 0.64940125 0.53162223\n",
      " 0.49398354 0.7277199  0.9044826  0.90656424 0.8838318  0.55577546\n",
      " 0.63932586 0.76243556 0.76487803 0.62729955 0.55693698] @epoch 53\n",
      "Epoch 054: val_acc did not improve from 0.678\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0344 - val_loss: 0.0424\n",
      "Counter: 3, Global: 0.677849791944027, MyBest: 0.677849791944027\n",
      "\n",
      "Epoch 55/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0343\n",
      "validate accuracy:\n",
      " [0.67595185 0.49418235 0.53755867 0.6566065  0.66209781 0.53732598\n",
      " 0.50496095 0.73741317 0.9044826  0.91298884 0.88137555 0.56977236\n",
      " 0.63764048 0.77985233 0.78202093 0.63769132 0.57925969] @epoch 54\n",
      "Epoch 055: val_acc did not improve from 0.678\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0343 - val_loss: 0.0421\n",
      "Counter: 4, Global: 0.677849791944027, MyBest: 0.677849791944027\n",
      "\n",
      "Epoch 56/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0341\n",
      "validate accuracy:\n",
      " [0.67412806 0.48677808 0.53135478 0.67263538 0.65647095 0.53782922\n",
      " 0.51066077 0.74030674 0.90629798 0.90768158 0.88224244 0.55648237\n",
      " 0.63553369 0.78598303 0.77365851 0.63488275 0.56725061] @epoch 55\n",
      "Epoch 056: val_acc did not improve from 0.678\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0341 - val_loss: 0.0420\n",
      "Counter: 5, Global: 0.677849791944027, MyBest: 0.677849791944027\n",
      "\n",
      "Epoch 57/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0341\n",
      "validate accuracy:\n",
      " [0.67125738 0.49841338 0.52598929 0.65198559 0.64680421 0.53363532\n",
      " 0.50812751 0.72989005 0.9042033  0.90837991 0.88209796 0.56284463\n",
      " 0.63553369 0.76982027 0.77602786 0.63600618 0.57035887] @epoch 56\n",
      "Epoch 057: val_acc did not improve from 0.678\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0341 - val_loss: 0.0423\n",
      "Counter: 6, Global: 0.677849791944027, MyBest: 0.677849791944027\n",
      "\n",
      "Epoch 58/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0340\n",
      "validate accuracy:\n",
      " [0.67410769 0.49143219 0.53051645 0.65617329 0.65921223 0.53648716\n",
      " 0.51340508 0.73538774 0.90504122 0.9094972  0.87848574 0.56595504\n",
      " 0.63918537 0.77971298 0.77491289 0.63642746 0.57389092] @epoch 57\n",
      "Epoch 058: val_acc did not improve from 0.678\n",
      "576/576 [==============================] - 139s 241ms/step - loss: 0.0340 - val_loss: 0.0422\n",
      "Counter: 7, Global: 0.677849791944027, MyBest: 0.677849791944027\n",
      "\n",
      "Epoch 058: Updating Learning rate.. New value is 0.001340\n",
      "Epoch 59/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0325\n",
      "validate accuracy:\n",
      " [0.68707662 0.50412524 0.54812205 0.6727798  0.67277449 0.55460495\n",
      " 0.52754909 0.74739581 0.91076666 0.9134078  0.88686603 0.57571048\n",
      " 0.65814608 0.79336768 0.78871077 0.65285772 0.58604127] @epoch 58\n",
      "Epoch 059: val_acc improved from 0.678 to 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0325 - val_loss: 0.0417\n",
      "Counter: 0, Global: 0.6870766207575798, MyBest: 0.677849791944027\n",
      "\n",
      "Epoch 60/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0320\n",
      "validate accuracy:\n",
      " [0.68629137 0.50349057 0.5449363  0.67624551 0.67768002 0.55091429\n",
      " 0.51973826 0.74609375 0.90937018 0.91424578 0.88672155 0.57783121\n",
      " 0.65365171 0.79253167 0.78606272 0.65468335 0.58646512] @epoch 59\n",
      "Epoch 060: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 241ms/step - loss: 0.0320 - val_loss: 0.0418\n",
      "Counter: 0, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 61/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0317\n",
      "validate accuracy:\n",
      " [0.68559083 0.50306749 0.54946345 0.67624551 0.6721974  0.54755914\n",
      " 0.51804942 0.74623841 0.91062701 0.9131285  0.88686603 0.57726568\n",
      " 0.65126407 0.79197437 0.78745645 0.65299815 0.58505225] @epoch 60\n",
      "Epoch 061: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0317 - val_loss: 0.0419\n",
      "Counter: 1, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 62/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0315\n",
      "validate accuracy:\n",
      " [0.68408694 0.49735561 0.54627764 0.67292416 0.66830182 0.54772687\n",
      " 0.51804942 0.74594909 0.90923053 0.91382682 0.88787746 0.57457936\n",
      " 0.65491575 0.78835166 0.78578395 0.65215558 0.58208531] @epoch 61\n",
      "Epoch 062: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0315 - val_loss: 0.0420\n",
      "Counter: 2, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 63/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0314\n",
      "validate accuracy:\n",
      " [0.68380162 0.50031734 0.54879278 0.66888088 0.67234164 0.55007547\n",
      " 0.51467174 0.74189812 0.9099288  0.91550279 0.88686603 0.57316554\n",
      " 0.64901686 0.78960568 0.78494775 0.65075129 0.58406329] @epoch 62\n",
      "Epoch 063: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0314 - val_loss: 0.0420\n",
      "Counter: 3, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 64/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0313\n",
      "validate accuracy:\n",
      " [0.68207433 0.49947113 0.54309189 0.66512638 0.66988891 0.54722363\n",
      " 0.51446062 0.73943865 0.90559977 0.9134078  0.88628811 0.57528633\n",
      " 0.64971912 0.7875157  0.78369337 0.65075129 0.58222663] @epoch 63\n",
      "Epoch 064: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0313 - val_loss: 0.0422\n",
      "Counter: 4, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 65/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0312\n",
      "validate accuracy:\n",
      " [0.68184334 0.50031734 0.53973842 0.66570395 0.66758043 0.55041099\n",
      " 0.51678276 0.73929399 0.90629798 0.91354746 0.8842653  0.57245862\n",
      " 0.64985955 0.78849101 0.77993029 0.65061086 0.58420455] @epoch 64\n",
      "Epoch 065: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0312 - val_loss: 0.0422\n",
      "Counter: 5, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 66/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0311\n",
      "validate accuracy:\n",
      " [0.6825076  0.49947113 0.54594231 0.66527075 0.66700333 0.5483979\n",
      " 0.51509392 0.73784721 0.90839267 0.9146648  0.88729954 0.57288277\n",
      " 0.65224719 0.78667969 0.78257841 0.65384078 0.58250916] @epoch 65\n",
      "Epoch 066: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0311 - val_loss: 0.0423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 6, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 67/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0310\n",
      "validate accuracy:\n",
      " [0.68180069 0.50306749 0.53990608 0.6675812  0.66281921 0.54185539\n",
      " 0.52227145 0.73726851 0.90853232 0.9146648  0.88701057 0.57019651\n",
      " 0.65070224 0.787655   0.78341466 0.64780229 0.58406329] @epoch 66\n",
      "Epoch 067: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0310 - val_loss: 0.0423\n",
      "Counter: 7, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 067: Updating Learning rate.. New value is 0.000268\n",
      "Epoch 68/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0306\n",
      "validate accuracy:\n",
      " [0.68293054 0.50306749 0.5449363  0.66916966 0.66512769 0.54521054\n",
      " 0.5191049  0.74131942 0.90978914 0.91717875 0.88672155 0.57217586\n",
      " 0.65365171 0.78654033 0.78062719 0.65131301 0.58095509] @epoch 67\n",
      "Epoch 068: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0306 - val_loss: 0.0423\n",
      "Counter: 0, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 69/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0305\n",
      "validate accuracy:\n",
      " [0.68298409 0.50349057 0.54426557 0.67176896 0.6636849  0.544204\n",
      " 0.51973826 0.74131942 0.90853232 0.91550279 0.88701057 0.57373106\n",
      " 0.65238762 0.78584367 0.78229964 0.65089172 0.58307433] @epoch 68\n",
      "Epoch 069: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0305 - val_loss: 0.0424\n",
      "Counter: 1, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 70/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0305\n",
      "validate accuracy:\n",
      " [0.68225286 0.50200975 0.54376256 0.6701805  0.66469485 0.54135209\n",
      " 0.51826048 0.74001735 0.90811342 0.91606146 0.88643259 0.57486212\n",
      " 0.65168542 0.78542566 0.78104532 0.64878529 0.58335686] @epoch 69\n",
      "Epoch 070: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0305 - val_loss: 0.0424\n",
      "Counter: 2, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 71/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0304\n",
      "validate accuracy:\n",
      " [0.68167451 0.50116354 0.54309189 0.66945851 0.66166496 0.54470724\n",
      " 0.51488286 0.73943865 0.90825301 0.91662014 0.88628811 0.57260001\n",
      " 0.65294945 0.784729   0.77993029 0.64836401 0.58265048] @epoch 70\n",
      "Epoch 071: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0304 - val_loss: 0.0425\n",
      "Counter: 3, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 72/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0304\n",
      "validate accuracy:\n",
      " [0.68179038 0.50222129 0.54309189 0.66931409 0.66455054 0.5465526\n",
      " 0.51509392 0.73900461 0.90825301 0.91675979 0.88527668 0.5718931\n",
      " 0.64985955 0.78667969 0.77937281 0.64990872 0.58081377] @epoch 71\n",
      "Epoch 072: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0304 - val_loss: 0.0424\n",
      "Counter: 4, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 73/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0304\n",
      "validate accuracy:\n",
      " [0.68091272 0.49841338 0.54191816 0.66916966 0.66570479 0.544204\n",
      " 0.51446062 0.73654515 0.90825301 0.91689944 0.88571018 0.56920683\n",
      " 0.64887643 0.7848683  0.77937281 0.65061086 0.58038992] @epoch 72\n",
      "Epoch 073: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0304 - val_loss: 0.0425\n",
      "Counter: 5, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 74/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0304\n",
      "validate accuracy:\n",
      " [0.68136509 0.50052887 0.54191816 0.66714799 0.66498339 0.54437172\n",
      " 0.5136162  0.73914933 0.90769446 0.91564244 0.88628811 0.57062066\n",
      " 0.65126407 0.78500766 0.77951217 0.65271729 0.58137894] @epoch 73\n",
      "Epoch 074: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0304 - val_loss: 0.0425\n",
      "Counter: 6, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 75/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0303\n",
      "validate accuracy:\n",
      " [0.68050549 0.49820182 0.54091215 0.66483754 0.66296351 0.5437007\n",
      " 0.51572728 0.73582178 0.90811342 0.91634077 0.88628811 0.57217586\n",
      " 0.65042132 0.78319633 0.77993029 0.64892572 0.58053124] @epoch 74\n",
      "Epoch 075: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0303 - val_loss: 0.0425\n",
      "Counter: 7, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 075: Updating Learning rate.. New value is 0.000054\n",
      "Epoch 76/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68081238 0.49947113 0.54191816 0.66801447 0.66224211 0.54336518\n",
      " 0.51403844 0.73987269 0.90699625 0.91578209 0.88513219 0.5710448\n",
      " 0.65028089 0.78319633 0.78006971 0.64934701 0.58222663] @epoch 75\n",
      "Epoch 076: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0302 - val_loss: 0.0425\n",
      "Counter: 0, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 77/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68062764 0.49735561 0.54158282 0.66729242 0.6644063  0.54202312\n",
      " 0.51488286 0.73842591 0.90741515 0.91634077 0.88599914 0.57118618\n",
      " 0.64915729 0.78361434 0.7809059  0.64934701 0.58010739] @epoch 76\n",
      "Epoch 077: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0302 - val_loss: 0.0425\n",
      "Counter: 1, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 78/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68101413 0.49820182 0.54175049 0.66844767 0.66267496 0.54252642\n",
      " 0.51530504 0.74001735 0.90825301 0.91648042 0.88643259 0.57019651\n",
      " 0.65028089 0.78375363 0.78132403 0.64990872 0.5806725 ] @epoch 77\n",
      "Epoch 078: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0302 - val_loss: 0.0425\n",
      "Counter: 2, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 79/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68076582 0.49968266 0.54007375 0.66830325 0.6644063  0.54219091\n",
      " 0.51509392 0.73943865 0.9071359  0.91648042 0.8855657  0.5686413\n",
      " 0.65014046 0.784311   0.78062719 0.64920658 0.58095509] @epoch 78\n",
      "Epoch 079: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0302 - val_loss: 0.0425\n",
      "Counter: 3, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 80/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68080641 0.49883646 0.54074448 0.66916966 0.6644063  0.54302967\n",
      " 0.51446062 0.74016201 0.90755481 0.91648042 0.88513219 0.56849992\n",
      " 0.64985955 0.78361434 0.77895468 0.64934701 0.58265048] @epoch 79\n",
      "Epoch 080: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0302 - val_loss: 0.0425\n",
      "Counter: 4, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 81/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68068926 0.49925956 0.54141515 0.66859204 0.66455054 0.54403621\n",
      " 0.5136162  0.73828125 0.9072755  0.91634077 0.8855657  0.56991374\n",
      " 0.64901686 0.78347498 0.77797908 0.65004915 0.58166146] @epoch 80\n",
      "Epoch 081: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0302 - val_loss: 0.0425\n",
      "Counter: 5, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 82/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302- ETA: 0s - loss: 0.03\n",
      "validate accuracy:\n",
      " [0.68034954 0.49968266 0.54024142 0.66714799 0.6644063  0.54202312\n",
      " 0.51382732 0.73799187 0.90797377 0.91606146 0.8855657  0.56835854\n",
      " 0.65014046 0.78375363 0.7783972  0.64906615 0.58095509] @epoch 81\n",
      "Epoch 082: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0302 - val_loss: 0.0426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 6, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 83/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68079392 0.49714407 0.54325956 0.66873646 0.66556054 0.54185539\n",
      " 0.51509392 0.73857063 0.90783411 0.91634077 0.88455427 0.56835854\n",
      " 0.65028089 0.78347498 0.77979094 0.65075129 0.58109635] @epoch 82\n",
      "Epoch 083: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 7, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 083: Updating Learning rate.. New value is 0.000011\n",
      "Epoch 84/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68055684 0.49947113 0.54074448 0.66815883 0.66397345 0.54151988\n",
      " 0.51340508 0.73871529 0.90741515 0.91620111 0.88513219 0.56892407\n",
      " 0.65014046 0.784729   0.77951217 0.64934701 0.5815202 ] @epoch 83\n",
      "Epoch 084: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 0, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 85/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68061877 0.49947113 0.54107982 0.66873646 0.66411775 0.54151988\n",
      " 0.51277179 0.73813659 0.90825301 0.91662014 0.88571018 0.57005513\n",
      " 0.64985955 0.78403234 0.7790941  0.64948744 0.58095509] @epoch 84\n",
      "Epoch 085: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 1, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 86/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68031702 0.49799028 0.54074448 0.66859204 0.6644063  0.54118437\n",
      " 0.51277179 0.73799187 0.90769446 0.91592181 0.88542116 0.56963098\n",
      " 0.65042132 0.78445035 0.77867597 0.64878529 0.58038992] @epoch 85\n",
      "Epoch 086: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 2, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 87/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68037123 0.49904802 0.54158282 0.66801447 0.66224211 0.54084885\n",
      " 0.5129829  0.73770255 0.90741515 0.91648042 0.88599914 0.56963098\n",
      " 0.64971912 0.78417164 0.77923346 0.64920658 0.58166146] @epoch 86\n",
      "Epoch 087: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 3, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 88/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68067244 0.49756718 0.54242122 0.66916966 0.66469485 0.54202312\n",
      " 0.51340508 0.73813659 0.90755481 0.91662014 0.88527668 0.56934822\n",
      " 0.65014046 0.78514701 0.7790941  0.64948744 0.5806725 ] @epoch 87\n",
      "Epoch 088: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 4, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 89/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68038194 0.49756718 0.54175049 0.66815883 0.66411775 0.54118437\n",
      " 0.5142495  0.73799187 0.90839267 0.91564244 0.88498771 0.5694896\n",
      " 0.6494382  0.784311   0.7790941  0.64962786 0.58010739] @epoch 88\n",
      "Epoch 089: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 5, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 90/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.6803245  0.49862492 0.54040915 0.66787004 0.66253066 0.54101658\n",
      " 0.51636058 0.73726851 0.90769446 0.91620111 0.88498771 0.5694896\n",
      " 0.65042132 0.78389299 0.77783972 0.64934701 0.58123761] @epoch 89\n",
      "Epoch 090: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 6, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 91/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0301\n",
      "validate accuracy:\n",
      " [0.68042653 0.49947113 0.54074448 0.66888088 0.6635406  0.54219091\n",
      " 0.51446062 0.73755789 0.90769446 0.91606146 0.88498771 0.56906545\n",
      " 0.64957863 0.784311   0.77811849 0.64934701 0.58081377] @epoch 90\n",
      "Epoch 091: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0301 - val_loss: 0.0426\n",
      "Counter: 7, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 091: Updating Learning rate.. New value is 0.000002\n",
      "Epoch 92/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68030264 0.49777871 0.54141515 0.66859204 0.66281921 0.54202312\n",
      " 0.51256067 0.73842591 0.90811342 0.91648042 0.88498771 0.5686413\n",
      " 0.64999998 0.78333563 0.77993029 0.64906615 0.5806725 ] @epoch 91\n",
      "Epoch 092: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 0, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 93/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0301\n",
      "validate accuracy:\n",
      " [0.68052876 0.5001058  0.54208583 0.66974729 0.66455054 0.54017782\n",
      " 0.51382732 0.73770255 0.90825301 0.91634077 0.88513219 0.56906545\n",
      " 0.64999998 0.78291768 0.77881533 0.64906615 0.5806725 ] @epoch 92\n",
      "Epoch 093: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0301 - val_loss: 0.0426\n",
      "Counter: 1, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 94/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0301\n",
      "validate accuracy:\n",
      " [0.68064037 0.49904802 0.54309189 0.66931409 0.6635406  0.54286194\n",
      " 0.51403844 0.73741317 0.90783411 0.91634077 0.88484323 0.56849992\n",
      " 0.64859551 0.7848683  0.77923346 0.64990872 0.58081377] @epoch 93\n",
      "Epoch 094: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0301 - val_loss: 0.0426\n",
      "Counter: 2, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 95/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68087752 0.49841338 0.54258889 0.66931409 0.66483915 0.54269415\n",
      " 0.51319402 0.73813659 0.90839267 0.91634077 0.88455427 0.56963098\n",
      " 0.65028089 0.78598303 0.77937281 0.64892572 0.58137894] @epoch 94\n",
      "Epoch 095: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 139s 242ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 3, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 96/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0301\n",
      "validate accuracy:\n",
      " [0.68065002 0.49777871 0.54208583 0.66873646 0.6635406  0.54202312\n",
      " 0.51403844 0.73784721 0.90783411 0.91648042 0.88571018 0.56892407\n",
      " 0.65056181 0.784311   0.77895468 0.64934701 0.58222663] @epoch 95\n",
      "Epoch 096: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 242ms/step - loss: 0.0301 - val_loss: 0.0426\n",
      "Counter: 4, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 97/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.68052792 0.49820182 0.54292423 0.6701805  0.66310775 0.54118437\n",
      " 0.51446062 0.73828125 0.90783411 0.91592181 0.88484323 0.56878269\n",
      " 0.64915729 0.78403234 0.77979094 0.64822358 0.5815202 ] @epoch 96\n",
      "Epoch 097: val_acc did not improve from 0.687\n",
      "576/576 [==============================] - 140s 243ms/step - loss: 0.0302 - val_loss: 0.0426\n",
      "Counter: 5, Global: 0.6870766207575798, MyBest: 0.6870766207575798\n",
      "\n",
      "Epoch 98/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302"
     ]
    }
   ],
   "source": [
    "\n",
    "net.compile(optimizer=optimizer, loss=weighted_mse_loss, metrics=[])\n",
    "#checkpointer = ModelCheckpoint(filepath=f'../cunet{nUNet}_{m}{n}_best.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True)\n",
    "history = net.fit(train_images,train_hms,validation_data=(eval_images,eval_hms2),epochs=151, batch_size=32,shuffle=True,verbose=1, callbacks=[evalcallback,updateLR])#,use_multiprocessing=True,workers=20)#[checkpointer,updateLR])\n",
    "#np.savez_compressed(\"../history\",history)\n",
    "net.save_weights(f'../cunet{nUNet}_{m}{n}_last_sigmoidAdamQuasiDef3sx12_restartsig8.h5', overwrite=True)\n",
    "np.save('../history_sigmoidAdamQuasiDef3sx12_restartsig8.npy',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "net.load_weights(\"../cunet2_6416_last_sigmoidAdamQuasiDef3sx12.h5\")\n",
    "output = net.predict(train_images)\n",
    "output = np.transpose(output,(0,3,1,2))\n",
    "print(accuracy(output,np.transpose(train_hms,(0,3,1,2))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ho aumentato solo patience \n",
    "#Provato con dropout ma andato male"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
