{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import setGPU\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "160\n",
      "240\n",
      "320\n",
      "96\n",
      "192\n",
      "288\n",
      "384\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 64) 9408        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 64)   256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 64)   4096        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   9216        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64, 64, 80)   0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 80)   320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   5120        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   4096        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   9216        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 80)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 80)   320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 80)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 64)   5120        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 64)   4096        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 16)   9216        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 80)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 80)   320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 80)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   5120        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 64)     4096        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 16)     9216        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 80)     0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 80)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     5120        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 64)     0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4, 4, 64)     256         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4, 4, 64)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 4, 4, 64)     4096        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 64)     256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 16)     9216        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 80)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4, 4, 80)     0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     5120        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 80)     0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 144)    0           conv2d_16[0][0]                  \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 144)    576         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 144)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     9216        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 64)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 80)   320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 16)     9216        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 80)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 8, 160)    0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 64)   5120        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 160)  0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 224)  0           conv2d_12[0][0]                  \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 224)  896         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 224)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 64)   14336       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 64)   256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 80)   320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 16)   9216        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 80)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 240)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 64)   5120        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 240)  0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 304)  0           conv2d_8[0][0]                   \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 32, 304)  1216        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 32, 304)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 64)   19456       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 64)   256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 80)   320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 16)   9216        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 320)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   5120        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 320)  0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 64, 64, 384)  0           conv2d_4[0][0]                   \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 384)  1536        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 384)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 64)   24576       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 64)   256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 64)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 16)   9216        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 64, 64, 400)  0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64, 64, 464)  0           max_pooling2d[0][0]              \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 64, 64, 464)  1856        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 64, 64, 464)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 64)   29696       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 64, 64, 80)   0           conv2d_31[0][0]                  \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64, 64, 80)   320         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64, 64, 80)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 64, 64, 64)   5120        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 64, 64, 64)   256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 64, 64, 64)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 64, 64, 16)   9216        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 64, 64, 96)   0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 64, 64, 96)   384         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 64, 64, 96)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 64)   6144        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 32, 32, 80)   0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 32, 80)   320         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 80)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 64)   5120        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 64)   256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 64)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 16)   9216        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 32, 32, 96)   0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 96)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 64)   6144        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 80)   0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 80)   320         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16, 16, 80)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 64)   5120        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 64)   256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 64)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 16)   9216        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 96)   0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 96)   384         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 96)   0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 64)   6144        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 80)     0           max_pooling2d_7[0][0]            \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 80)     320         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 80)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 64)     5120        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 64)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 16)     9216        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 96)     0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 96)     384         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 96)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 64)     6144        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 64)     0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 4, 4, 80)     0           max_pooling2d_8[0][0]            \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 80)     320         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 80)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 64)     5120        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 64)     256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 64)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 96)     384         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 16)     9216        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 96)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 4, 4, 96)     0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 64)     6144        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 8, 8, 96)     0           concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 176)    0           conv2d_47[0][0]                  \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 8, 8, 176)    704         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 8, 8, 176)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 8, 64)     11264       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 64)     256         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 8, 8, 64)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 96)   384         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 8, 8, 16)     9216        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 96)   0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 192)    0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 64)   6144        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 192)  0           concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 16, 16, 272)  0           conv2d_43[0][0]                  \n",
      "                                                                 up_sampling2d_5[0][0]            \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16, 16, 272)  1088        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 16, 272)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 16, 16, 64)   17408       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 64)   256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 64)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 16)   9216        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 96)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 288)  0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 64)   6144        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 288)  0           concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 32, 32, 368)  0           conv2d_39[0][0]                  \n",
      "                                                                 up_sampling2d_6[0][0]            \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 32, 368)  1472        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 32, 32, 368)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 32, 64)   23552       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 64)   256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 64)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 64, 64, 96)   384         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 16)   9216        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 64, 64, 96)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 32, 32, 384)  0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 64, 64)   6144        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 384)  0           concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 64, 64, 464)  0           conv2d_35[0][0]                  \n",
      "                                                                 up_sampling2d_7[0][0]            \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 64, 64, 464)  1856        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 64, 64, 464)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 64, 64, 64)   29696       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 64, 64, 64)   256         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 64, 64, 64)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 64, 64, 16)   9216        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 64, 64, 480)  0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 64, 64, 480)  1920        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 64, 64, 480)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 64, 64, 16)   7680        activation_62[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 524,864\n",
      "Trainable params: 511,616\n",
      "Non-trainable params: 13,248\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Definizione Modello\n",
    "def getUnit1(layerPrec, filters, kernel_size = (1, 1), activation='relu', kernel_initializer='he_normal'):\n",
    "  l = BatchNormalization()(layerPrec)\n",
    "  l = Activation('relu')(l)\n",
    "  l = Conv2D(filters, kernel_size, kernel_initializer=kernel_initializer, padding='same',use_bias=False)(l)\n",
    "  return l\n",
    "\n",
    "def getUnit2(layerPrec, filters, kernel_size = (3, 3), activation='relu', kernel_initializer='he_normal'):\n",
    "  l = BatchNormalization()(layerPrec)\n",
    "  l = Activation('relu')(l)\n",
    "  l = Conv2D(filters, kernel_size, kernel_initializer=kernel_initializer, padding='same',use_bias=False)(l)\n",
    "  return l\n",
    "\n",
    "def getDownBlock(layerPrec,m,n,indexBlock):\n",
    "  l = getUnit1(layerPrec,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][f\"down{indexBlock}\"].append(l)\n",
    "\n",
    "  concat = Concatenate()([layerPrec,l])\n",
    "  l = getUnit1(concat,m)\n",
    "  maxPooling = MaxPool2D(padding='same')(l)\n",
    "  return maxPooling, getUnit1(concat,m)\n",
    "\n",
    "def getUpBlock(layerPrec,skipConn,m,n,indexBlock,upLayers=[]):\n",
    "  l = getUnit1(layerPrec,m)\n",
    "  l = UpSampling2D()(layerPrec)\n",
    "  concat = Concatenate()([skipConn,l]+upLayers)\n",
    "  l = getUnit1(concat,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][f\"up{indexBlock}\"].append(l)\n",
    "  concat = Concatenate()([concat,l])\n",
    "  return concat\n",
    "\n",
    "def getUNet(input,m,n,indexUNet,nUNet, nBlocks):\n",
    "  layerPrec = input\n",
    "  listSkipConn = []\n",
    "\n",
    "  if indexUNet != 0:\n",
    "    layerPrec = nets[f\"unet{indexUNet-1}\"]\n",
    "    #layerPrec = Concatenate()([input,layerPrec]) #l'abbiamo fatto giù con l'if dopo l'up\n",
    "    layerPrec = getUnit1(layerPrec,m)\n",
    "\n",
    "  # down\n",
    "  for i in range(nBlocks):\n",
    "    if nets[\"layers\"][f\"down{i}\"]:\n",
    "        layerPrec = Concatenate()([layerPrec]+nets[\"layers\"][f\"down{i}\"])\n",
    "    layerPrec, skipConn = getDownBlock(layerPrec,m,n,i)\n",
    "    listSkipConn.append(skipConn)\n",
    "\n",
    "  # bottle neck\n",
    "  if nets[\"layers\"][f\"bn\"]:\n",
    "      layerPrec = Concatenate()([layerPrec]+nets[\"layers\"][f\"bn\"])\n",
    "\n",
    "  l = getUnit1(layerPrec,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][\"bn\"].append(l)\n",
    "  concat = Concatenate()([layerPrec,l])\n",
    "\n",
    "  # up\n",
    "  layerPrec = concat\n",
    "  for i in range(nBlocks):\n",
    "    layerPrec = getUpBlock(layerPrec,listSkipConn[-(i+1)],m,n,i,upLayers=nets[\"layers\"][f\"up{i}\"])\n",
    "\n",
    "\n",
    "  if indexUNet != nUNet - 1:\n",
    "    l = Concatenate()([input,layerPrec])\n",
    "  else:\n",
    "    l = getUnit1(layerPrec,16,activation=\"sigmoid\") #era linear\n",
    "\n",
    "  return l\n",
    "\n",
    "def trasformationInput(x, filters):\n",
    "  #x = BatchNormalization()(x)\n",
    "  #x = Activation('relu')(x)\n",
    "  #x = Conv2D(filters, kernel_size=(7,7), strides=(2,2), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x)\n",
    "  #maxPooling = MaxPool2D(padding='same')(x)\n",
    "  \n",
    "  #x = Conv2D(filters*2, kernel_size=(5,5), strides=(1,1), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x)\n",
    "  #x = BatchNormalization()(x)\n",
    "  #x = Activation('relu')(x)\n",
    "  x = Conv2D(filters, kernel_size=(7,7), strides=(1,1), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x) #era 3x3\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPool2D(padding='same')(x)\n",
    "\n",
    "  return x#maxPooling\n",
    "\n",
    "def getCUNet(shape,m,n,nUNet,nBlocks):\n",
    "  for i in range(nUNet):\n",
    "    nets[f\"unet{i}\"] = None\n",
    "\n",
    "  for j in range(nBlocks):\n",
    "    nets[\"layers\"][f\"down{j}\"] = []\n",
    "    nets[\"layers\"][f\"up{j}\"] = []\n",
    "    \n",
    "  input = Input(shape=shape)\n",
    "  \n",
    "  t_input = trasformationInput(input,m) # per le heatmap da 64x64\n",
    "\n",
    "  for i in range(nUNet):\n",
    "    nets[f\"unet{i}\"] = getUNet(t_input,m,n,i,nUNet,nBlocks)\n",
    "\n",
    "  output = nets[f\"unet{nUNet-1}\"]\n",
    "  return Model(inputs=input, outputs=output)\n",
    "\n",
    "nets = {}\n",
    "nets[\"layers\"] = {}\n",
    "nets[\"layers\"][\"bn\"] = []\n",
    "shape = (128,128,3)\n",
    "m = 64\n",
    "n = 16\n",
    "nUNet = 2\n",
    "nBlocks = 4\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "net = getCUNet(shape,m,n,nUNet,nBlocks)\n",
    "\n",
    "layers = net.layers\n",
    "#print(layers[57].input)\n",
    "for i in range(len(layers)):\n",
    "    if isinstance(layers[i], tf.python.keras.layers.convolutional.Conv2D):\n",
    "        if isinstance(layers[i-1].input,list):\n",
    "            in_chan = 0\n",
    "            for k in layers[i-1].input:\n",
    "                in_chan = in_chan + k.shape[-1]\n",
    "            print(in_chan)\n",
    "        else:\n",
    "            in_chan = layers[i-1].input.shape[-1]\n",
    "            \n",
    "        n1 = layers[i].kernel_size[0] * layers[i].kernel_size[1] * in_chan\n",
    "        stdv = 1/math.sqrt(n1)\n",
    "        layers[i].kernel_initializer = tf.keras.initializers.RandomUniform(minval=-stdv, maxval=stdv)\n",
    "\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_preds(heatmaps):\n",
    "    \"\"\"Get keypoint predictions from score maps.\n",
    "    Note:\n",
    "        batch_size: N\n",
    "        num_keypoints: K\n",
    "        heatmap height: H\n",
    "        heatmap width: W\n",
    "    Args:\n",
    "        heatmaps (np.ndarray[N, K, H, W]): model predicted heatmaps.\n",
    "    Returns:\n",
    "        tuple: A tuple containing aggregated results.\n",
    "        - preds (np.ndarray[N, K, 2]): Predicted keypoint location.  \n",
    "        - maxvals (np.ndarray[N, K, 1]): Scores (confidence) of the keypoints. non lo restituiamo\n",
    "    \"\"\"\n",
    "    #assert isinstance(heatmaps, np.ndarray), ('heatmaps should be numpy.ndarray')\n",
    "    #assert heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n",
    "\n",
    "    N, K, _, W = heatmaps.shape\n",
    "    heatmaps_reshaped = heatmaps.reshape((N, K, -1))\n",
    "    idx = np.argmax(heatmaps_reshaped, 2).reshape((N, K, 1))\n",
    "    maxvals = np.amax(heatmaps_reshaped, 2).reshape((N, K, 1))\n",
    "\n",
    "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n",
    "    preds[:, :, 0] = preds[:, :, 0] % W\n",
    "    preds[:, :, 1] = preds[:, :, 1] // W\n",
    "\n",
    "    preds = np.where(np.tile(maxvals, (1, 1, 2)) > 0.0, preds, -1)\n",
    "    return preds#, maxvals\n",
    "\n",
    "def calc_dists(preds, target, normalize, use_zero=False):\n",
    "    preds = preds.astype(np.float32)\n",
    "    target = target.astype(np.float32)\n",
    "    normalize = normalize.astype(np.float32)\n",
    "    dists = np.zeros((preds.shape[1], preds.shape[0]))\n",
    "    if use_zero:\n",
    "        boundary = 0\n",
    "    else:\n",
    "        boundary = 1\n",
    "    for n in range(preds.shape[0]):\n",
    "        for c in range(preds.shape[1]):\n",
    "            if target[n,c,0] > boundary and target[n, c, 1] > boundary:\n",
    "                dists[c, n] = np.linalg.norm((preds[n,c,:]- target[n,c,:])/normalize[n]) # axis ricavato da solo\n",
    "            else:\n",
    "                dists[c, n] = -1\n",
    "    return dists\n",
    "\n",
    "def dist_acc(dists, thr=0.5):\n",
    "\n",
    "  #Return percentage below threshold while ignoring values with a -1\n",
    "\n",
    "  if (dists!=-1).sum() > 0:\n",
    "\n",
    "    return ((dists<=thr) == (dists!=-1)).sum().astype(np.float32) / (dists!=-1).sum().astype(np.float32)\n",
    "\n",
    "  else:\n",
    "\n",
    "    return -1\n",
    "\n",
    "def accuracy(output, target, thr=0.5):\n",
    "  ''' Calculate accuracy according to PCK, but uses ground truth heatmap rather than x,y locations\n",
    "        First value to be returned is average accuracy across 'idxs', followed by individual accuracies\n",
    "    '''\n",
    "  #output = output.numpy()\n",
    "  #print(output.__class__)\n",
    "  #target = target.numpy()\n",
    "\n",
    "  idkp = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "  preds = get_max_preds(output)\n",
    "  gts = get_max_preds(target)\n",
    "  norm = np.ones(preds.shape[0])*output.shape[3]/10\n",
    "\n",
    "  dists = calc_dists(preds, gts, norm)\n",
    "\n",
    "  acc = np.zeros(len(idkp)+1)\n",
    "  avg_acc = 0\n",
    "  cnt = 0\n",
    "\n",
    "  for i in range(len(idkp)):\n",
    "    acc[i+1] = dist_acc(dists[idkp[i]])\n",
    "    if acc[i+1] >= 0: \n",
    "      avg_acc = avg_acc + acc[i+1]\n",
    "      cnt += 1\n",
    "            \n",
    "  if cnt != 0:  \n",
    "    acc[0] = avg_acc / cnt\n",
    "\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import cv2\n",
    "\n",
    "# =============================================================================\n",
    "# General image processing functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_transform(center, scale, res, rot=0):\n",
    "    # Generate transformation matrix\n",
    "    h = 200 * scale\n",
    "    t = np.zeros((3, 3))\n",
    "    t[0, 0] = float(res[1]) / h\n",
    "    t[1, 1] = float(res[0]) / h\n",
    "    t[0, 2] = res[1] * (-float(center[0]) / h + .5)\n",
    "    t[1, 2] = res[0] * (-float(center[1]) / h + .5)\n",
    "    t[2, 2] = 1\n",
    "    if not rot == 0:\n",
    "        rot = -rot # To match direction of rotation from cropping\n",
    "        rot_mat = np.zeros((3,3))\n",
    "        rot_rad = rot * np.pi / 180\n",
    "        sn,cs = np.sin(rot_rad), np.cos(rot_rad)\n",
    "        rot_mat[0,:2] = [cs, -sn]\n",
    "        rot_mat[1,:2] = [sn, cs]\n",
    "        rot_mat[2,2] = 1\n",
    "        # Need to rotate around center\n",
    "        t_mat = np.eye(3)\n",
    "        t_mat[0,2] = -res[1]/2\n",
    "        t_mat[1,2] = -res[0]/2\n",
    "        t_inv = t_mat.copy()\n",
    "        t_inv[:2,2] *= -1\n",
    "        t = np.dot(t_inv,np.dot(rot_mat,np.dot(t_mat,t)))\n",
    "    return t\n",
    "\n",
    "def transform(pt, center, scale, res, invert=0, rot=0):\n",
    "    # Transform pixel location to different reference\n",
    "    t = get_transform(center, scale, res, rot=rot)\n",
    "    if invert:\n",
    "        t = np.linalg.inv(t)\n",
    "    new_pt = np.array([pt[0], pt[1], 1.]).T\n",
    "    new_pt = np.dot(t, new_pt)\n",
    "    return new_pt[:2].astype(int)\n",
    "\n",
    "def crop(img, center, scale, res, rot=0):\n",
    "    # Upper left point\n",
    "    ul = np.array(transform([0, 0], center, scale, res, invert=1))\n",
    "    # Bottom right point\n",
    "    br = np.array(transform(res, center, scale, res, invert=1))\n",
    "\n",
    "    new_shape = [br[1] - ul[1], br[0] - ul[0]]\n",
    "    if len(img.shape) > 2:\n",
    "        new_shape += [img.shape[2]]\n",
    "    new_img = np.zeros(new_shape)\n",
    "\n",
    "    # Range to fill new array\n",
    "    new_x = max(0, -ul[0]), min(br[0], len(img[0])) - ul[0]\n",
    "    new_y = max(0, -ul[1]), min(br[1], len(img)) - ul[1]\n",
    "    # Range to sample from original image\n",
    "    old_x = max(0, ul[0]), min(len(img[0]), br[0])\n",
    "    old_y = max(0, ul[1]), min(len(img), br[1])\n",
    "    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], old_x[0]:old_x[1]]\n",
    "\n",
    "    return cv2.resize(new_img, res)\n",
    "\n",
    "def inv_mat(mat):\n",
    "    ans = np.linalg.pinv(np.array(mat).tolist() + [[0,0,1]])\n",
    "    return ans[:2]\n",
    "\n",
    "def kpt_affine(kpt, mat):\n",
    "    kpt = np.array(kpt)\n",
    "    shape = kpt.shape\n",
    "    kpt = kpt.reshape(-1, 2)\n",
    "    return np.dot( np.concatenate((kpt, kpt[:, 0:1]*0+1), axis = 1), mat.T ).reshape(shape)\n",
    "\n",
    "\n",
    "def resize(im, res):\n",
    "    return np.array([cv2.resize(im[i],res) for i in range(im.shape[0])])\n",
    "\n",
    "def generateHeatmap(keypoints,output_res,num_parts):\n",
    "    #Init\n",
    "    sigma = output_res/64\n",
    "    size = 6*sigma+3\n",
    "    x = np.arange(0, size, 1, float) #crea un array composto da nove 1.0 vettore 1-D (9,)\n",
    "    y = x[:, np.newaxis] #Vettore 2-D (9,1)\n",
    "    x0, y0 = 3*sigma + 1, 3*sigma + 1 #vedi giù a tutto\n",
    "    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
    "    #Generation\n",
    "    hms = np.zeros(shape = (num_parts, output_res, output_res), dtype = np.float32) #crea vettore (16,64,64), cioè 16 heatmaps nere\n",
    "    for p in keypoints:\n",
    "        for idx, pt in enumerate(p): #ottiene id + [x,y] di ogni keypoint\n",
    "            if pt[0] > 0: \n",
    "                x, y = int(pt[0]), int(pt[1])\n",
    "                if x<0 or y<0 or x>=output_res or y>=output_res: #se succede questo, rimane heatmap idx-esima  tutta a 0\n",
    "                    continue\n",
    "                ul = int(x - 3*sigma - 1), int(y - 3*sigma - 1)\n",
    "                br = int(x + 3*sigma + 2), int(y + 3*sigma + 2)\n",
    "\n",
    "                c,d = max(0, -ul[0]), min(br[0], output_res) - ul[0]\n",
    "                a,b = max(0, -ul[1]), min(br[1], output_res) - ul[1]\n",
    "\n",
    "                cc,dd = max(0, ul[0]), min(br[0], output_res)\n",
    "                aa,bb = max(0, ul[1]), min(br[1], output_res)\n",
    "                hms[idx, aa:bb,cc:dd] = np.maximum(hms[idx, aa:bb,cc:dd], g[a:b,c:d])\n",
    "    return hms\n",
    "def getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    cropped = crop(img,c,s,inp_res)\n",
    "    orig_keypoints = []\n",
    "    for i in keypoints:\n",
    "        orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "    orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "    kptmp = np.copy(orig_keypoints)\n",
    "    for i in range(orig_keypoints.shape[1]):\n",
    "        if orig_keypoints[0,i,0] > 0:\n",
    "            orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "    keypoints = np.copy(orig_keypoints)\n",
    "    h, w = cropped.shape[0:2]\n",
    "    center = np.array((w/2,h/2))\n",
    "    scale = max(h,w)/200\n",
    "    aug_rot = (np.random.random()*2-1)*30\n",
    "    aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "    scale *= aug_scale\n",
    "\n",
    "    mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "    mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "    inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "    keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "    \n",
    "    #print('\\n')\n",
    "\n",
    "\n",
    "    #Flip 50% probability\n",
    "    if np.random.randint(2) == 0:\n",
    "        inp = inp[:, ::-1]\n",
    "        keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "        keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "        orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "        orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "    ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "        for i in range(np.shape(orig_keypoints)[1]):\n",
    "            if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                keypoints[0,i,0] = 0\n",
    "                keypoints[0,i,1] = 0\n",
    "                orig_keypoints[0,i,0] = 0\n",
    "                orig_keypoints[0,i,1] = 0\n",
    "\n",
    "    heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "    return inp,heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=2.5e-4, rho=0.99, momentum=0.0, epsilon=1e-08\n",
    ")\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=6.7e-3)\n",
    "\n",
    "\n",
    "def heatmapLoss(y_true,y_pred):\n",
    "    l = tf.math.square((y_pred - y_true))\n",
    "    l = tf.reduce_mean(l,axis=3)\n",
    "    l = tf.reduce_mean(l,axis=2)\n",
    "    l = tf.reduce_mean(l,axis=1)\n",
    "    return l\n",
    "\n",
    "def euclidean_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_true - y_pred)))\n",
    "\n",
    "def dice(y_true,y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice(y_true, y_pred)\n",
    "\n",
    "def weighted_mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    apply weights on heatmap mse loss to only pick valid keypoint heatmap\n",
    "    since y_true would be gt_heatmap with shape\n",
    "    (batch_size, heatmap_size[0], heatmap_size[1], num_keypoints)\n",
    "    we sum up the heatmap for each keypoints and check. Sum for invalid\n",
    "    keypoint would be 0, so we can get a keypoint weights tensor with shape\n",
    "    (batch_size, 1, 1, num_keypoints)\n",
    "    and multiply to loss\n",
    "    \"\"\"\n",
    "    heatmap_sum = K.sum(K.sum(y_true, axis=1, keepdims=True), axis=2, keepdims=True)\n",
    "\n",
    "    # keypoint_weights shape: (batch_size, 1, 1, num_keypoints), with\n",
    "    # valid_keypoint = 1.0, invalid_keypoint = 0.0\n",
    "    keypoint_weights = 1.0 - K.cast(K.equal(heatmap_sum, 0.0), 'float32')\n",
    "\n",
    "    return K.sqrt(K.mean(K.square((y_true - y_pred) * keypoint_weights)))\n",
    "\n",
    "def smooth_l1_loss(y_true, y_pred):\n",
    "    diff = K.abs(y_true - y_pred)\n",
    "    less_than_one = K.cast(K.less(diff, 1.0), 'float32')\n",
    "    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "parts = {'mpii':['rank', 'rkne', 'rhip',\n",
    "                 'lhip', 'lkne', 'lank',\n",
    "                 'pelv', 'thrx', 'neck', 'head',\n",
    "                 'rwri', 'relb', 'rsho',\n",
    "                 'lsho', 'lelb', 'lwri']}\n",
    "\n",
    "flipped_parts = {'mpii':[5, 4, 3, 2, 1, 0, 6, 7, 8, 9, 15, 14, 13, 12, 11, 10]}\n",
    "\n",
    "part_pairs = {'mpii':[[0, 5], [1, 4], [2, 3], [6], [7], [8], [9], [10, 15], [11, 14], [12, 13]]}\n",
    "\n",
    "pair_names = {'mpii':['ankle', 'knee', 'hip', 'pelvis', 'thorax', 'neck', 'head', 'wrist', 'elbow', 'shoulder']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imageio\n",
    "\n",
    "class Dataset(tf.keras.utils.Sequence):\n",
    "    #Costruttore\n",
    "    def __init__(self,json_path,batch_size,input_shape=(256,256,3),output_shape=(64,64,16),train=True,dataset_name=\"MPII\"):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.batch_size = batch_size\n",
    "        f = open(json_path)\n",
    "        self.dataset = json.load(f)\n",
    "        if dataset_name == \"MPII\":\n",
    "            self.dataset = self.dataset[\"dataset\"][\"MPII\"][\"people\"]\n",
    "        if train:\n",
    "            self.dataset = self.dataset[:-7221]\n",
    "        self.batch_images = np.zeros(shape=((self.batch_size,)+(self.input_shape)), dtype=np.float32)\n",
    "        self.batch_heatmaps = np.zeros(shape=((self.batch_size,)+(self.output_shape)), dtype=np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        for n, annotation in enumerate(self.dataset[i*self.batch_size:(i+1)*self.batch_size]):\n",
    "            #sample_index = i*self.batch_size + n\n",
    "            image, gt_heatmap = self.extract_imgs_hms(annotation)\n",
    "            gt_heatmap = np.transpose(gt_heatmap,(1,2,0))\n",
    "            self.batch_images[n,:,:,:] = image\n",
    "            self.batch_heatmaps[n, :, :, :] = gt_heatmap#gt_heatmap.reshape((self.output_shape))\n",
    "            \n",
    "        return self.batch_images, self.batch_heatmaps\n",
    "#getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    \n",
    "    def extract_imgs_hms(self,annotation):\n",
    "        inp_res = self.input_shape[0:2]\n",
    "        out_res = self.output_shape[0:2]\n",
    "        img = imageio.imread(f\"../images_mpii/{annotation['filepath']}\")\n",
    "        c = [annotation[\"objpos\"][\"x\"],annotation[\"objpos\"][\"y\"]]\n",
    "        s = annotation[\"scale\"]\n",
    "        keypoints = annotation[\"keypoints\"]\n",
    "        cropped = crop(img,c,s,inp_res)\n",
    "        \n",
    "        orig_keypoints = []\n",
    "        for i in keypoints:\n",
    "            orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "        orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "        kptmp = np.copy(orig_keypoints)\n",
    "        for i in range(orig_keypoints.shape[1]):\n",
    "            if orig_keypoints[0,i,0] > 0:\n",
    "                orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "        keypoints = np.copy(orig_keypoints)\n",
    "        h, w = cropped.shape[0:2]\n",
    "        center = np.array((w/2,h/2))\n",
    "        scale = max(h,w)/200\n",
    "        aug_rot = (np.random.random()*2-1)*30\n",
    "        aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "        scale *= aug_scale\n",
    "\n",
    "        mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "        mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "        inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "        keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "\n",
    "        #print('\\n')\n",
    "\n",
    "\n",
    "        #Flip 50% probability\n",
    "        if np.random.randint(2) == 0:\n",
    "            inp = inp[:, ::-1]\n",
    "            keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "            keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "            orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "            orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "        ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "            for i in range(np.shape(orig_keypoints)[1]):\n",
    "                if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                    keypoints[0,i,0] = 0\n",
    "                    keypoints[0,i,1] = 0\n",
    "                    orig_keypoints[0,i,0] = 0\n",
    "                    orig_keypoints[0,i,1] = 0\n",
    "\n",
    "        heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "        return inp,heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imageio\n",
    "\n",
    "class Dataset2(tf.keras.utils.Sequence):\n",
    "    #Costruttore\n",
    "    def __init__(self,json_path,batch_size,input_shape=(256,256,3),output_shape=(64,64,16),train=True,dataset_name=\"MPII\"):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.images = np.load(\"../imgs_mpii.npz\")['arr_0']\n",
    "        self.hms = np.load(\"../hms_mpii.npz\")['arr_0']\n",
    "        self.hms = self.hms.reshape((self.hms.shape[0],64,64,16))\n",
    "        if dataset_name == \"MPII\":\n",
    "            self.images = self.images[\"dataset\"][\"MPII\"][\"people\"]\n",
    "        if train:\n",
    "            self.images = self.images[:-8665]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.images[i*self.batch_size:(i+1)*self.batch_size], self.hms[i*self.batch_size:(i+1)*self.batch_size]\n",
    "#getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    \n",
    "    def extract_imgs_hms(self,annotation):\n",
    "        inp_res = self.input_shape[0:2]\n",
    "        out_res = self.output_shape[0:2]\n",
    "        img = imageio.imread(f\"../images_mpii/{annotation['filepath']}\")\n",
    "        c = [annotation[\"objpos\"][\"x\"],annotation[\"objpos\"][\"y\"]]\n",
    "        s = annotation[\"scale\"]\n",
    "        keypoints = annotation[\"keypoints\"]\n",
    "        cropped = crop(img,c,s,inp_res)\n",
    "        \n",
    "        orig_keypoints = []\n",
    "        for i in keypoints:\n",
    "            orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "        orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "        kptmp = np.copy(orig_keypoints)\n",
    "        for i in range(orig_keypoints.shape[1]):\n",
    "            if orig_keypoints[0,i,0] > 0:\n",
    "                orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "        keypoints = np.copy(orig_keypoints)\n",
    "        h, w = cropped.shape[0:2]\n",
    "        center = np.array((w/2,h/2))\n",
    "        scale = max(h,w)/200\n",
    "        aug_rot = (np.random.random()*2-1)*30\n",
    "        aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "        scale *= aug_scale\n",
    "\n",
    "        mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "        mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "        inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "        keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "\n",
    "        #print('\\n')\n",
    "\n",
    "\n",
    "        #Flip 50% probability\n",
    "        if np.random.randint(2) == 0:\n",
    "            inp = inp[:, ::-1]\n",
    "            keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "            keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "            orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "            orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "        ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "            for i in range(np.shape(orig_keypoints)[1]):\n",
    "                if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                    keypoints[0,i,0] = 0\n",
    "                    keypoints[0,i,1] = 0\n",
    "                    orig_keypoints[0,i,0] = 0\n",
    "                    orig_keypoints[0,i,1] = 0\n",
    "\n",
    "        heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "        return inp,heatmaps\n",
    "####################################################################àà\n",
    "\n",
    "\n",
    "def get_normalize(input_shape):\n",
    "    \"\"\"\n",
    "    rescale keypoint distance normalize coefficient\n",
    "    based on input shape, used for PCK evaluation\n",
    "    NOTE: 6.4 is standard normalize coefficient under\n",
    "          input shape (256,256)\n",
    "    # Arguments\n",
    "        input_shape: input image shape as (height, width)\n",
    "    # Returns\n",
    "        scale: normalize coefficient\n",
    "    \"\"\"\n",
    "    #assert input_shape[0] == input_shape[1], 'only support square input shape.'\n",
    "\n",
    "    # use averaged scale factor for non square input shape\n",
    "    scale = float((input_shape[0] + input_shape[1]) / 2) / 256.0\n",
    "\n",
    "    return 6.4*scale\n",
    "\n",
    "global_best_acc = 0.0\n",
    "\n",
    "class EvalCallBack(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model_input_shape, fileName, imgs,hms):\n",
    "        self.normalize = get_normalize(model_input_shape)\n",
    "        self.model_input_shape = model_input_shape\n",
    "        self.best_acc = 0.0\n",
    "        self.fileName = fileName\n",
    "        self.eval_images = imgs#np.load(\"../imgs_val_mpii128.npz\")['arr_0']\n",
    "        self.eval_hms = hms#np.load(\"../hms_val_mpii128.npz\")['arr_0']\n",
    "        self.listAcc = []\n",
    "        #self.eval_hms = np.reshape(self.eval_hms,(self.eval_hms.shape[0],64,64,16))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global global_best_acc\n",
    "        output = self.model.predict(self.eval_images)\n",
    "        #output = output.reshape( (output.shape[0],)+(16,64,64) )\n",
    "        output = np.transpose(output,(0,3,1,2))\n",
    "        val_acc = accuracy(output,self.eval_hms)\n",
    "        self.listAcc.append(val_acc[0])\n",
    "        print('\\nvalidate accuracy:\\n', val_acc, '@epoch', epoch)\n",
    "        f = open(self.fileName, \"a\")\n",
    "        if val_acc[0] > self.best_acc:\n",
    "            # Save best accuray value and model checkpoint\n",
    "            #checkpoint_dir = os.path.join(self.log_dir, 'ep{epoch:03d}-loss{loss:.3f}-val_acc{val_acc:.3f}.h5'.format(epoch=(epoch+1), loss=logs.get('loss'), val_acc=val_acc))\n",
    "            #self.model.save(f\"../modelsave/ep{epoch}_acc{val_acc[0]}.h5\")\n",
    "            self.model.save_weights(f\"../cunet{nUNet}_{m}{n}_best_sigmoidAdamQuasiDef3sx12_restartsig.h5\")\n",
    "            print('Epoch {epoch:03d}: val_acc improved from {best_acc:.3f} to {val_acc:.3f}'.format(epoch=epoch+1, best_acc=self.best_acc, val_acc=val_acc[0]))# checkpoint_dir=checkpoint_dir))\n",
    "            message = 'Epoch {epoch:03d}: val_acc improved from {best_acc:.3f} to {val_acc:.3f}. Train loss is {loss:.3f}\\n'.format(epoch=epoch+1, best_acc=self.best_acc, val_acc=val_acc[0],loss=logs.get('loss'))\n",
    "            self.best_acc = val_acc[0]\n",
    "            global_best_acc = val_acc[0]\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            message = 'Epoch {epoch:03d}: val_acc did not improve from {best_acc:.3f}. Train loss is {loss:.3f}\\n'.format(epoch=epoch+1, best_acc=self.best_acc,loss=logs.get('loss'))\n",
    "            print('Epoch {epoch:03d}: val_acc did not improve from {best_acc:.3f}'.format(epoch=epoch+1, best_acc=self.best_acc))\n",
    "        f.write(message)\n",
    "        f.close()\n",
    "        if epoch == 150:\n",
    "            np.savez_compressed(f'../cunet{nUNet}_{m}{n}_best_sigmoidAdamQuasiDef3sx12_restartsig_accs', self.listAcc)\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "class CustomLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience,factor,fileName=None):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.fileName = fileName\n",
    "        self.factor = factor\n",
    "        self.best_acc = 0.0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        global global_best_acc\n",
    "        print(f\"Counter: {self.counter}, Global: {global_best_acc}, MyBest: {self.best_acc}\\n\")\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "    \n",
    "    def schedule(self,epoch,lr):\n",
    "        global global_best_acc\n",
    "        if self.counter == self.patience:\n",
    "            self.counter = 0 \n",
    "            print(\"Epoch %03d: Updating Learning rate.. New value is %f\" % (epoch, lr*self.factor))\n",
    "            message = \"Epoch %03d: Updating Learning rate.. New value is %f\" % (epoch, lr*self.factor)\n",
    "            f = open(self.fileName, \"a\")\n",
    "            f.write(message)\n",
    "            f.close()\n",
    "            return lr*self.factor\n",
    "        if self.best_acc == global_best_acc:\n",
    "            self.counter = self.counter + 1\n",
    "        elif self.best_acc < global_best_acc:\n",
    "            self.counter = 0\n",
    "            self.best_acc = global_best_acc\n",
    "        return lr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finito 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndataset = Dataset(\"datasets.json\",32)\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "with open('../train_imgssx12.pickle', 'rb') as handle:\n",
    "    train_images = pickle.load(handle)\n",
    "with open('../train_hmssx12.pickle', 'rb') as handle:\n",
    "    train_hms = pickle.load(handle)\n",
    "\n",
    "eval_images = np.load(\"../imgs_val_mpii128sx12.npz\")['arr_0']\n",
    "eval_hms = np.load(\"../hms_val_mpii128sx12.npz\")['arr_0']\n",
    "\n",
    "#train_images = np.load(\"../imgs_train_mpii128.npz\")['arr_0']\n",
    "#train_hms = np.load(\"../hms_train_mpii128.npz\")['arr_0']\n",
    "train_hms = np.transpose(train_hms,(0,2,3,1))#np.reshape(train_hms,(train_hms.shape[0],64,64,16))\n",
    "eval_hms2 = np.transpose(eval_hms,(0,2,3,1))\n",
    "print(\"Finito 1\")\n",
    "\"\"\"\n",
    "dataset = Dataset(\"datasets.json\",32)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finito 2\n"
     ]
    }
   ],
   "source": [
    "evalcallback = EvalCallBack((128,128),\"../sigmoidLayersAdamSchedulingQuasiDef3sx12_restartsig.txt\",eval_images,eval_hms)\n",
    "updateLR = CustomLearningRateScheduler(7,0.2,fileName=\"../sigmoidLayersAdamSchedulingQuasiDef3sx12_restartsig.txt\")\n",
    "print(\"Finito 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 0, Global: 0.0, MyBest: 0.0\n",
      "\n",
      "Epoch 1/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.1003WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0133s vs `on_test_batch_end` time: 0.0371s). Check your callbacks.\n",
      "\n",
      "validate accuracy:\n",
      " [0.01854912 0.01803522 0.0145485  0.         0.01628008 0.01426174\n",
      " 0.02190557 0.00910536 0.0512713  0.00027952 0.05497685 0.00833804\n",
      " 0.01795735 0.02425087 0.         0.03426485 0.01131062] @epoch 0\n",
      "Epoch 001: val_acc improved from 0.000 to 0.019\n",
      "576/576 [==============================] - 131s 227ms/step - loss: 0.1003 - val_loss: 0.0433\n",
      "Counter: 1, Global: 0.018549117927250336, MyBest: 0.0\n",
      "\n",
      "Epoch 2/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0458\n",
      "validate accuracy:\n",
      " [0.01835888 0.01166985 0.01103679 0.02161695 0.02146665 0.00956376\n",
      " 0.01361123 0.02514814 0.02528639 0.01914745 0.02893518 0.01639344\n",
      " 0.01585297 0.02229965 0.02536939 0.01390254 0.01244168] @epoch 1\n",
      "Epoch 002: val_acc did not improve from 0.019\n",
      "576/576 [==============================] - 126s 219ms/step - loss: 0.0458 - val_loss: 0.0580\n",
      "Counter: 0, Global: 0.018549117927250336, MyBest: 0.018549117927250336\n",
      "\n",
      "Epoch 3/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0408\n",
      "validate accuracy:\n",
      " [0.02954375 0.01273074 0.01772575 0.02939905 0.03961965 0.02063758\n",
      " 0.01701404 0.05463217 0.04694049 0.04626136 0.03226273 0.02091577\n",
      " 0.02413019 0.02912892 0.03442989 0.03075411 0.01611763] @epoch 2\n",
      "Epoch 003: val_acc improved from 0.019 to 0.030\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0408 - val_loss: 0.0543\n",
      "Counter: 1, Global: 0.029543754237238318, MyBest: 0.018549117927250336\n",
      "\n",
      "Epoch 4/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0387\n",
      "validate accuracy:\n",
      " [0.04372236 0.0131551  0.0277592  0.06168036 0.0530183  0.03389262\n",
      " 0.02552105 0.09697933 0.07027102 0.05953879 0.03703704 0.02967778\n",
      " 0.03493266 0.03860627 0.0593811  0.04184806 0.01625901] @epoch 3\n",
      "Epoch 004: val_acc improved from 0.030 to 0.044\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0387 - val_loss: 0.0387\n",
      "Counter: 0, Global: 0.04372235550545156, MyBest: 0.029543754237238318\n",
      "\n",
      "Epoch 5/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0374\n",
      "validate accuracy:\n",
      " [0.04824034 0.01485254 0.06036789 0.04193688 0.07174759 0.02651007\n",
      " 0.02764781 0.09379968 0.08354289 0.05366876 0.03226273 0.02586207\n",
      " 0.04419192 0.05839721 0.07262336 0.04704395 0.01739007] @epoch 4\n",
      "Epoch 005: val_acc improved from 0.044 to 0.048\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0374 - val_loss: 0.0610\n",
      "Counter: 0, Global: 0.04824033932527527, MyBest: 0.04372235550545156\n",
      "\n",
      "Epoch 6/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0366\n",
      "validate accuracy:\n",
      " [0.06638983 0.03076597 0.05685619 0.10404958 0.08615473 0.03171141\n",
      " 0.03360272 0.14539674 0.11288069 0.09657582 0.05598958 0.02671001\n",
      " 0.04826038 0.07470383 0.06523557 0.05785704 0.03548706] @epoch 5\n",
      "Epoch 006: val_acc improved from 0.048 to 0.066\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0366 - val_loss: 0.0633\n",
      "Counter: 0, Global: 0.06638983299490064, MyBest: 0.04824033932527527\n",
      "\n",
      "Epoch 7/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0358\n",
      "validate accuracy:\n",
      " [0.10955517 0.03225122 0.05183946 0.14108661 0.1453681  0.0864094\n",
      " 0.06039983 0.17791589 0.16904163 0.15695319 0.1213831  0.04536461\n",
      " 0.10718294 0.16250871 0.14524673 0.0945092  0.05542203] @epoch 6\n",
      "Epoch 007: val_acc improved from 0.066 to 0.110\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0358 - val_loss: 0.0502\n",
      "Counter: 0, Global: 0.10955516551621258, MyBest: 0.06638983299490064\n",
      "\n",
      "Epoch 8/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0354\n",
      "validate accuracy:\n",
      " [0.0942212  0.03564608 0.06622074 0.12480184 0.15516496 0.07885906\n",
      " 0.04168439 0.14221708 0.14822575 0.14786862 0.10387731 0.05963821\n",
      " 0.05934343 0.09630662 0.09645943 0.09127931 0.05994628] @epoch 7\n",
      "Epoch 008: val_acc did not improve from 0.110\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0354 - val_loss: 0.0433\n",
      "Counter: 0, Global: 0.10955516551621258, MyBest: 0.10955516551621258\n",
      "\n",
      "Epoch 9/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0355\n",
      "validate accuracy:\n",
      " [0.10021426 0.03225122 0.0632107  0.12681943 0.1312491  0.05352349\n",
      " 0.03764356 0.18282989 0.16974016 0.16939203 0.12644675 0.06076879\n",
      " 0.06635802 0.12376307 0.10217452 0.0983008  0.0589566 ] @epoch 8\n",
      "Epoch 009: val_acc did not improve from 0.110\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0355 - val_loss: 0.0335\n",
      "Counter: 1, Global: 0.10955516551621258, MyBest: 0.10955516551621258\n",
      "\n",
      "Epoch 10/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0348\n",
      "validate accuracy:\n",
      " [0.07943308 0.02588585 0.05819398 0.07003891 0.10286702 0.04614094\n",
      " 0.03615483 0.13340078 0.13299805 0.12760307 0.08969907 0.0303844\n",
      " 0.08207071 0.10494774 0.0935322  0.08158966 0.05542203] @epoch 9\n",
      "Epoch 010: val_acc did not improve from 0.110\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0348 - val_loss: 0.0652\n",
      "Counter: 2, Global: 0.10955516551621258, MyBest: 0.10955516551621258\n",
      "\n",
      "Epoch 11/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0344\n",
      "validate accuracy:\n",
      " [0.14229654 0.04328453 0.09665552 0.17769131 0.15761417 0.12097315\n",
      " 0.06954487 0.22113022 0.25649622 0.16687632 0.16912615 0.0986433\n",
      " 0.09750281 0.17296167 0.19389462 0.1342508  0.10009897] @epoch 10\n",
      "Epoch 011: val_acc improved from 0.110 to 0.142\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0344 - val_loss: 0.0508\n",
      "Counter: 3, Global: 0.14229654031805694, MyBest: 0.10955516551621258\n",
      "\n",
      "Epoch 12/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0341\n",
      "validate accuracy:\n",
      " [0.15989695 0.06089539 0.12943144 0.23677763 0.21769197 0.01275168\n",
      " 0.10017014 0.31810954 0.27787092 0.24696016 0.21426505 0.00734878\n",
      " 0.15249719 0.21783972 0.1906886  0.121893   0.0531599 ] @epoch 11\n",
      "Epoch 012: val_acc improved from 0.142 to 0.160\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0341 - val_loss: 0.0468\n",
      "Counter: 0, Global: 0.15989694531890564, MyBest: 0.14229654031805694\n",
      "\n",
      "Epoch 13/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0340\n",
      "validate accuracy:\n",
      " [0.18786176 0.08890303 0.11070234 0.24383917 0.24261634 0.12382551\n",
      " 0.10250957 0.30163318 0.29868677 0.27700907 0.25636575 0.13199548\n",
      " 0.15207632 0.20278746 0.19640368 0.16205589 0.11437862] @epoch 12\n",
      "Epoch 013: val_acc improved from 0.160 to 0.188\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0340 - val_loss: 0.0403\n",
      "Counter: 0, Global: 0.18786176247522235, MyBest: 0.15989694531890564\n",
      "\n",
      "Epoch 14/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0337\n",
      "validate accuracy:\n",
      " [0.16006972 0.06917038 0.12123746 0.18777922 0.20976804 0.11761745\n",
      " 0.08017865 0.26766872 0.20690137 0.25283018 0.20283565 0.10994913\n",
      " 0.15993266 0.1492683  0.16462225 0.15475355 0.10660257] @epoch 13\n",
      "Epoch 014: val_acc did not improve from 0.188\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0337 - val_loss: 0.0535\n",
      "Counter: 0, Global: 0.18786176247522235, MyBest: 0.18786176247522235\n",
      "\n",
      "Epoch 15/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0336\n",
      "validate accuracy:\n",
      " [0.20911881 0.10099724 0.1555184  0.28260556 0.21942084 0.13338926\n",
      " 0.09974479 0.34051162 0.3535904  0.32480782 0.28457755 0.12747315\n",
      " 0.16512346 0.23400697 0.24407583 0.16977952 0.11027852] @epoch 14\n",
      "Epoch 015: val_acc improved from 0.188 to 0.209\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0336 - val_loss: 0.0420\n",
      "Counter: 1, Global: 0.20911880861967802, MyBest: 0.18786176247522235\n",
      "\n",
      "Epoch 16/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0335\n",
      "validate accuracy:\n",
      " [0.12606474 0.06598769 0.06170569 0.1841764  0.16236854 0.05838926\n",
      " 0.03828158 0.27330539 0.24364348 0.24150944 0.13512732 0.09383833\n",
      " 0.09750281 0.16585366 0.09032618 0.08734728 0.01767284] @epoch 15\n",
      "Epoch 016: val_acc did not improve from 0.209\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0335 - val_loss: 0.0393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 0, Global: 0.20911880861967802, MyBest: 0.20911880861967802\n",
      "\n",
      "Epoch 17/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0342\n",
      "validate accuracy:\n",
      " [0.21745957 0.10205813 0.12006689 0.25565642 0.25889641 0.12684564\n",
      " 0.11952361 0.31926578 0.38083264 0.40307477 0.29557291 0.14429057\n",
      " 0.17368126 0.27038327 0.21996097 0.14955765 0.13968614] @epoch 16\n",
      "Epoch 017: val_acc improved from 0.209 to 0.217\n",
      "576/576 [==============================] - 128s 222ms/step - loss: 0.0342 - val_loss: 0.0441\n",
      "Counter: 1, Global: 0.2174595664255321, MyBest: 0.20911880861967802\n",
      "\n",
      "Epoch 18/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0337\n",
      "validate accuracy:\n",
      " [0.22638432 0.10481647 0.12943144 0.2619974  0.25702348 0.13993289\n",
      " 0.11186729 0.33285156 0.4414641  0.43018869 0.30700231 0.13750707\n",
      " 0.17789002 0.28738675 0.24853638 0.13888499 0.1153683 ] @epoch 17\n",
      "Epoch 018: val_acc improved from 0.217 to 0.226\n",
      "576/576 [==============================] - 128s 221ms/step - loss: 0.0337 - val_loss: 0.0550\n",
      "Counter: 0, Global: 0.22638431983068585, MyBest: 0.2174595664255321\n",
      "\n",
      "Epoch 19/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0334\n",
      "validate accuracy:\n",
      " [0.2492268  0.11330363 0.13511705 0.28750542 0.26062527 0.11912752\n",
      " 0.1065504  0.35236305 0.49818385 0.4810622  0.349103   0.1513567\n",
      " 0.1936027  0.30759582 0.27822694 0.20474653 0.14915878] @epoch 18\n",
      "Epoch 019: val_acc improved from 0.226 to 0.249\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0334 - val_loss: 0.0526\n",
      "Counter: 0, Global: 0.24922680389136076, MyBest: 0.22638431983068585\n",
      "\n",
      "Epoch 20/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0333\n",
      "validate accuracy:\n",
      " [0.2751367  0.14513049 0.16923077 0.29658452 0.29347357 0.16845638\n",
      " 0.15227562 0.36941755 0.53604358 0.52983928 0.40234375 0.17340305\n",
      " 0.19234006 0.30954704 0.29704487 0.20432523 0.16273151] @epoch 19\n",
      "Epoch 020: val_acc improved from 0.249 to 0.275\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0333 - val_loss: 0.0463\n",
      "Counter: 0, Global: 0.27513670455664396, MyBest: 0.24922680389136076\n",
      "\n",
      "Epoch 21/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0331\n",
      "validate accuracy:\n",
      " [0.26574622 0.11478888 0.14531772 0.24686554 0.29880419 0.16291946\n",
      " 0.14780945 0.33718747 0.54889631 0.55932915 0.36255786 0.15276992\n",
      " 0.20622896 0.31470382 0.28366324 0.2145766  0.15552099] @epoch 20\n",
      "Epoch 021: val_acc did not improve from 0.275\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0331 - val_loss: 0.0669\n",
      "Counter: 0, Global: 0.27513670455664396, MyBest: 0.27513670455664396\n",
      "\n",
      "Epoch 22/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0327\n",
      "validate accuracy:\n",
      " [0.21011147 0.09463187 0.12809364 0.18792333 0.19406426 0.13657717\n",
      " 0.1273926  0.25220409 0.4822576  0.4821803  0.35431135 0.10627473\n",
      " 0.14800785 0.23484321 0.19500975 0.15714085 0.08087092] @epoch 21\n",
      "Epoch 022: val_acc did not improve from 0.275\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0327 - val_loss: 0.0528\n",
      "Counter: 1, Global: 0.27513670455664396, MyBest: 0.27513670455664396\n",
      "\n",
      "Epoch 23/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0323\n",
      "validate accuracy:\n",
      " [0.28738921 0.15552726 0.15217391 0.31315753 0.32372856 0.14580537\n",
      " 0.13823904 0.36883941 0.54721987 0.60908455 0.44820601 0.18216507\n",
      " 0.22362514 0.31818816 0.27920267 0.22609185 0.16697299] @epoch 22\n",
      "Epoch 023: val_acc improved from 0.275 to 0.287\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0323 - val_loss: 0.0295\n",
      "Counter: 2, Global: 0.28738921228796244, MyBest: 0.27513670455664396\n",
      "\n",
      "Epoch 24/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0321\n",
      "validate accuracy:\n",
      " [0.30019409 0.16486314 0.18444817 0.33448625 0.31321135 0.17281879\n",
      " 0.15546575 0.4136436  0.58829284 0.61802936 0.45341435 0.16266252\n",
      " 0.20917508 0.34675959 0.31669918 0.21050414 0.15863141] @epoch 23\n",
      "Epoch 024: val_acc improved from 0.287 to 0.300\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0321 - val_loss: 0.0362\n",
      "Counter: 0, Global: 0.30019409488886595, MyBest: 0.28738921228796244\n",
      "\n",
      "Epoch 25/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0319\n",
      "validate accuracy:\n",
      " [0.30483628 0.17504774 0.18862876 0.3251189  0.32286415 0.15486577\n",
      " 0.14738409 0.42260441 0.56021237 0.63633823 0.49117476 0.18273036\n",
      " 0.23639169 0.31512195 0.29258433 0.24350512 0.18280786] @epoch 24\n",
      "Epoch 025: val_acc improved from 0.300 to 0.305\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0319 - val_loss: 0.0343\n",
      "Counter: 0, Global: 0.30483628157526255, MyBest: 0.30019409488886595\n",
      "\n",
      "Epoch 26/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0318\n",
      "validate accuracy:\n",
      " [0.30043575 0.15934649 0.1889632  0.29600808 0.29116842 0.17651007\n",
      " 0.13675033 0.37924555 0.61944675 0.60000002 0.49768519 0.18654607\n",
      " 0.19809203 0.3594425  0.34611097 0.19323128 0.178425  ] @epoch 25\n",
      "Epoch 026: val_acc did not improve from 0.305\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0318 - val_loss: 0.0479\n",
      "Counter: 0, Global: 0.30483628157526255, MyBest: 0.30483628157526255\n",
      "\n",
      "Epoch 27/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0317\n",
      "validate accuracy:\n",
      " [0.29707373 0.14534266 0.18294315 0.32958639 0.3189742  0.14379194\n",
      " 0.13143343 0.37664402 0.52710253 0.64430469 0.47887731 0.16789146\n",
      " 0.24424803 0.30034843 0.32269305 0.24926275 0.18973562] @epoch 26\n",
      "Epoch 027: val_acc did not improve from 0.305\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0317 - val_loss: 0.0425\n",
      "Counter: 1, Global: 0.30483628157526255, MyBest: 0.30483628157526255\n",
      "\n",
      "Epoch 28/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0315\n",
      "validate accuracy:\n",
      " [0.30694573 0.17398685 0.17658862 0.33880964 0.33323729 0.16694631\n",
      " 0.15929392 0.4322879  0.60114557 0.62222224 0.49305555 0.19403619\n",
      " 0.1969697  0.36571428 0.29885697 0.2083977  0.14958292] @epoch 27\n",
      "Epoch 028: val_acc improved from 0.305 to 0.307\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0315 - val_loss: 0.0364\n",
      "Counter: 2, Global: 0.30694572906941175, MyBest: 0.30483628157526255\n",
      "\n",
      "Epoch 29/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0314\n",
      "validate accuracy:\n",
      " [0.32107554 0.20199448 0.17976588 0.34486237 0.33914423 0.19463088\n",
      " 0.15695448 0.43749097 0.5944398  0.64150941 0.53703701 0.20576596\n",
      " 0.1934624  0.33519164 0.39224979 0.21036372 0.17234553] @epoch 28\n",
      "Epoch 029: val_acc improved from 0.307 to 0.321\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0314 - val_loss: 0.0296\n",
      "Counter: 0, Global: 0.3210755353793502, MyBest: 0.30694572906941175\n",
      "\n",
      "Epoch 30/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0312\n",
      "validate accuracy:\n",
      " [0.33494165 0.1956291  0.20033444 0.32728058 0.33323729 0.18959731\n",
      " 0.18247554 0.43286601 0.60938811 0.6668064  0.56394678 0.17919728\n",
      " 0.25056118 0.3630662  0.39963758 0.26569301 0.19934964] @epoch 29\n",
      "Epoch 030: val_acc improved from 0.321 to 0.335\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0312 - val_loss: 0.0338\n",
      "Counter: 0, Global: 0.33494165353477, MyBest: 0.3210755353793502\n",
      "\n",
      "Epoch 31/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0312\n",
      "validate accuracy:\n",
      " [0.34119511 0.20963293 0.21839465 0.33016285 0.33323729 0.21644296\n",
      " 0.19948958 0.44428384 0.66135794 0.63535988 0.55931711 0.18159978\n",
      " 0.24565095 0.40724739 0.38709229 0.24280298 0.18704934] @epoch 30\n",
      "Epoch 031: val_acc improved from 0.335 to 0.341\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0312 - val_loss: 0.0349\n",
      "Counter: 0, Global: 0.34119511116296053, MyBest: 0.33494165353477\n",
      "\n",
      "Epoch 32/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0310\n",
      "validate accuracy:\n",
      " [0.32891166 0.20729896 0.16220737 0.34587115 0.34750035 0.17197986\n",
      " 0.18736708 0.44572917 0.6431964  0.62990916 0.54267937 0.18979649\n",
      " 0.24775533 0.33031359 0.38737106 0.2249684  0.19864273] @epoch 31\n",
      "Epoch 032: val_acc did not improve from 0.341\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0310 - val_loss: 0.0309\n",
      "Counter: 0, Global: 0.34119511116296053, MyBest: 0.34119511116296053\n",
      "\n",
      "Epoch 33/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0308\n",
      "validate accuracy:\n",
      " [0.35701743 0.225122   0.19648829 0.36575875 0.34692407 0.2238255\n",
      " 0.22458528 0.44341668 0.64333612 0.65576518 0.58695024 0.1978519\n",
      " 0.27427047 0.39958188 0.45009756 0.2697655  0.20853952] @epoch 32\n",
      "Epoch 033: val_acc improved from 0.341 to 0.357\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0308 - val_loss: 0.0327\n",
      "Counter: 1, Global: 0.35701743327081203, MyBest: 0.34119511116296053\n",
      "\n",
      "Epoch 34/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0309\n",
      "validate accuracy:\n",
      " [0.28846091 0.12815617 0.12324414 0.3196426  0.29880419 0.14697987\n",
      " 0.13653764 0.405839   0.56272703 0.59245282 0.432147   0.18866591\n",
      " 0.18027498 0.35707316 0.36841372 0.2032018  0.17121448] @epoch 33\n",
      "Epoch 034: val_acc did not improve from 0.357\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0309 - val_loss: 0.0294\n",
      "Counter: 0, Global: 0.35701743327081203, MyBest: 0.35701743327081203\n",
      "\n",
      "Epoch 35/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0309\n",
      "validate accuracy:\n",
      " [0.34234773 0.1977509  0.18444817 0.34673584 0.32776257 0.2033557\n",
      " 0.17566992 0.42101461 0.67532831 0.64178896 0.53211808 0.17962125\n",
      " 0.25575197 0.42787457 0.41636464 0.27580395 0.21617419] @epoch 34\n",
      "Epoch 035: val_acc did not improve from 0.357\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0309 - val_loss: 0.0367\n",
      "Counter: 1, Global: 0.35701743327081203, MyBest: 0.35701743327081203\n",
      "\n",
      "Epoch 36/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0305\n",
      "validate accuracy:\n",
      " [0.3484931  0.19966051 0.2048495  0.33794495 0.32905921 0.21459731\n",
      " 0.21288814 0.43604568 0.67490917 0.6156534  0.52025461 0.19686264\n",
      " 0.27974185 0.44655052 0.44535825 0.26485044 0.19666336] @epoch 35\n",
      "Epoch 036: val_acc did not improve from 0.357\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0305 - val_loss: 0.0548\n",
      "Counter: 2, Global: 0.35701743327081203, MyBest: 0.35701743327081203\n",
      "\n",
      "Epoch 37/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0304\n",
      "validate accuracy:\n",
      " [0.36495158 0.22300021 0.21237458 0.33866552 0.34144935 0.22567114\n",
      " 0.21522756 0.44760802 0.69125456 0.68539482 0.5769676  0.20972301\n",
      " 0.28507295 0.43665504 0.44591582 0.28071901 0.22352609] @epoch 36\n",
      "Epoch 037: val_acc improved from 0.357 to 0.365\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0304 - val_loss: 0.0324\n",
      "Counter: 3, Global: 0.36495158076286316, MyBest: 0.35701743327081203\n",
      "\n",
      "Epoch 38/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0303\n",
      "validate accuracy:\n",
      " [0.36367893 0.22703162 0.18946488 0.36993805 0.35009363 0.21510068\n",
      " 0.20267971 0.45526811 0.69083542 0.65576518 0.609375   0.22752967\n",
      " 0.2892817  0.46146342 0.44535825 0.24771802 0.18195957] @epoch 37\n",
      "Epoch 038: val_acc did not improve from 0.365\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0303 - val_loss: 0.0283\n",
      "Counter: 0, Global: 0.36495158076286316, MyBest: 0.36495158076286316\n",
      "\n",
      "Epoch 39/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0302\n",
      "validate accuracy:\n",
      " [0.34904078 0.21451305 0.20451505 0.30508718 0.37083992 0.2011745\n",
      " 0.21288814 0.43980345 0.65549034 0.65394831 0.57161456 0.18329565\n",
      " 0.27595398 0.4420906  0.44466129 0.22342367 0.18535274] @epoch 38\n",
      "Epoch 039: val_acc did not improve from 0.365\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0302 - val_loss: 0.0364\n",
      "Counter: 1, Global: 0.36495158076286316, MyBest: 0.36495158076286316\n",
      "\n",
      "Epoch 40/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0300\n",
      "validate accuracy:\n",
      " [0.37108222 0.21323997 0.22909699 0.37397319 0.36435673 0.22768456\n",
      " 0.21863037 0.47767019 0.7049455  0.71139061 0.59968174 0.20604861\n",
      " 0.24116161 0.45881534 0.46529132 0.23311333 0.21221547] @epoch 39\n",
      "Epoch 040: val_acc improved from 0.365 to 0.371\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0300 - val_loss: 0.0267\n",
      "Counter: 2, Global: 0.37108222115784883, MyBest: 0.36495158076286316\n",
      "\n",
      "Epoch 41/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0300\n",
      "validate accuracy:\n",
      " [0.38100022 0.23657967 0.2125418  0.383917   0.33323729 0.23187919\n",
      " 0.21629094 0.46104929 0.70145291 0.73221523 0.58188659 0.23784624\n",
      " 0.3012065  0.50257838 0.43852803 0.29434067 0.23045383] @epoch 40\n",
      "Epoch 041: val_acc improved from 0.371 to 0.381\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0300 - val_loss: 0.0313\n",
      "Counter: 0, Global: 0.3810002226382494, MyBest: 0.37108222115784883\n",
      "\n",
      "Epoch 42/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0299\n",
      "validate accuracy:\n",
      " [0.36172548 0.21642266 0.20819397 0.37772015 0.31191471 0.22516778\n",
      " 0.20374309 0.40410465 0.71444535 0.7287212  0.494647   0.19827586\n",
      " 0.29868126 0.49449477 0.49038193 0.25909284 0.16160046] @epoch 41\n",
      "Epoch 042: val_acc did not improve from 0.381\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0299 - val_loss: 0.0359\n",
      "Counter: 0, Global: 0.3810002226382494, MyBest: 0.3810002226382494\n",
      "\n",
      "Epoch 43/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0298\n",
      "validate accuracy:\n",
      " [0.36786319 0.22851686 0.23060201 0.37584665 0.35556835 0.22181208\n",
      " 0.18736708 0.48171702 0.70410728 0.66037738 0.53370947 0.18711136\n",
      " 0.23498878 0.5062021  0.50418174 0.23957309 0.23412979] @epoch 42\n",
      "Epoch 043: val_acc did not improve from 0.381\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0298 - val_loss: 0.0322\n",
      "Counter: 1, Global: 0.3810002226382494, MyBest: 0.3810002226382494\n",
      "\n",
      "Epoch 44/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0297\n",
      "validate accuracy:\n",
      " [0.37803712 0.21154255 0.19715719 0.36619109 0.37444171 0.21224833\n",
      " 0.21714164 0.48981068 0.63090247 0.73123688 0.62876159 0.21947429\n",
      " 0.3152357  0.50731707 0.43392807 0.27299535 0.24020925] @epoch 43\n",
      "Epoch 044: val_acc did not improve from 0.381\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0297 - val_loss: 0.0349\n",
      "Counter: 2, Global: 0.3810002226382494, MyBest: 0.3810002226382494\n",
      "\n",
      "Epoch 45/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0296\n",
      "validate accuracy:\n",
      " [0.39883691 0.25249311 0.22441472 0.37642312 0.39245066 0.23288591\n",
      " 0.23713313 0.48749819 0.73847443 0.74744934 0.66030091 0.24604297\n",
      " 0.27076319 0.49672472 0.47616392 0.29616627 0.24600594] @epoch 44\n",
      "Epoch 045: val_acc improved from 0.381 to 0.399\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0296 - val_loss: 0.0267\n",
      "Counter: 3, Global: 0.39883690886199474, MyBest: 0.3810002226382494\n",
      "\n",
      "Epoch 46/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0296\n",
      "validate accuracy:\n",
      " [0.37748347 0.2374284  0.21337792 0.38377288 0.33136436 0.21862416\n",
      " 0.22841345 0.49400201 0.73302597 0.68050313 0.63483799 0.20293951\n",
      " 0.27974185 0.47428572 0.44494006 0.23816879 0.24430935] @epoch 45\n",
      "Epoch 046: val_acc did not improve from 0.399\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0296 - val_loss: 0.0311\n",
      "Counter: 0, Global: 0.39883690886199474, MyBest: 0.39883690886199474\n",
      "\n",
      "Epoch 47/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0295\n",
      "validate accuracy:\n",
      " [0.37725983 0.2372162  0.19648829 0.35552675 0.33323729 0.2442953\n",
      " 0.22075713 0.44486198 0.67868119 0.71726066 0.65986687 0.22399661\n",
      " 0.27693602 0.45226482 0.49024254 0.32270747 0.18181819] @epoch 46\n",
      "Epoch 047: val_acc did not improve from 0.399\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0295 - val_loss: 0.0448\n",
      "Counter: 1, Global: 0.39883690886199474, MyBest: 0.39883690886199474\n",
      "\n",
      "Epoch 48/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0293\n",
      "validate accuracy:\n",
      " [0.40853798 0.24188416 0.23913044 0.37368497 0.36133122 0.25067115\n",
      " 0.24478945 0.49804884 0.75579768 0.76296294 0.67549187 0.17170718\n",
      " 0.33501685 0.5201394  0.52341789 0.32790339 0.2546303 ] @epoch 47\n",
      "Epoch 048: val_acc improved from 0.399 to 0.409\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0293 - val_loss: 0.0277\n",
      "Counter: 2, Global: 0.40853798296302557, MyBest: 0.39883690886199474\n",
      "\n",
      "Epoch 49/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0293\n",
      "validate accuracy:\n",
      " [0.4164424  0.25610015 0.2374582  0.34702405 0.39562023 0.24630873\n",
      " 0.25244576 0.5020957  0.76348144 0.77204752 0.68315971 0.23318259\n",
      " 0.33936587 0.54411149 0.50418174 0.3274821  0.25901315] @epoch 48\n",
      "Epoch 049: val_acc improved from 0.409 to 0.416\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0293 - val_loss: 0.0264\n",
      "Counter: 0, Global: 0.4164424017071724, MyBest: 0.40853798296302557\n",
      "\n",
      "Epoch 50/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0292\n",
      "validate accuracy:\n",
      " [0.41225651 0.25716105 0.24481605 0.39876062 0.39648464 0.25385907\n",
      " 0.25202042 0.51192367 0.71402627 0.74311668 0.69198495 0.26172978\n",
      " 0.30190796 0.5393728  0.54181767 0.28296587 0.20415665] @epoch 49\n",
      "Epoch 050: val_acc did not improve from 0.416\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0292 - val_loss: 0.0311\n",
      "Counter: 0, Global: 0.4164424017071724, MyBest: 0.4164424017071724\n",
      "\n",
      "Epoch 51/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0291\n",
      "validate accuracy:\n",
      " [0.35370149 0.23955017 0.21204014 0.28447902 0.33438987 0.21845637\n",
      " 0.21480221 0.42853013 0.53283042 0.71432567 0.65625    0.18400227\n",
      " 0.29278901 0.42299652 0.46696404 0.29012778 0.16669023] @epoch 50\n",
      "Epoch 051: val_acc did not improve from 0.416\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0291 - val_loss: 0.0713\n",
      "Counter: 1, Global: 0.4164424017071724, MyBest: 0.4164424017071724\n",
      "\n",
      "Epoch 52/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0290\n",
      "validate accuracy:\n",
      " [0.38946407 0.26140463 0.19648829 0.33880964 0.32603371 0.22348994\n",
      " 0.23139089 0.4942911  0.76683432 0.69909155 0.62991899 0.24137931\n",
      " 0.26164421 0.53895468 0.44912183 0.33745259 0.23511946] @epoch 51\n",
      "Epoch 052: val_acc did not improve from 0.416\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0290 - val_loss: 0.0479\n",
      "Counter: 2, Global: 0.4164424017071724, MyBest: 0.4164424017071724\n",
      "\n",
      "Epoch 53/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0289\n",
      "validate accuracy:\n",
      " [0.43434058 0.25037131 0.25618729 0.39285201 0.40051866 0.24513423\n",
      " 0.24649085 0.5129354  0.78751045 0.79245281 0.71195024 0.24378181\n",
      " 0.35578004 0.55804878 0.56913853 0.35837662 0.26792026] @epoch 52\n",
      "Epoch 053: val_acc improved from 0.416 to 0.434\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0289 - val_loss: 0.0311\n",
      "Counter: 3, Global: 0.4343405803665519, MyBest: 0.4164424017071724\n",
      "\n",
      "Epoch 54/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0288\n",
      "validate accuracy:\n",
      " [0.4133298  0.25143221 0.25050166 0.40466926 0.32977957 0.24345638\n",
      " 0.25159508 0.44413933 0.77982676 0.78756112 0.68475115 0.20732053\n",
      " 0.34778339 0.54885018 0.57192641 0.29181296 0.21787077] @epoch 53\n",
      "Epoch 054: val_acc did not improve from 0.434\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0288 - val_loss: 0.0427\n",
      "Counter: 0, Global: 0.4343405803665519, MyBest: 0.4343405803665519\n",
      "\n",
      "Epoch 55/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0288\n",
      "validate accuracy:\n",
      " [0.4340528  0.27455974 0.25100335 0.39933708 0.38596743 0.26895973\n",
      " 0.27605274 0.52334154 0.7915619  0.76575822 0.67751735 0.24674958\n",
      " 0.36686307 0.54397213 0.56286591 0.35627019 0.25406474] @epoch 54\n",
      "Epoch 055: val_acc did not improve from 0.434\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0288 - val_loss: 0.0280\n",
      "Counter: 1, Global: 0.4343405803665519, MyBest: 0.4343405803665519\n",
      "\n",
      "Epoch 56/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0287\n",
      "validate accuracy:\n",
      " [0.4092067  0.26225334 0.25785953 0.39400491 0.30917734 0.25822148\n",
      " 0.25861335 0.37967914 0.78751045 0.77526206 0.70081019 0.20421141\n",
      " 0.31439394 0.56125438 0.56439924 0.25979498 0.25986144] @epoch 55\n",
      "Epoch 056: val_acc did not improve from 0.434\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0287 - val_loss: 0.0440\n",
      "Counter: 2, Global: 0.4343405803665519, MyBest: 0.4343405803665519\n",
      "\n",
      "Epoch 57/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0287\n",
      "validate accuracy:\n",
      " [0.44348559 0.28538087 0.26321071 0.35999423 0.41809538 0.26761746\n",
      " 0.27647808 0.52767742 0.78485608 0.79762405 0.7254051  0.27459016\n",
      " 0.3684063  0.53797907 0.5790354  0.37747508 0.25194401] @epoch 56\n",
      "Epoch 057: val_acc improved from 0.434 to 0.443\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0287 - val_loss: 0.0306\n",
      "Counter: 3, Global: 0.4434855878353119, MyBest: 0.4343405803665519\n",
      "\n",
      "Epoch 58/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0287\n",
      "validate accuracy:\n",
      " [0.45126274 0.27562061 0.26889631 0.38103473 0.42515486 0.27214766\n",
      " 0.26860911 0.51134557 0.80385584 0.80810624 0.70515049 0.29197288\n",
      " 0.37317622 0.60850173 0.55617505 0.39292234 0.27753428] @epoch 57\n",
      "Epoch 058: val_acc improved from 0.443 to 0.451\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0287 - val_loss: 0.0282\n",
      "Counter: 0, Global: 0.45126274414360523, MyBest: 0.4434855878353119\n",
      "\n",
      "Epoch 59/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0285\n",
      "validate accuracy:\n",
      " [0.44453382 0.27137706 0.24481605 0.4074074  0.4139173  0.2671141\n",
      " 0.28136963 0.51857203 0.80511314 0.80559051 0.72410303 0.27614471\n",
      " 0.37149271 0.53505224 0.59492612 0.34812525 0.24741976] @epoch 58\n",
      "Epoch 059: val_acc did not improve from 0.451\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0285 - val_loss: 0.0277\n",
      "Counter: 0, Global: 0.45126274414360523, MyBest: 0.45126274414360523\n",
      "\n",
      "Epoch 60/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0285\n",
      "validate accuracy:\n",
      " [0.41675621 0.27349883 0.2367893  0.35365325 0.36219564 0.22600672\n",
      " 0.27350065 0.43416679 0.77884883 0.73962265 0.7271412  0.26257774\n",
      " 0.29180697 0.59177703 0.47532758 0.37270045 0.26848578] @epoch 59\n",
      "Epoch 060: val_acc did not improve from 0.451\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0285 - val_loss: 0.0390\n",
      "Counter: 1, Global: 0.45126274414360523, MyBest: 0.45126274414360523\n",
      "\n",
      "Epoch 61/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0285\n",
      "validate accuracy:\n",
      " [0.39378969 0.23191173 0.23294315 0.35581496 0.28468519 0.24043624\n",
      " 0.24819227 0.45541263 0.75216544 0.75038433 0.70543981 0.15856417\n",
      " 0.31762066 0.54229963 0.54544187 0.30726022 0.17206277] @epoch 60\n",
      "Epoch 061: val_acc did not improve from 0.451\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0285 - val_loss: 0.0503\n",
      "Counter: 2, Global: 0.45126274414360523, MyBest: 0.45126274414360523\n",
      "\n",
      "Epoch 62/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0283\n",
      "validate accuracy:\n",
      " [0.43572811 0.28113729 0.26772577 0.4218187  0.4250108  0.26694632\n",
      " 0.28796256 0.5299899  0.69823974 0.81229907 0.63773149 0.23049745\n",
      " 0.38720539 0.58174217 0.56885976 0.3266395  0.24784391] @epoch 61\n",
      "Epoch 062: val_acc did not improve from 0.451\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0283 - val_loss: 0.0325\n",
      "Counter: 3, Global: 0.45126274414360523, MyBest: 0.45126274414360523\n",
      "\n",
      "Epoch 63/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0285\n",
      "validate accuracy:\n",
      " [0.42725843 0.28644174 0.24280937 0.40553394 0.38222158 0.2364094\n",
      " 0.21671629 0.52666569 0.79980439 0.79804331 0.73423034 0.22060487\n",
      " 0.28577441 0.49268293 0.60245329 0.32453308 0.28121024] @epoch 62\n",
      "Epoch 063: val_acc did not improve from 0.451\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0285 - val_loss: 0.0446\n",
      "Counter: 4, Global: 0.45126274414360523, MyBest: 0.45126274414360523\n",
      "\n",
      "Epoch 64/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0282\n",
      "validate accuracy:\n",
      " [0.46037343 0.29429239 0.26438126 0.43205073 0.44057053 0.27567115\n",
      " 0.28817526 0.5388062  0.79603243 0.81928724 0.7439236  0.23629169\n",
      " 0.39043209 0.59763068 0.60802901 0.40513971 0.23526084] @epoch 63\n",
      "Epoch 064: val_acc improved from 0.451 to 0.460\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0282 - val_loss: 0.0284\n",
      "Counter: 5, Global: 0.4603734267875552, MyBest: 0.45126274414360523\n",
      "\n",
      "Epoch 65/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0281\n",
      "validate accuracy:\n",
      " [0.40961414 0.27816677 0.24515051 0.39155498 0.35196659 0.26258388\n",
      " 0.26690769 0.50932217 0.80483377 0.60377359 0.54484951 0.20180893\n",
      " 0.30078563 0.61114985 0.59924728 0.30122173 0.28050333] @epoch 64\n",
      "Epoch 065: val_acc did not improve from 0.460\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0281 - val_loss: 0.0588\n",
      "Counter: 0, Global: 0.4603734267875552, MyBest: 0.4603734267875552\n",
      "\n",
      "Epoch 66/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0281\n",
      "validate accuracy:\n",
      " [0.45131029 0.29110971 0.2660535  0.43133017 0.34418672 0.27986577\n",
      " 0.28987664 0.47058824 0.81740707 0.80489171 0.7494213  0.20661391\n",
      " 0.3905724  0.60250872 0.60482299 0.39488837 0.27682737] @epoch 65\n",
      "Epoch 066: val_acc did not improve from 0.460\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0281 - val_loss: 0.0370\n",
      "Counter: 1, Global: 0.4603734267875552, MyBest: 0.4603734267875552\n",
      "\n",
      "Epoch 67/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0280\n",
      "validate accuracy:\n",
      " [0.39795086 0.25461489 0.23795986 0.3829082  0.32185563 0.26661074\n",
      " 0.26329222 0.38387051 0.76334172 0.60139763 0.72916669 0.25692481\n",
      " 0.25406846 0.5549826  0.55631447 0.29686841 0.2430369 ] @epoch 66\n",
      "Epoch 067: val_acc did not improve from 0.460\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0280 - val_loss: 0.0463\n",
      "Counter: 2, Global: 0.4603734267875552, MyBest: 0.4603734267875552\n",
      "\n",
      "Epoch 68/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0280\n",
      "validate accuracy:\n",
      " [0.45793226 0.26628473 0.27725753 0.43334773 0.37833166 0.27416107\n",
      " 0.23649511 0.5476225  0.82020116 0.81453532 0.75376159 0.29253817\n",
      " 0.31776094 0.62216026 0.62155002 0.38884988 0.28205854] @epoch 67\n",
      "Epoch 068: val_acc did not improve from 0.460\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0280 - val_loss: 0.0362\n",
      "Counter: 3, Global: 0.4603734267875552, MyBest: 0.4603734267875552\n",
      "\n",
      "Epoch 69/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0278\n",
      "validate accuracy:\n",
      " [0.46291516 0.2747719  0.28494984 0.43738291 0.38236564 0.26442954\n",
      " 0.30199915 0.47116634 0.82453197 0.80936408 0.76475692 0.21594121\n",
      " 0.40712681 0.62843204 0.63437414 0.42044657 0.28460342] @epoch 68\n",
      "Epoch 069: val_acc improved from 0.460 to 0.463\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0278 - val_loss: 0.0353\n",
      "Counter: 4, Global: 0.4629151551052928, MyBest: 0.4603734267875552\n",
      "\n",
      "Epoch 70/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0279\n",
      "validate accuracy:\n",
      " [0.43755889 0.26034373 0.26153848 0.36518231 0.42702779 0.26476511\n",
      " 0.28711188 0.5325914  0.79519421 0.80866528 0.75318289 0.28363481\n",
      " 0.38818744 0.54118466 0.47044885 0.28745961 0.27442387] @epoch 69\n",
      "Epoch 070: val_acc did not improve from 0.463\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0279 - val_loss: 0.0455\n",
      "Counter: 0, Global: 0.4629151551052928, MyBest: 0.4629151551052928\n",
      "\n",
      "Epoch 71/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0278\n",
      "validate accuracy:\n",
      " [0.45148338 0.25716105 0.26304349 0.43940049 0.41449359 0.28976509\n",
      " 0.30263719 0.50036132 0.77256215 0.73990214 0.76345485 0.25890332\n",
      " 0.39828843 0.5814634  0.57206577 0.42606375 0.24416797] @epoch 70\n",
      "Epoch 071: val_acc did not improve from 0.463\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0278 - val_loss: 0.0459\n",
      "Counter: 1, Global: 0.4629151551052928, MyBest: 0.4629151551052928\n",
      "\n",
      "Epoch 72/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0278\n",
      "validate accuracy:\n",
      " [0.4698461  0.30256736 0.28612041 0.44473267 0.46203718 0.28506711\n",
      " 0.30837941 0.56482148 0.76669461 0.83312368 0.7583912  0.29918033\n",
      " 0.41666666 0.57031357 0.5560357  0.36933014 0.29407606] @epoch 71\n",
      "Epoch 072: val_acc improved from 0.463 to 0.470\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0278 - val_loss: 0.0309\n",
      "Counter: 2, Global: 0.4698460977524519, MyBest: 0.4629151551052928\n",
      "\n",
      "Epoch 73/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0277\n",
      "validate accuracy:\n",
      " [0.47198447 0.30638659 0.27608696 0.39688715 0.4735629  0.26426175\n",
      " 0.28200766 0.56857926 0.83556861 0.82921034 0.76345485 0.258762\n",
      " 0.42185748 0.63066202 0.60635626 0.38884988 0.24925774] @epoch 72\n",
      "Epoch 073: val_acc improved from 0.470 to 0.472\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0277 - val_loss: 0.0307\n",
      "Counter: 0, Global: 0.4719844665378332, MyBest: 0.4698460977524519\n",
      "\n",
      "Epoch 74/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0276\n",
      "validate accuracy:\n",
      " [0.45450537 0.29089752 0.29966554 0.40178701 0.42414638 0.28154361\n",
      " 0.29094002 0.52681023 0.77284157 0.83032846 0.75347221 0.30144149\n",
      " 0.42508417 0.55986065 0.58391416 0.33099285 0.19835997] @epoch 73\n",
      "Epoch 074: val_acc did not improve from 0.472\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0276 - val_loss: 0.0412\n",
      "Counter: 0, Global: 0.4719844665378332, MyBest: 0.4719844665378332\n",
      "\n",
      "Epoch 75/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0276\n",
      "validate accuracy:\n",
      " [0.42435951 0.27986419 0.24949832 0.37209973 0.38092494 0.25369129\n",
      " 0.26712036 0.53013444 0.80860573 0.62571627 0.62659144 0.2574901\n",
      " 0.32744107 0.62104529 0.62085307 0.36989188 0.19878411] @epoch 74\n",
      "Epoch 075: val_acc did not improve from 0.472\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0276 - val_loss: 0.0519\n",
      "Counter: 1, Global: 0.4719844665378332, MyBest: 0.4719844665378332\n",
      "\n",
      "Epoch 76/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0275\n",
      "validate accuracy:\n",
      " [0.45114597 0.29768726 0.28645486 0.46606138 0.41002738 0.27869126\n",
      " 0.30540195 0.55730599 0.82788491 0.70132774 0.6863426  0.23855285\n",
      " 0.32954547 0.59010452 0.59185952 0.37199831 0.27908948] @epoch 75\n",
      "Epoch 076: val_acc did not improve from 0.472\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0275 - val_loss: 0.0328\n",
      "Counter: 2, Global: 0.4719844665378332, MyBest: 0.4719844665378332\n",
      "\n",
      "Epoch 77/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0275\n",
      "validate accuracy:\n",
      " [0.47405721 0.29959685 0.28377926 0.47283471 0.47284254 0.27751678\n",
      " 0.22458528 0.55745047 0.81084102 0.81761009 0.75434029 0.29918033\n",
      " 0.41498315 0.61156791 0.59590185 0.40205029 0.28983459] @epoch 76\n",
      "Epoch 077: val_acc improved from 0.472 to 0.474\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0275 - val_loss: 0.0283\n",
      "Counter: 3, Global: 0.47405721340328455, MyBest: 0.4719844665378332\n",
      "\n",
      "Epoch 78/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0274\n",
      "validate accuracy:\n",
      " [0.49439301 0.30490133 0.29180601 0.4860931  0.48119867 0.24765101\n",
      " 0.30625266 0.57378232 0.83892149 0.83591896 0.78269678 0.30440927\n",
      " 0.43532547 0.65059233 0.6459437  0.42760849 0.29718649] @epoch 77\n",
      "Epoch 078: val_acc improved from 0.474 to 0.494\n",
      "576/576 [==============================] - 128s 222ms/step - loss: 0.0274 - val_loss: 0.0260\n",
      "Counter: 0, Global: 0.4943930059671402, MyBest: 0.47405721340328455\n",
      "\n",
      "Epoch 79/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0273\n",
      "validate accuracy:\n",
      " [0.45779232 0.28750265 0.27959865 0.44199452 0.36320415 0.29496643\n",
      " 0.29136539 0.5129354  0.82048059 0.63284415 0.76851851 0.28815717\n",
      " 0.41863075 0.63163763 0.62141067 0.41609326 0.25533721] @epoch 78\n",
      "Epoch 079: val_acc did not improve from 0.494\n",
      "576/576 [==============================] - 128s 221ms/step - loss: 0.0273 - val_loss: 0.0483\n",
      "Counter: 0, Global: 0.4943930059671402, MyBest: 0.4943930059671402\n",
      "\n",
      "Epoch 80/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0274\n",
      "validate accuracy:\n",
      " [0.50534421 0.29832378 0.30986622 0.48292261 0.49157181 0.30553693\n",
      " 0.30540195 0.58650094 0.84339201 0.8408106  0.79282409 0.30667043\n",
      " 0.44851291 0.65519166 0.65179819 0.45386884 0.31231442] @epoch 79\n",
      "Epoch 080: val_acc improved from 0.494 to 0.505\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0274 - val_loss: 0.0249\n",
      "Counter: 1, Global: 0.5053442120552063, MyBest: 0.4943930059671402\n",
      "\n",
      "Epoch 81/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0273\n",
      "validate accuracy:\n",
      " [0.49705307 0.29259494 0.27826086 0.48652545 0.47140181 0.30436242\n",
      " 0.30965546 0.58548921 0.83975971 0.83410203 0.79282409 0.29366875\n",
      " 0.44304153 0.65811849 0.6568163  0.43434912 0.27187899] @epoch 80\n",
      "Epoch 081: val_acc did not improve from 0.505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0273 - val_loss: 0.0277\n",
      "Counter: 0, Global: 0.5053442120552063, MyBest: 0.5053442120552063\n",
      "\n",
      "Epoch 82/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0272\n",
      "validate accuracy:\n",
      " [0.47198385 0.29980904 0.30150503 0.4837873  0.42933294 0.30117449\n",
      " 0.25499788 0.53923976 0.78303993 0.82697415 0.7332176  0.29734313\n",
      " 0.4332211  0.62898952 0.60956234 0.33773348 0.29181394] @epoch 81\n",
      "Epoch 082: val_acc did not improve from 0.505\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0272 - val_loss: 0.0384\n",
      "Counter: 1, Global: 0.5053442120552063, MyBest: 0.5053442120552063\n",
      "\n",
      "Epoch 83/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0272\n",
      "validate accuracy:\n",
      " [0.46624859 0.28580523 0.27056855 0.46606138 0.42169717 0.28255033\n",
      " 0.28136963 0.55932939 0.69125456 0.82557654 0.77184606 0.29508197\n",
      " 0.42746913 0.53268296 0.63953161 0.42002529 0.28912768] @epoch 82\n",
      "Epoch 083: val_acc did not improve from 0.505\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0272 - val_loss: 0.0336\n",
      "Counter: 2, Global: 0.5053442120552063, MyBest: 0.5053442120552063\n",
      "\n",
      "Epoch 84/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0271\n",
      "validate accuracy:\n",
      " [0.49950319 0.29959685 0.28862876 0.50511599 0.49013111 0.30486578\n",
      " 0.31071883 0.59083682 0.83808327 0.83228511 0.78776044 0.30553985\n",
      " 0.42873177 0.65003484 0.6423195  0.43364695 0.28375512] @epoch 83\n",
      "Epoch 084: val_acc did not improve from 0.505\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0271 - val_loss: 0.0253\n",
      "Counter: 3, Global: 0.5053442120552063, MyBest: 0.5053442120552063\n",
      "\n",
      "Epoch 85/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0271\n",
      "validate accuracy:\n",
      " [0.47314823 0.29683852 0.29698998 0.4729788  0.48724967 0.28993288\n",
      " 0.30518928 0.57103628 0.74518019 0.83703703 0.63368058 0.29691917\n",
      " 0.43742985 0.64919859 0.64204073 0.30992839 0.2987417 ] @epoch 84\n",
      "Epoch 085: val_acc did not improve from 0.505\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0271 - val_loss: 0.0404\n",
      "Counter: 4, Global: 0.5053442120552063, MyBest: 0.5053442120552063\n",
      "\n",
      "Epoch 86/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0270\n",
      "validate accuracy:\n",
      " [0.49951042 0.30086994 0.28043479 0.4928664  0.4937329  0.29362416\n",
      " 0.3102935  0.57999712 0.82942164 0.83144653 0.78804976 0.30794233\n",
      " 0.43658811 0.6426481  0.64747697 0.44446003 0.31231442] @epoch 85\n",
      "Epoch 086: val_acc did not improve from 0.505\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0270 - val_loss: 0.0254\n",
      "Counter: 5, Global: 0.5053442120552063, MyBest: 0.5053442120552063\n",
      "\n",
      "Epoch 87/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0270\n",
      "validate accuracy:\n",
      " [0.47703015 0.31826863 0.30351171 0.46966422 0.41579023 0.2785235\n",
      " 0.30114844 0.54574358 0.81307626 0.83647799 0.78081596 0.2512719\n",
      " 0.42817059 0.58327526 0.59799278 0.45836258 0.2503888 ] @epoch 86\n",
      "Epoch 087: val_acc did not improve from 0.505\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0270 - val_loss: 0.0370\n",
      "Counter: 6, Global: 0.5053442120552063, MyBest: 0.5053442120552063\n",
      "\n",
      "Epoch 88/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0269\n",
      "validate accuracy:\n",
      " [0.49717138 0.30681095 0.28578594 0.50756592 0.51606399 0.3112416\n",
      " 0.32390472 0.59531724 0.81992179 0.84248775 0.79918981 0.272329\n",
      " 0.40151516 0.6096167  0.61235017 0.43154052 0.3191008 ] @epoch 87\n",
      "Epoch 088: val_acc did not improve from 0.505\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0269 - val_loss: 0.0333\n",
      "Counter: 7, Global: 0.5053442120552063, MyBest: 0.5053442120552063\n",
      "\n",
      "Epoch 088: Updating Learning rate.. New value is 0.000050\n",
      "Epoch 89/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0235\n",
      "validate accuracy:\n",
      " [0.51134689 0.31147888 0.3075251  0.50526011 0.51534361 0.31644297\n",
      " 0.32177797 0.59589535 0.84758312 0.84472394 0.79904515 0.31953081\n",
      " 0.41217732 0.66996515 0.66699189 0.43224266 0.31556624] @epoch 88\n",
      "Epoch 089: val_acc improved from 0.505 to 0.511\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0235 - val_loss: 0.0250\n",
      "Counter: 0, Global: 0.5113468915224075, MyBest: 0.5053442120552063\n",
      "\n",
      "Epoch 90/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0235\n",
      "validate accuracy:\n",
      " [0.51967978 0.31657118 0.30635452 0.52010375 0.52024204 0.31526846\n",
      " 0.32518077 0.60326636 0.84716403 0.84528303 0.79817706 0.32263991\n",
      " 0.46001685 0.6733101  0.67465848 0.46032861 0.32631132] @epoch 89\n",
      "Epoch 090: val_acc improved from 0.511 to 0.520\n",
      "576/576 [==============================] - 128s 222ms/step - loss: 0.0235 - val_loss: 0.0244\n",
      "Counter: 0, Global: 0.5196797791868448, MyBest: 0.5113468915224075\n",
      "\n",
      "Epoch 91/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0235\n",
      "validate accuracy:\n",
      " [0.51619413 0.31593466 0.30234113 0.51650095 0.51404697 0.32013422\n",
      " 0.31837517 0.59893048 0.84604639 0.84654087 0.796875   0.32179198\n",
      " 0.45679012 0.66341466 0.66768891 0.45120066 0.322494  ] @epoch 90\n",
      "Epoch 091: val_acc did not improve from 0.520\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0235 - val_loss: 0.0253\n",
      "Counter: 0, Global: 0.5196797791868448, MyBest: 0.5196797791868448\n",
      "\n",
      "Epoch 92/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0235\n",
      "validate accuracy:\n",
      " [0.51529373 0.31402504 0.30886286 0.51289809 0.50511456 0.31744966\n",
      " 0.31837517 0.59791875 0.84506845 0.84192872 0.8029514  0.32009611\n",
      " 0.45426488 0.66522646 0.6653192  0.45850301 0.3166973 ] @epoch 91\n",
      "Epoch 092: val_acc did not improve from 0.520\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0235 - val_loss: 0.0245\n",
      "Counter: 1, Global: 0.5196797791868448, MyBest: 0.5196797791868448\n",
      "\n",
      "Epoch 93/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0235\n",
      "validate accuracy:\n",
      " [0.51199077 0.31126672 0.30685619 0.48811069 0.49229217 0.32197988\n",
      " 0.31922585 0.59343839 0.84772283 0.84947592 0.79383683 0.32348785\n",
      " 0.46212122 0.66843206 0.66211319 0.43111923 0.32037324] @epoch 92\n",
      "Epoch 093: val_acc did not improve from 0.520\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0235 - val_loss: 0.0248\n",
      "Counter: 2, Global: 0.5196797791868448, MyBest: 0.5196797791868448\n",
      "\n",
      "Epoch 94/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0235\n",
      "validate accuracy:\n",
      " [0.49283213 0.30956927 0.2923077  0.50857472 0.42832446 0.31359062\n",
      " 0.30412591 0.51177913 0.84548759 0.78784066 0.7957176  0.32278123\n",
      " 0.42382154 0.66216028 0.66183442 0.41328466 0.30411422] @epoch 93\n",
      "Epoch 094: val_acc did not improve from 0.520\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0235 - val_loss: 0.0297\n",
      "Counter: 3, Global: 0.5196797791868448, MyBest: 0.5196797791868448\n",
      "\n",
      "Epoch 95/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0235\n",
      "validate accuracy:\n",
      " [0.51769899 0.32230002 0.30317727 0.52197725 0.51303846 0.32583892\n",
      " 0.31518504 0.5951727  0.84786254 0.84891683 0.80613428 0.32716224\n",
      " 0.44696969 0.66898954 0.67382216 0.44951552 0.31712145] @epoch 94\n",
      "Epoch 095: val_acc did not improve from 0.520\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0235 - val_loss: 0.0250\n",
      "Counter: 4, Global: 0.5196797791868448, MyBest: 0.5196797791868448\n",
      "\n",
      "Epoch 96/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0234\n",
      "validate accuracy:\n",
      " [0.51100049 0.29089752 0.30886286 0.51232165 0.50497049 0.29765099\n",
      " 0.29327947 0.59878594 0.84478903 0.84430468 0.80439812 0.32108536\n",
      " 0.45791245 0.65101045 0.66448283 0.46201375 0.31924218] @epoch 95\n",
      "Epoch 096: val_acc did not improve from 0.520\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0234 - val_loss: 0.0270\n",
      "Counter: 5, Global: 0.5196797791868448, MyBest: 0.5196797791868448\n",
      "\n",
      "Epoch 97/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0234\n",
      "validate accuracy:\n",
      " [0.51038258 0.31826863 0.30200669 0.51131284 0.51145369 0.29328859\n",
      " 0.28966397 0.59488368 0.84632581 0.84164917 0.79759836 0.30893159\n",
      " 0.43630752 0.66787457 0.67145246 0.45190284 0.32320091] @epoch 96\n",
      "Epoch 097: val_acc did not improve from 0.520\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0234 - val_loss: 0.0267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 6, Global: 0.5196797791868448, MyBest: 0.5196797791868448\n",
      "\n",
      "Epoch 98/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0234\n",
      "validate accuracy:\n",
      " [0.51220785 0.29683852 0.30852842 0.51419514 0.51159775 0.3010067\n",
      " 0.29902169 0.58693451 0.84590667 0.84765899 0.80338544 0.31825891\n",
      " 0.44570708 0.66675961 0.66824645 0.45850301 0.32277676] @epoch 97\n",
      "Epoch 098: val_acc did not improve from 0.520\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0234 - val_loss: 0.0251\n",
      "Counter: 7, Global: 0.5196797791868448, MyBest: 0.5196797791868448\n",
      "\n",
      "Epoch 098: Updating Learning rate.. New value is 0.000010\n",
      "Epoch 99/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52037099 0.31678337 0.30735785 0.51664507 0.51404697 0.32449666\n",
      " 0.32050192 0.60398901 0.84842134 0.85073376 0.8052662  0.32094404\n",
      " 0.46408531 0.66926831 0.67354333 0.46608624 0.32376644] @epoch 98\n",
      "Epoch 099: val_acc improved from 0.520 to 0.520\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 0, Global: 0.5203709881752729, MyBest: 0.5196797791868448\n",
      "\n",
      "Epoch 100/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52092742 0.31784427 0.30602008 0.51765382 0.5157758  0.32130873\n",
      " 0.3215653  0.60268825 0.84702432 0.85115302 0.80685765 0.32447711\n",
      " 0.46506733 0.67135888 0.67674935 0.46496278 0.32433197] @epoch 99\n",
      "Epoch 100: val_acc improved from 0.520 to 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 0, Global: 0.5209274161607027, MyBest: 0.5203709881752729\n",
      "\n",
      "Epoch 101/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.51949912 0.31126672 0.30685619 0.51981556 0.51534361 0.32365772\n",
      " 0.30901745 0.60384446 0.84702432 0.84849757 0.80656826 0.32405314\n",
      " 0.46282268 0.67052263 0.67368275 0.46482235 0.32419059] @epoch 100\n",
      "Epoch 101: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 0, Global: 0.5209274161607027, MyBest: 0.5209274161607027\n",
      "\n",
      "Epoch 102/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52113766 0.31911734 0.30903009 0.52183312 0.51462322 0.32634228\n",
      " 0.32411739 0.60471165 0.84674489 0.84877706 0.80685765 0.32518372\n",
      " 0.46296296 0.66898954 0.67214942 0.46313721 0.32362506] @epoch 101\n",
      "Epoch 102: val_acc improved from 0.521 to 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 1, Global: 0.5211376622319221, MyBest: 0.5209274161607027\n",
      "\n",
      "Epoch 103/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52024215 0.31232759 0.30735785 0.51895088 0.5157758  0.32651007\n",
      " 0.31922585 0.60659057 0.8461861  0.84821802 0.8064236  0.32263991\n",
      " 0.463945   0.67108011 0.67089492 0.46369892 0.3240492 ] @epoch 102\n",
      "Epoch 103: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 0, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 104/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52037967 0.31593466 0.30802676 0.51722151 0.51332659 0.32416108\n",
      " 0.32177797 0.60355544 0.84800225 0.84989518 0.80512154 0.32221594\n",
      " 0.46478677 0.67205572 0.67493725 0.46454149 0.32051462] @epoch 103\n",
      "Epoch 104: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 1, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 105/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.51919292 0.31763208 0.30635452 0.5175097  0.51260626 0.32348993\n",
      " 0.3213526  0.60008669 0.84646547 0.84765899 0.80627894 0.32334653\n",
      " 0.46057799 0.66522646 0.67117369 0.46327764 0.3240492 ] @epoch 104\n",
      "Epoch 105: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 2, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 106/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.51979106 0.31911734 0.30719063 0.51823032 0.51073331 0.32348993\n",
      " 0.32220331 0.60297734 0.84590667 0.84891683 0.80483216 0.32348785\n",
      " 0.46338382 0.67066205 0.67187065 0.46271592 0.3209388 ] @epoch 105\n",
      "Epoch 106: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 3, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 107/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.51604286 0.31911734 0.30936456 0.51981556 0.49344474 0.32499999\n",
      " 0.3109315  0.58794624 0.84590667 0.8483578  0.80627894 0.32150933\n",
      " 0.46029741 0.665784   0.66671312 0.45429012 0.32192847] @epoch 106\n",
      "Epoch 107: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0248\n",
      "Counter: 4, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 108/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.51996521 0.31784427 0.30702341 0.521689   0.51159775 0.32449666\n",
      " 0.32241601 0.60037577 0.84814197 0.84793848 0.8041088  0.32278123\n",
      " 0.46352413 0.66954702 0.66950095 0.46327764 0.32518026] @epoch 107\n",
      "Epoch 108: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 5, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 109/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52012496 0.31657118 0.3061873  0.52212137 0.51606399 0.32416108\n",
      " 0.32199064 0.60211015 0.84688461 0.84821802 0.80541086 0.3232052\n",
      " 0.46212122 0.66731709 0.66922218 0.46664795 0.32376644] @epoch 108\n",
      "Epoch 109: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 6, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 110/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.51960306 0.31635901 0.30719063 0.51693326 0.51044518 0.32516778\n",
      " 0.32177797 0.60196561 0.84814197 0.85017473 0.80714697 0.32278123\n",
      " 0.46127945 0.6701045  0.66615558 0.46454149 0.32348368] @epoch 109\n",
      "Epoch 110: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 7, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 110: Updating Learning rate.. New value is 0.000002\n",
      "Epoch 111/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52044494 0.31784427 0.30819398 0.51923907 0.51231813 0.32315436\n",
      " 0.32220331 0.60153204 0.84716403 0.84849757 0.80671299 0.32391182\n",
      " 0.46422559 0.66954702 0.67117369 0.46706924 0.32433197] @epoch 110\n",
      "Epoch 111: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 0, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 112/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52040748 0.31805643 0.30719063 0.51851851 0.51303846 0.32432887\n",
      " 0.32199064 0.60124296 0.84953898 0.84919637 0.80570024 0.32278123\n",
      " 0.46352413 0.67066205 0.67117369 0.46510321 0.32447335] @epoch 111\n",
      "Epoch 112: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 1, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 113/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52001881 0.31614682 0.3075251  0.51693326 0.50986892 0.3261745\n",
      " 0.31965122 0.60153204 0.84772283 0.84989518 0.80685765 0.32334653\n",
      " 0.46492705 0.66898954 0.67256761 0.46510321 0.32305953] @epoch 112\n",
      "Epoch 113: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 2, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 114/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.5203951  0.31614682 0.30852842 0.52024788 0.51375884 0.3248322\n",
      " 0.32092726 0.60167652 0.84758312 0.84877706 0.80570024 0.32362917\n",
      " 0.46296296 0.67052263 0.67075551 0.46664795 0.32362506] @epoch 113\n",
      "Epoch 114: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 3, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 115/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52049551 0.31741992 0.30869564 0.51952732 0.51303846 0.32432887\n",
      " 0.32050192 0.60239917 0.84925956 0.85059398 0.8058449  0.3232052\n",
      " 0.46352413 0.66996515 0.67117369 0.46440107 0.3240492 ] @epoch 114\n",
      "Epoch 115: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 4, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 116/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52025973 0.31805643 0.30702341 0.51967144 0.5124622  0.32416108\n",
      " 0.32007656 0.60109842 0.84786254 0.85017473 0.80541086 0.32348785\n",
      " 0.46352413 0.66926831 0.67089492 0.46622667 0.32475612] @epoch 115\n",
      "Epoch 116: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 5, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 117/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52049475 0.31741992 0.3075251  0.52212137 0.51289439 0.3248322\n",
      " 0.32028922 0.59965312 0.84884048 0.84891683 0.8064236  0.32278123\n",
      " 0.46338382 0.67066205 0.66950095 0.4673501  0.32532164] @epoch 116\n",
      "Epoch 117: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 126s 219ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 6, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 118/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52044429 0.31869298 0.3075251  0.52096844 0.51073331 0.32348993\n",
      " 0.32113993 0.60037577 0.84828162 0.84877706 0.80700231 0.32391182\n",
      " 0.46338382 0.67038327 0.67089492 0.46622667 0.32532164] @epoch 117\n",
      "Epoch 118: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 7, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 118: Updating Learning rate.. New value is 0.000000\n",
      "Epoch 119/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52010756 0.31869298 0.30685619 0.52082431 0.51289439 0.32332215\n",
      " 0.32262868 0.59936404 0.84646547 0.84877706 0.80555558 0.32278123\n",
      " 0.46198091 0.6701045  0.66936159 0.46650752 0.32560441] @epoch 118\n",
      "Epoch 119: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 0, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 120/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52060878 0.31996605 0.30769232 0.52053612 0.51347071 0.32348993\n",
      " 0.32241601 0.59994221 0.8474434  0.84849757 0.80598956 0.32475975\n",
      " 0.46324354 0.66857141 0.67019796 0.46678838 0.32673547] @epoch 119\n",
      "Epoch 120: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 1, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 121/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.51988416 0.31741992 0.30936456 0.52082431 0.5124622  0.32281879\n",
      " 0.31986389 0.60066485 0.84716403 0.84947592 0.80541086 0.32263991\n",
      " 0.46184063 0.66871083 0.67033732 0.46566492 0.32348368] @epoch 120\n",
      "Epoch 121: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 2, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 122/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52026978 0.31699553 0.30802676 0.52125669 0.51289439 0.322651\n",
      " 0.32241601 0.60066485 0.8474434  0.84933615 0.80671299 0.32362917\n",
      " 0.46296296 0.66843206 0.67117369 0.46468192 0.32503888] @epoch 121\n",
      "Epoch 122: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 3, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 123/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52030539 0.31826863 0.30852842 0.52024788 0.51188588 0.32332215\n",
      " 0.3213526  0.59936404 0.8461861  0.85003495 0.80627894 0.32405314\n",
      " 0.4622615  0.6701045  0.67214942 0.46524364 0.32560441] @epoch 122\n",
      "Epoch 123: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 4, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 124/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52056281 0.31826863 0.30852842 0.52197725 0.51260626 0.32298657\n",
      " 0.32177797 0.59907502 0.84828162 0.84961563 0.80743635 0.32447711\n",
      " 0.46324354 0.66940767 0.67047673 0.46552449 0.32532164] @epoch 123\n",
      "Epoch 124: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 5, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 125/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52057985 0.31848079 0.30802676 0.52183312 0.5124622  0.32332215\n",
      " 0.3213526  0.59893048 0.8474434  0.8490566  0.80627894 0.32405314\n",
      " 0.46324354 0.67052263 0.67145246 0.46650752 0.32631132] @epoch 124\n",
      "Epoch 125: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 6, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 126/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52054906 0.31678337 0.30769232 0.52053612 0.51145369 0.32533556\n",
      " 0.32071459 0.60167652 0.84674489 0.84919637 0.80671299 0.32475975\n",
      " 0.463945   0.67052263 0.67173123 0.46664795 0.32433197] @epoch 125\n",
      "Epoch 126: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 7, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 126: Updating Learning rate.. New value is 0.000000\n",
      "Epoch 127/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52065157 0.31975389 0.3083612  0.52125669 0.5143351  0.3239933\n",
      " 0.32199064 0.59907502 0.84730369 0.8490566  0.80627894 0.32362917\n",
      " 0.46352413 0.66912889 0.67019796 0.46594578 0.32659408] @epoch 126\n",
      "Epoch 127: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 0, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 128/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52030966 0.31720772 0.3083612  0.52125669 0.51217401 0.32449666\n",
      " 0.32177797 0.60239917 0.84604639 0.8490566  0.8058449  0.32362917\n",
      " 0.46352413 0.67024392 0.66964036 0.46482235 0.32447335] @epoch 127\n",
      "Epoch 128: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 1, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 129/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52029907 0.31826863 0.3075251  0.52125669 0.51318252 0.32315436\n",
      " 0.32092726 0.59994221 0.8461861  0.84919637 0.80627894 0.32278123\n",
      " 0.46380472 0.66926831 0.67061609 0.46622667 0.32616994] @epoch 128\n",
      "Epoch 129: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 2, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 130/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.5204867  0.31954169 0.30785954 0.52226543 0.51260626 0.32315436\n",
      " 0.3213526  0.59893048 0.84814197 0.84919637 0.80671299 0.32292256\n",
      " 0.46338382 0.67024392 0.66977978 0.46538407 0.32631132] @epoch 129\n",
      "Epoch 130: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 3, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 131/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52024836 0.31784427 0.30719063 0.520392   0.51174182 0.32449666\n",
      " 0.32113993 0.60037577 0.84688461 0.84947592 0.80613428 0.32362917\n",
      " 0.46324354 0.66996515 0.67103428 0.46496278 0.32546303] @epoch 130\n",
      "Epoch 131: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 4, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 132/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52014147 0.31678337 0.30668896 0.51952732 0.51159775 0.32315436\n",
      " 0.32177797 0.60052031 0.84730369 0.84961563 0.80714697 0.32348785\n",
      " 0.46268237 0.67094076 0.67061609 0.46580535 0.32461473] @epoch 131\n",
      "Epoch 132: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 5, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 133/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52010449 0.31720772 0.30903009 0.51967144 0.51260626 0.32365772\n",
      " 0.32199064 0.59878594 0.84786254 0.84891683 0.80671299 0.32249859\n",
      " 0.46296296 0.66885018 0.66866463 0.46650752 0.32574579] @epoch 132\n",
      "Epoch 133: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 6, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 134/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52008137 0.31848079 0.3083612  0.52183312 0.51260626 0.32315436\n",
      " 0.32113993 0.6013875  0.84660518 0.84877706 0.80598956 0.32263991\n",
      " 0.46268237 0.66885018 0.66991913 0.46426064 0.32461473] @epoch 133\n",
      "Epoch 134: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 7, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 134: Updating Learning rate.. New value is 0.000000\n",
      "Epoch 135/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52028728 0.31805643 0.30568561 0.51995963 0.51073331 0.32298657\n",
      " 0.32050192 0.60109842 0.84716403 0.84933615 0.80685765 0.32377049\n",
      " 0.46380472 0.67135888 0.67131305 0.46664795 0.32532164] @epoch 134\n",
      "Epoch 135: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0242\n",
      "Counter: 0, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 136/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0234\n",
      "validate accuracy:\n",
      " [0.52007168 0.31741992 0.30785954 0.51952732 0.51044518 0.32298657\n",
      " 0.32050192 0.59878594 0.84814197 0.84877706 0.80671299 0.32391182\n",
      " 0.46254209 0.6701045  0.67075551 0.46692881 0.32574579] @epoch 135\n",
      "Epoch 136: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0234 - val_loss: 0.0243\n",
      "Counter: 1, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 137/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52048044 0.31720772 0.3083612  0.52111256 0.51260626 0.32348993\n",
      " 0.32092726 0.60095388 0.84716403 0.84863734 0.80613428 0.32292256\n",
      " 0.46478677 0.67024392 0.67145246 0.46664795 0.32503888] @epoch 136\n",
      "Epoch 137: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 2, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 138/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52055014 0.31911734 0.3083612  0.52111256 0.51462322 0.32315436\n",
      " 0.32177797 0.60109842 0.8461861  0.84919637 0.80627894 0.3232052\n",
      " 0.463945   0.66871083 0.67005855 0.46566492 0.32631132] @epoch 137\n",
      "Epoch 138: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 3, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 139/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52040247 0.31890514 0.30735785 0.52096844 0.51275033 0.32298657\n",
      " 0.32007656 0.60037577 0.84842134 0.84933615 0.80613428 0.32391182\n",
      " 0.46282268 0.6701045  0.67145246 0.46706924 0.32376644] @epoch 138\n",
      "Epoch 139: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0242\n",
      "Counter: 4, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 140/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52043562 0.31826863 0.30852842 0.52111256 0.51289439 0.32365772\n",
      " 0.32092726 0.59936404 0.84814197 0.8490566  0.8064236  0.32433578\n",
      " 0.46296296 0.66926831 0.67075551 0.46524364 0.32602856] @epoch 139\n",
      "Epoch 140: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 5, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 141/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52036425 0.31784427 0.30769232 0.52096844 0.51332659 0.32298657\n",
      " 0.3213526  0.59878594 0.84856105 0.84947592 0.80555558 0.32377049\n",
      " 0.46212122 0.67024392 0.67145246 0.46608624 0.32560441] @epoch 140\n",
      "Epoch 141: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 6, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 142/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.5205449  0.31848079 0.3075251  0.52111256 0.51275033 0.32365772\n",
      " 0.32092726 0.59994221 0.84856105 0.84961563 0.80714697 0.32362917\n",
      " 0.46212122 0.67094076 0.67075551 0.46566492 0.32588717] @epoch 141\n",
      "Epoch 142: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 7, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 142: Updating Learning rate.. New value is 0.000000\n",
      "Epoch 143/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52034427 0.31763208 0.30852842 0.51995963 0.5139029  0.32348993\n",
      " 0.32113993 0.59936404 0.84814197 0.84975541 0.80613428 0.32377049\n",
      " 0.4622615  0.66940767 0.67089492 0.46608624 0.32503888] @epoch 142\n",
      "Epoch 143: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 0, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 144/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52046616 0.31975389 0.30903009 0.52125669 0.51361477 0.32231542\n",
      " 0.3215653  0.60023123 0.84562725 0.84947592 0.8064236  0.32348785\n",
      " 0.46282268 0.66968644 0.67033732 0.4663671  0.32546303] @epoch 143\n",
      "Epoch 144: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 1, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 145/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52009509 0.31954169 0.30903009 0.51981556 0.51289439 0.32365772\n",
      " 0.3213526  0.59893048 0.84604639 0.8490566  0.80541086 0.32292256\n",
      " 0.46268237 0.66926831 0.66936159 0.46594578 0.32560441] @epoch 144\n",
      "Epoch 145: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 2, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 146/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.51998952 0.31890514 0.30785954 0.52053612 0.51217401 0.32315436\n",
      " 0.32071459 0.59994221 0.84674489 0.8490566  0.80483216 0.32249859\n",
      " 0.463945   0.66898954 0.67019796 0.46538407 0.3248975 ] @epoch 145\n",
      "Epoch 146: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 127s 220ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 3, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 147/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52055926 0.31975389 0.30769232 0.52197725 0.5139029  0.32382551\n",
      " 0.32284135 0.59994221 0.84688461 0.8490566  0.80700231 0.32263991\n",
      " 0.46338382 0.66898954 0.66950095 0.46524364 0.32631132] @epoch 146\n",
      "Epoch 147: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 126s 219ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 4, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 148/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0233\n",
      "validate accuracy:\n",
      " [0.52005634 0.31848079 0.3061873  0.51967144 0.51130962 0.32298657\n",
      " 0.32241601 0.59907502 0.84716403 0.84877706 0.80541086 0.32249859\n",
      " 0.463945   0.66926831 0.67089492 0.46692881 0.32588717] @epoch 147\n",
      "Epoch 148: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 126s 219ms/step - loss: 0.0233 - val_loss: 0.0243\n",
      "Counter: 5, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 149/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52025295 0.31763208 0.3083612  0.52082431 0.51188588 0.32332215\n",
      " 0.32113993 0.59965312 0.84730369 0.8490566  0.80627894 0.32405314\n",
      " 0.46366441 0.67038327 0.66950095 0.46552449 0.32546303] @epoch 148\n",
      "Epoch 149: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 126s 219ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 6, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 150/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52030425 0.31826863 0.30802676 0.52053612 0.51231813 0.32416108\n",
      " 0.32177797 0.6008094  0.84716403 0.84919637 0.80671299 0.32263991\n",
      " 0.46338382 0.66996515 0.67131305 0.46397978 0.32461473] @epoch 149\n",
      "Epoch 150: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 126s 219ms/step - loss: 0.0232 - val_loss: 0.0243\n",
      "Counter: 7, Global: 0.5211376622319221, MyBest: 0.5211376622319221\n",
      "\n",
      "Epoch 150: Updating Learning rate.. New value is 0.000000\n",
      "Epoch 151/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0232\n",
      "validate accuracy:\n",
      " [0.52045803 0.31699553 0.30819398 0.51995963 0.51130962 0.32466444\n",
      " 0.32220331 0.60182106 0.84730369 0.84947592 0.80613428 0.32362917\n",
      " 0.46464646 0.66982579 0.67159188 0.46552449 0.3240492 ] @epoch 150\n",
      "Epoch 151: val_acc did not improve from 0.521\n",
      "576/576 [==============================] - 126s 219ms/step - loss: 0.0232 - val_loss: 0.0243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net.compile(optimizer=optimizer, loss=weighted_mse_loss, metrics=[])\n",
    "#checkpointer = ModelCheckpoint(filepath=f'../cunet{nUNet}_{m}{n}_best.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True)\n",
    "history = net.fit(train_images,train_hms,validation_data=(eval_images,eval_hms2),epochs=151, batch_size=32,shuffle=True,verbose=1, callbacks=[evalcallback,updateLR])#,use_multiprocessing=True,workers=20)#[checkpointer,updateLR])\n",
    "#np.savez_compressed(\"../history\",history)\n",
    "net.save_weights(f'../cunet{nUNet}_{m}{n}_last_sigmoidAdamQuasiDef3sx12_restartsig.h5', overwrite=True)\n",
    "np.save('../history_sigmoidAdamQuasiDef3sx12_restartsig.npy',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnet.load_weights(\"../cunet2_6416_last_sigmoidAdamQuasiDef3sx12.h5\")\\noutput = net.predict(train_images)\\noutput = np.transpose(output,(0,3,1,2))\\nprint(accuracy(output,np.transpose(train_hms,(0,3,1,2))))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "net.load_weights(\"../cunet2_6416_last_sigmoidAdamQuasiDef3sx12.h5\")\n",
    "output = net.predict(train_images)\n",
    "output = np.transpose(output,(0,3,1,2))\n",
    "print(accuracy(output,np.transpose(train_hms,(0,3,1,2))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ho aumentato solo patience \n",
    "#Provato con dropout ma andato male"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
