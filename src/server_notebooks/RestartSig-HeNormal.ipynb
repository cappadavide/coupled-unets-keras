{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import setGPU\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 64) 9408        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 64)   256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 64)   4096        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   9216        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64, 64, 80)   0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 80)   320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   5120        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   4096        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   9216        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 80)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 80)   320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 80)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 64)   5120        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 64)   4096        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 16)   9216        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 80)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 80)   320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 80)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 64)   5120        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 64)     256         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 64)     4096        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 16)     9216        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 80)     0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 80)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     5120        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 64)     0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4, 4, 64)     256         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4, 4, 64)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 4, 4, 64)     4096        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 64)     256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 16)     9216        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 80)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4, 4, 80)     0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     5120        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 80)     0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 144)    0           conv2d_16[0][0]                  \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 144)    576         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 144)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     9216        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 64)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 80)   320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 16)     9216        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 80)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 8, 160)    0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 64)   5120        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 160)  0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 224)  0           conv2d_12[0][0]                  \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 224)  896         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 224)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 64)   14336       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 64)   256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 80)   320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 16)   9216        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 80)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 240)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 64)   5120        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 240)  0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 304)  0           conv2d_8[0][0]                   \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 32, 304)  1216        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 32, 304)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 64)   19456       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 64)   256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 80)   320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 16)   9216        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 320)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   5120        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 320)  0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 64, 64, 384)  0           conv2d_4[0][0]                   \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 384)  1536        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 384)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 64)   24576       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 64)   256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 64)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 16)   9216        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 64, 64, 400)  0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 64, 64, 464)  0           max_pooling2d[0][0]              \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 64, 64, 464)  1856        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 64, 64, 464)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 64)   29696       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 64, 64, 80)   0           conv2d_31[0][0]                  \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64, 64, 80)   320         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64, 64, 80)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 64, 64, 64)   5120        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 64, 64, 64)   256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 64, 64, 64)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 64, 64, 16)   9216        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 64, 64, 96)   0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 64, 64, 96)   384         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 64, 64, 96)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 64)   6144        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 32, 32, 80)   0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 32, 80)   320         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 80)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 64)   5120        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 64)   256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 64)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 16)   9216        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 32, 32, 96)   0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 96)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 64)   6144        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 80)   0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 80)   320         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16, 16, 80)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 64)   5120        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 64)   256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 64)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 16)   9216        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 96)   0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 96)   384         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 96)   0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 64)   6144        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 80)     0           max_pooling2d_7[0][0]            \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 80)     320         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 80)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 64)     5120        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 64)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 16)     9216        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 96)     0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 96)     384         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 96)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 64)     6144        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 64)     0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 4, 4, 80)     0           max_pooling2d_8[0][0]            \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 80)     320         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 80)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 64)     5120        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 64)     256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 64)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 96)     384         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 16)     9216        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 96)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 4, 4, 96)     0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 64)     6144        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 8, 8, 96)     0           concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 176)    0           conv2d_47[0][0]                  \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 8, 8, 176)    704         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 8, 8, 176)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 8, 64)     11264       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 64)     256         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 8, 8, 64)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 96)   384         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 8, 8, 16)     9216        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 96)   0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 192)    0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 64)   6144        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 192)  0           concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 16, 16, 272)  0           conv2d_43[0][0]                  \n",
      "                                                                 up_sampling2d_5[0][0]            \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16, 16, 272)  1088        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 16, 272)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 16, 16, 64)   17408       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 64)   256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 64)   0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 96)   384         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 16)   9216        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 96)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 16, 16, 288)  0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 64)   6144        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 288)  0           concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 32, 32, 368)  0           conv2d_39[0][0]                  \n",
      "                                                                 up_sampling2d_6[0][0]            \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 32, 368)  1472        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 32, 32, 368)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 32, 64)   23552       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 64)   256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 64)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 64, 64, 96)   384         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 16)   9216        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 64, 64, 96)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 32, 32, 384)  0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 64, 64)   6144        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 384)  0           concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 64, 64, 464)  0           conv2d_35[0][0]                  \n",
      "                                                                 up_sampling2d_7[0][0]            \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 64, 64, 464)  1856        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 64, 64, 464)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 64, 64, 64)   29696       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 64, 64, 64)   256         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 64, 64, 64)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 64, 64, 16)   9216        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 64, 64, 480)  0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 64, 64, 480)  1920        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 64, 64, 480)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 64, 64, 16)   7680        activation_62[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 524,864\n",
      "Trainable params: 511,616\n",
      "Non-trainable params: 13,248\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Definizione Modello\n",
    "def getUnit1(layerPrec, filters, kernel_size = (1, 1), activation='relu', kernel_initializer='he_normal'):\n",
    "  l = BatchNormalization()(layerPrec)\n",
    "  l = Activation('relu')(l)\n",
    "  l = Conv2D(filters, kernel_size, kernel_initializer=kernel_initializer, padding='same',use_bias=False)(l)\n",
    "  return l\n",
    "\n",
    "def getUnit2(layerPrec, filters, kernel_size = (3, 3), activation='relu', kernel_initializer='he_normal'):\n",
    "  l = BatchNormalization()(layerPrec)\n",
    "  l = Activation('relu')(l)\n",
    "  l = Conv2D(filters, kernel_size, kernel_initializer=kernel_initializer, padding='same',use_bias=False)(l)\n",
    "  return l\n",
    "\n",
    "def getDownBlock(layerPrec,m,n,indexBlock):\n",
    "  l = getUnit1(layerPrec,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][f\"down{indexBlock}\"].append(l)\n",
    "\n",
    "  concat = Concatenate()([layerPrec,l])\n",
    "  l = getUnit1(concat,m)\n",
    "  maxPooling = MaxPool2D(padding='same')(l)\n",
    "  return maxPooling, getUnit1(concat,m)\n",
    "\n",
    "def getUpBlock(layerPrec,skipConn,m,n,indexBlock,upLayers=[]):\n",
    "  l = getUnit1(layerPrec,m)\n",
    "  l = UpSampling2D()(layerPrec)\n",
    "  concat = Concatenate()([skipConn,l]+upLayers)\n",
    "  l = getUnit1(concat,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][f\"up{indexBlock}\"].append(l)\n",
    "  concat = Concatenate()([concat,l])\n",
    "  return concat\n",
    "\n",
    "def getUNet(input,m,n,indexUNet,nUNet, nBlocks):\n",
    "  layerPrec = input\n",
    "  listSkipConn = []\n",
    "\n",
    "  if indexUNet != 0:\n",
    "    layerPrec = nets[f\"unet{indexUNet-1}\"]\n",
    "    #layerPrec = Concatenate()([input,layerPrec]) #l'abbiamo fatto giù con l'if dopo l'up\n",
    "    layerPrec = getUnit1(layerPrec,m)\n",
    "\n",
    "  # down\n",
    "  for i in range(nBlocks):\n",
    "    if nets[\"layers\"][f\"down{i}\"]:\n",
    "        layerPrec = Concatenate()([layerPrec]+nets[\"layers\"][f\"down{i}\"])\n",
    "    layerPrec, skipConn = getDownBlock(layerPrec,m,n,i)\n",
    "    listSkipConn.append(skipConn)\n",
    "\n",
    "  # bottle neck\n",
    "  if nets[\"layers\"][f\"bn\"]:\n",
    "      layerPrec = Concatenate()([layerPrec]+nets[\"layers\"][f\"bn\"])\n",
    "\n",
    "  l = getUnit1(layerPrec,4*n)\n",
    "  l = getUnit2(l,n)\n",
    "  nets[\"layers\"][\"bn\"].append(l)\n",
    "  concat = Concatenate()([layerPrec,l])\n",
    "\n",
    "  # up\n",
    "  layerPrec = concat\n",
    "  for i in range(nBlocks):\n",
    "    layerPrec = getUpBlock(layerPrec,listSkipConn[-(i+1)],m,n,i,upLayers=nets[\"layers\"][f\"up{i}\"])\n",
    "\n",
    "\n",
    "  if indexUNet != nUNet - 1:\n",
    "    l = Concatenate()([input,layerPrec])\n",
    "  else:\n",
    "    l = getUnit1(layerPrec,16,activation=\"sigmoid\") #era linear\n",
    "\n",
    "  return l\n",
    "\n",
    "def trasformationInput(x, filters):\n",
    "  #x = BatchNormalization()(x)\n",
    "  #x = Activation('relu')(x)\n",
    "  #x = Conv2D(filters, kernel_size=(7,7), strides=(2,2), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x)\n",
    "  #maxPooling = MaxPool2D(padding='same')(x)\n",
    "  \n",
    "  #x = Conv2D(filters*2, kernel_size=(5,5), strides=(1,1), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x)\n",
    "  #x = BatchNormalization()(x)\n",
    "  #x = Activation('relu')(x)\n",
    "  x = Conv2D(filters, kernel_size=(7,7), strides=(1,1), kernel_initializer=\"he_normal\", padding='same',use_bias=False)(x) #era 3x3\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPool2D(padding='same')(x)\n",
    "\n",
    "  return x#maxPooling\n",
    "\n",
    "def getCUNet(shape,m,n,nUNet,nBlocks):\n",
    "  for i in range(nUNet):\n",
    "    nets[f\"unet{i}\"] = None\n",
    "\n",
    "  for j in range(nBlocks):\n",
    "    nets[\"layers\"][f\"down{j}\"] = []\n",
    "    nets[\"layers\"][f\"up{j}\"] = []\n",
    "    \n",
    "  input = Input(shape=shape)\n",
    "  \n",
    "  t_input = trasformationInput(input,m) # per le heatmap da 64x64\n",
    "\n",
    "  for i in range(nUNet):\n",
    "    nets[f\"unet{i}\"] = getUNet(t_input,m,n,i,nUNet,nBlocks)\n",
    "\n",
    "  output = nets[f\"unet{nUNet-1}\"]\n",
    "  return Model(inputs=input, outputs=output)\n",
    "\n",
    "nets = {}\n",
    "nets[\"layers\"] = {}\n",
    "nets[\"layers\"][\"bn\"] = []\n",
    "shape = (128,128,3)\n",
    "m = 64\n",
    "n = 16\n",
    "nUNet = 2\n",
    "nBlocks = 4\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "net = getCUNet(shape,m,n,nUNet,nBlocks)\n",
    "\n",
    "\"\"\"\n",
    "layers = net.layers\n",
    "#print(layers[57].input)\n",
    "for i in range(len(layers)):\n",
    "    if isinstance(layers[i], tf.python.keras.layers.convolutional.Conv2D):\n",
    "        if isinstance(layers[i-1].input,list):\n",
    "            in_chan = 0\n",
    "            for k in layers[i-1].input:\n",
    "                in_chan = in_chan + k.shape[-1]\n",
    "            print(in_chan)\n",
    "        else:\n",
    "            in_chan = layers[i-1].input.shape[-1]\n",
    "            \n",
    "        n1 = layers[i].kernel_size[0] * layers[i].kernel_size[1] * in_chan\n",
    "        stdv = 1/math.sqrt(n1)\n",
    "        layers[i].kernel_initializer = tf.keras.initializers.RandomUniform(minval=-stdv, maxval=stdv)\n",
    "\"\"\"\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_preds(heatmaps):\n",
    "    \"\"\"Get keypoint predictions from score maps.\n",
    "    Note:\n",
    "        batch_size: N\n",
    "        num_keypoints: K\n",
    "        heatmap height: H\n",
    "        heatmap width: W\n",
    "    Args:\n",
    "        heatmaps (np.ndarray[N, K, H, W]): model predicted heatmaps.\n",
    "    Returns:\n",
    "        tuple: A tuple containing aggregated results.\n",
    "        - preds (np.ndarray[N, K, 2]): Predicted keypoint location.  \n",
    "        - maxvals (np.ndarray[N, K, 1]): Scores (confidence) of the keypoints. non lo restituiamo\n",
    "    \"\"\"\n",
    "    #assert isinstance(heatmaps, np.ndarray), ('heatmaps should be numpy.ndarray')\n",
    "    #assert heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n",
    "\n",
    "    N, K, _, W = heatmaps.shape\n",
    "    heatmaps_reshaped = heatmaps.reshape((N, K, -1))\n",
    "    idx = np.argmax(heatmaps_reshaped, 2).reshape((N, K, 1))\n",
    "    maxvals = np.amax(heatmaps_reshaped, 2).reshape((N, K, 1))\n",
    "\n",
    "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n",
    "    preds[:, :, 0] = preds[:, :, 0] % W\n",
    "    preds[:, :, 1] = preds[:, :, 1] // W\n",
    "\n",
    "    preds = np.where(np.tile(maxvals, (1, 1, 2)) > 0.0, preds, -1)\n",
    "    return preds#, maxvals\n",
    "\n",
    "def calc_dists(preds, target, normalize, use_zero=False):\n",
    "    preds = preds.astype(np.float32)\n",
    "    target = target.astype(np.float32)\n",
    "    normalize = normalize.astype(np.float32)\n",
    "    dists = np.zeros((preds.shape[1], preds.shape[0]))\n",
    "    if use_zero:\n",
    "        boundary = 0\n",
    "    else:\n",
    "        boundary = 1\n",
    "    for n in range(preds.shape[0]):\n",
    "        for c in range(preds.shape[1]):\n",
    "            if target[n,c,0] > boundary and target[n, c, 1] > boundary:\n",
    "                dists[c, n] = np.linalg.norm((preds[n,c,:]- target[n,c,:])/normalize[n]) # axis ricavato da solo\n",
    "            else:\n",
    "                dists[c, n] = -1\n",
    "    return dists\n",
    "\n",
    "def dist_acc(dists, thr=0.5):\n",
    "\n",
    "  #Return percentage below threshold while ignoring values with a -1\n",
    "\n",
    "  if (dists!=-1).sum() > 0:\n",
    "\n",
    "    return ((dists<=thr) == (dists!=-1)).sum().astype(np.float32) / (dists!=-1).sum().astype(np.float32)\n",
    "\n",
    "  else:\n",
    "\n",
    "    return -1\n",
    "\n",
    "def accuracy(output, target, thr=0.5):\n",
    "  ''' Calculate accuracy according to PCK, but uses ground truth heatmap rather than x,y locations\n",
    "        First value to be returned is average accuracy across 'idxs', followed by individual accuracies\n",
    "    '''\n",
    "  #output = output.numpy()\n",
    "  #print(output.__class__)\n",
    "  #target = target.numpy()\n",
    "\n",
    "  idkp = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "  preds = get_max_preds(output)\n",
    "  gts = get_max_preds(target)\n",
    "  norm = np.ones(preds.shape[0])*output.shape[3]/10\n",
    "\n",
    "  dists = calc_dists(preds, gts, norm)\n",
    "\n",
    "  acc = np.zeros(len(idkp)+1)\n",
    "  avg_acc = 0\n",
    "  cnt = 0\n",
    "\n",
    "  for i in range(len(idkp)):\n",
    "    acc[i+1] = dist_acc(dists[idkp[i]])\n",
    "    if acc[i+1] >= 0: \n",
    "      avg_acc = avg_acc + acc[i+1]\n",
    "      cnt += 1\n",
    "            \n",
    "  if cnt != 0:  \n",
    "    acc[0] = avg_acc / cnt\n",
    "\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import cv2\n",
    "\n",
    "# =============================================================================\n",
    "# General image processing functions\n",
    "# =============================================================================\n",
    "\n",
    "def get_transform(center, scale, res, rot=0):\n",
    "    # Generate transformation matrix\n",
    "    h = 200 * scale\n",
    "    t = np.zeros((3, 3))\n",
    "    t[0, 0] = float(res[1]) / h\n",
    "    t[1, 1] = float(res[0]) / h\n",
    "    t[0, 2] = res[1] * (-float(center[0]) / h + .5)\n",
    "    t[1, 2] = res[0] * (-float(center[1]) / h + .5)\n",
    "    t[2, 2] = 1\n",
    "    if not rot == 0:\n",
    "        rot = -rot # To match direction of rotation from cropping\n",
    "        rot_mat = np.zeros((3,3))\n",
    "        rot_rad = rot * np.pi / 180\n",
    "        sn,cs = np.sin(rot_rad), np.cos(rot_rad)\n",
    "        rot_mat[0,:2] = [cs, -sn]\n",
    "        rot_mat[1,:2] = [sn, cs]\n",
    "        rot_mat[2,2] = 1\n",
    "        # Need to rotate around center\n",
    "        t_mat = np.eye(3)\n",
    "        t_mat[0,2] = -res[1]/2\n",
    "        t_mat[1,2] = -res[0]/2\n",
    "        t_inv = t_mat.copy()\n",
    "        t_inv[:2,2] *= -1\n",
    "        t = np.dot(t_inv,np.dot(rot_mat,np.dot(t_mat,t)))\n",
    "    return t\n",
    "\n",
    "def transform(pt, center, scale, res, invert=0, rot=0):\n",
    "    # Transform pixel location to different reference\n",
    "    t = get_transform(center, scale, res, rot=rot)\n",
    "    if invert:\n",
    "        t = np.linalg.inv(t)\n",
    "    new_pt = np.array([pt[0], pt[1], 1.]).T\n",
    "    new_pt = np.dot(t, new_pt)\n",
    "    return new_pt[:2].astype(int)\n",
    "\n",
    "def crop(img, center, scale, res, rot=0):\n",
    "    # Upper left point\n",
    "    ul = np.array(transform([0, 0], center, scale, res, invert=1))\n",
    "    # Bottom right point\n",
    "    br = np.array(transform(res, center, scale, res, invert=1))\n",
    "\n",
    "    new_shape = [br[1] - ul[1], br[0] - ul[0]]\n",
    "    if len(img.shape) > 2:\n",
    "        new_shape += [img.shape[2]]\n",
    "    new_img = np.zeros(new_shape)\n",
    "\n",
    "    # Range to fill new array\n",
    "    new_x = max(0, -ul[0]), min(br[0], len(img[0])) - ul[0]\n",
    "    new_y = max(0, -ul[1]), min(br[1], len(img)) - ul[1]\n",
    "    # Range to sample from original image\n",
    "    old_x = max(0, ul[0]), min(len(img[0]), br[0])\n",
    "    old_y = max(0, ul[1]), min(len(img), br[1])\n",
    "    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], old_x[0]:old_x[1]]\n",
    "\n",
    "    return cv2.resize(new_img, res)\n",
    "\n",
    "def inv_mat(mat):\n",
    "    ans = np.linalg.pinv(np.array(mat).tolist() + [[0,0,1]])\n",
    "    return ans[:2]\n",
    "\n",
    "def kpt_affine(kpt, mat):\n",
    "    kpt = np.array(kpt)\n",
    "    shape = kpt.shape\n",
    "    kpt = kpt.reshape(-1, 2)\n",
    "    return np.dot( np.concatenate((kpt, kpt[:, 0:1]*0+1), axis = 1), mat.T ).reshape(shape)\n",
    "\n",
    "\n",
    "def resize(im, res):\n",
    "    return np.array([cv2.resize(im[i],res) for i in range(im.shape[0])])\n",
    "\n",
    "def generateHeatmap(keypoints,output_res,num_parts):\n",
    "    #Init\n",
    "    sigma = output_res/64\n",
    "    size = 6*sigma+3\n",
    "    x = np.arange(0, size, 1, float) #crea un array composto da nove 1.0 vettore 1-D (9,)\n",
    "    y = x[:, np.newaxis] #Vettore 2-D (9,1)\n",
    "    x0, y0 = 3*sigma + 1, 3*sigma + 1 #vedi giù a tutto\n",
    "    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n",
    "    #Generation\n",
    "    hms = np.zeros(shape = (num_parts, output_res, output_res), dtype = np.float32) #crea vettore (16,64,64), cioè 16 heatmaps nere\n",
    "    for p in keypoints:\n",
    "        for idx, pt in enumerate(p): #ottiene id + [x,y] di ogni keypoint\n",
    "            if pt[0] > 0: \n",
    "                x, y = int(pt[0]), int(pt[1])\n",
    "                if x<0 or y<0 or x>=output_res or y>=output_res: #se succede questo, rimane heatmap idx-esima  tutta a 0\n",
    "                    continue\n",
    "                ul = int(x - 3*sigma - 1), int(y - 3*sigma - 1)\n",
    "                br = int(x + 3*sigma + 2), int(y + 3*sigma + 2)\n",
    "\n",
    "                c,d = max(0, -ul[0]), min(br[0], output_res) - ul[0]\n",
    "                a,b = max(0, -ul[1]), min(br[1], output_res) - ul[1]\n",
    "\n",
    "                cc,dd = max(0, ul[0]), min(br[0], output_res)\n",
    "                aa,bb = max(0, ul[1]), min(br[1], output_res)\n",
    "                hms[idx, aa:bb,cc:dd] = np.maximum(hms[idx, aa:bb,cc:dd], g[a:b,c:d])\n",
    "    return hms\n",
    "def getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    cropped = crop(img,c,s,inp_res)\n",
    "    orig_keypoints = []\n",
    "    for i in keypoints:\n",
    "        orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "    orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "    kptmp = np.copy(orig_keypoints)\n",
    "    for i in range(orig_keypoints.shape[1]):\n",
    "        if orig_keypoints[0,i,0] > 0:\n",
    "            orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "    keypoints = np.copy(orig_keypoints)\n",
    "    h, w = cropped.shape[0:2]\n",
    "    center = np.array((w/2,h/2))\n",
    "    scale = max(h,w)/200\n",
    "    aug_rot = (np.random.random()*2-1)*30\n",
    "    aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "    scale *= aug_scale\n",
    "\n",
    "    mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "    mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "    inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "    keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "    \n",
    "    #print('\\n')\n",
    "\n",
    "\n",
    "    #Flip 50% probability\n",
    "    if np.random.randint(2) == 0:\n",
    "        inp = inp[:, ::-1]\n",
    "        keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "        keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "        orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "        orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "    ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "        for i in range(np.shape(orig_keypoints)[1]):\n",
    "            if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                keypoints[0,i,0] = 0\n",
    "                keypoints[0,i,1] = 0\n",
    "                orig_keypoints[0,i,0] = 0\n",
    "                orig_keypoints[0,i,1] = 0\n",
    "\n",
    "    heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "    return inp,heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=2.5e-4, rho=0.99, momentum=0.0, epsilon=1e-08\n",
    ")\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=6.7e-3)\n",
    "\n",
    "\n",
    "def heatmapLoss(y_true,y_pred):\n",
    "    l = tf.math.square((y_pred - y_true))\n",
    "    l = tf.reduce_mean(l,axis=3)\n",
    "    l = tf.reduce_mean(l,axis=2)\n",
    "    l = tf.reduce_mean(l,axis=1)\n",
    "    return l\n",
    "\n",
    "def euclidean_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_true - y_pred)))\n",
    "\n",
    "def dice(y_true,y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice(y_true, y_pred)\n",
    "\n",
    "def weighted_mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    apply weights on heatmap mse loss to only pick valid keypoint heatmap\n",
    "    since y_true would be gt_heatmap with shape\n",
    "    (batch_size, heatmap_size[0], heatmap_size[1], num_keypoints)\n",
    "    we sum up the heatmap for each keypoints and check. Sum for invalid\n",
    "    keypoint would be 0, so we can get a keypoint weights tensor with shape\n",
    "    (batch_size, 1, 1, num_keypoints)\n",
    "    and multiply to loss\n",
    "    \"\"\"\n",
    "    heatmap_sum = K.sum(K.sum(y_true, axis=1, keepdims=True), axis=2, keepdims=True)\n",
    "\n",
    "    # keypoint_weights shape: (batch_size, 1, 1, num_keypoints), with\n",
    "    # valid_keypoint = 1.0, invalid_keypoint = 0.0\n",
    "    keypoint_weights = 1.0 - K.cast(K.equal(heatmap_sum, 0.0), 'float32')\n",
    "\n",
    "    return K.sqrt(K.mean(K.square((y_true - y_pred) * keypoint_weights)))\n",
    "\n",
    "def smooth_l1_loss(y_true, y_pred):\n",
    "    diff = K.abs(y_true - y_pred)\n",
    "    less_than_one = K.cast(K.less(diff, 1.0), 'float32')\n",
    "    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "parts = {'mpii':['rank', 'rkne', 'rhip',\n",
    "                 'lhip', 'lkne', 'lank',\n",
    "                 'pelv', 'thrx', 'neck', 'head',\n",
    "                 'rwri', 'relb', 'rsho',\n",
    "                 'lsho', 'lelb', 'lwri']}\n",
    "\n",
    "flipped_parts = {'mpii':[5, 4, 3, 2, 1, 0, 6, 7, 8, 9, 15, 14, 13, 12, 11, 10]}\n",
    "\n",
    "part_pairs = {'mpii':[[0, 5], [1, 4], [2, 3], [6], [7], [8], [9], [10, 15], [11, 14], [12, 13]]}\n",
    "\n",
    "pair_names = {'mpii':['ankle', 'knee', 'hip', 'pelvis', 'thorax', 'neck', 'head', 'wrist', 'elbow', 'shoulder']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imageio\n",
    "\n",
    "class Dataset(tf.keras.utils.Sequence):\n",
    "    #Costruttore\n",
    "    def __init__(self,json_path,batch_size,input_shape=(256,256,3),output_shape=(64,64,16),train=True,dataset_name=\"MPII\"):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.batch_size = batch_size\n",
    "        f = open(json_path)\n",
    "        self.dataset = json.load(f)\n",
    "        if dataset_name == \"MPII\":\n",
    "            self.dataset = self.dataset[\"dataset\"][\"MPII\"][\"people\"]\n",
    "        if train:\n",
    "            self.dataset = self.dataset[:-7221]\n",
    "        self.batch_images = np.zeros(shape=((self.batch_size,)+(self.input_shape)), dtype=np.float32)\n",
    "        self.batch_heatmaps = np.zeros(shape=((self.batch_size,)+(self.output_shape)), dtype=np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        for n, annotation in enumerate(self.dataset[i*self.batch_size:(i+1)*self.batch_size]):\n",
    "            #sample_index = i*self.batch_size + n\n",
    "            image, gt_heatmap = self.extract_imgs_hms(annotation)\n",
    "            gt_heatmap = np.transpose(gt_heatmap,(1,2,0))\n",
    "            self.batch_images[n,:,:,:] = image\n",
    "            self.batch_heatmaps[n, :, :, :] = gt_heatmap#gt_heatmap.reshape((self.output_shape))\n",
    "            \n",
    "        return self.batch_images, self.batch_heatmaps\n",
    "#getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    \n",
    "    def extract_imgs_hms(self,annotation):\n",
    "        inp_res = self.input_shape[0:2]\n",
    "        out_res = self.output_shape[0:2]\n",
    "        img = imageio.imread(f\"../images_mpii/{annotation['filepath']}\")\n",
    "        c = [annotation[\"objpos\"][\"x\"],annotation[\"objpos\"][\"y\"]]\n",
    "        s = annotation[\"scale\"]\n",
    "        keypoints = annotation[\"keypoints\"]\n",
    "        cropped = crop(img,c,s,inp_res)\n",
    "        \n",
    "        orig_keypoints = []\n",
    "        for i in keypoints:\n",
    "            orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "        orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "        kptmp = np.copy(orig_keypoints)\n",
    "        for i in range(orig_keypoints.shape[1]):\n",
    "            if orig_keypoints[0,i,0] > 0:\n",
    "                orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "        keypoints = np.copy(orig_keypoints)\n",
    "        h, w = cropped.shape[0:2]\n",
    "        center = np.array((w/2,h/2))\n",
    "        scale = max(h,w)/200\n",
    "        aug_rot = (np.random.random()*2-1)*30\n",
    "        aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "        scale *= aug_scale\n",
    "\n",
    "        mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "        mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "        inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "        keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "\n",
    "        #print('\\n')\n",
    "\n",
    "\n",
    "        #Flip 50% probability\n",
    "        if np.random.randint(2) == 0:\n",
    "            inp = inp[:, ::-1]\n",
    "            keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "            keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "            orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "            orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "        ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "            for i in range(np.shape(orig_keypoints)[1]):\n",
    "                if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                    keypoints[0,i,0] = 0\n",
    "                    keypoints[0,i,1] = 0\n",
    "                    orig_keypoints[0,i,0] = 0\n",
    "                    orig_keypoints[0,i,1] = 0\n",
    "\n",
    "        heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "        return inp,heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imageio\n",
    "\n",
    "class Dataset2(tf.keras.utils.Sequence):\n",
    "    #Costruttore\n",
    "    def __init__(self,json_path,batch_size,input_shape=(256,256,3),output_shape=(64,64,16),train=True,dataset_name=\"MPII\"):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.images = np.load(\"../imgs_mpii.npz\")['arr_0']\n",
    "        self.hms = np.load(\"../hms_mpii.npz\")['arr_0']\n",
    "        self.hms = self.hms.reshape((self.hms.shape[0],64,64,16))\n",
    "        if dataset_name == \"MPII\":\n",
    "            self.images = self.images[\"dataset\"][\"MPII\"][\"people\"]\n",
    "        if train:\n",
    "            self.images = self.images[:-8665]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.images[i*self.batch_size:(i+1)*self.batch_size], self.hms[i*self.batch_size:(i+1)*self.batch_size]\n",
    "#getImgHms(img,c,s,keypoints,inp_res=(256,256),out_res=(64,64)):\n",
    "    \n",
    "    def extract_imgs_hms(self,annotation):\n",
    "        inp_res = self.input_shape[0:2]\n",
    "        out_res = self.output_shape[0:2]\n",
    "        img = imageio.imread(f\"../images_mpii/{annotation['filepath']}\")\n",
    "        c = [annotation[\"objpos\"][\"x\"],annotation[\"objpos\"][\"y\"]]\n",
    "        s = annotation[\"scale\"]\n",
    "        keypoints = annotation[\"keypoints\"]\n",
    "        cropped = crop(img,c,s,inp_res)\n",
    "        \n",
    "        orig_keypoints = []\n",
    "        for i in keypoints:\n",
    "            orig_keypoints.append(np.array([i[\"x\"],i[\"y\"]]))\n",
    "        orig_keypoints = np.array(orig_keypoints).reshape((1,16,2))\n",
    "        kptmp = np.copy(orig_keypoints)\n",
    "        for i in range(orig_keypoints.shape[1]):\n",
    "            if orig_keypoints[0,i,0] > 0:\n",
    "                orig_keypoints[0,i,:2] = transform(orig_keypoints[0,i,:2], c, s, inp_res)\n",
    "        keypoints = np.copy(orig_keypoints)\n",
    "        h, w = cropped.shape[0:2]\n",
    "        center = np.array((w/2,h/2))\n",
    "        scale = max(h,w)/200\n",
    "        aug_rot = (np.random.random()*2-1)*30\n",
    "        aug_scale = np.random.random() * (1.25-0.75)+0.75\n",
    "        scale *= aug_scale\n",
    "\n",
    "        mat_mask = get_transform(center, scale, out_res, aug_rot)[:2]\n",
    "        mat = get_transform(center, scale, inp_res, aug_rot)[:2]\n",
    "        inp = cv2.warpAffine(cropped, mat, inp_res).astype(np.float32)/255\n",
    "\n",
    "        keypoints[:,:,0:2] = kpt_affine(keypoints[:,:,0:2], mat_mask) \n",
    "\n",
    "        #print('\\n')\n",
    "\n",
    "\n",
    "        #Flip 50% probability\n",
    "        if np.random.randint(2) == 0:\n",
    "            inp = inp[:, ::-1]\n",
    "            keypoints = keypoints[:, flipped_parts['mpii']]\n",
    "            keypoints[:, :, 0] = 64 - keypoints[:, :, 0]\n",
    "            orig_keypoints = orig_keypoints[:, flipped_parts['mpii']]\n",
    "            orig_keypoints[:, :, 0] = 256 - orig_keypoints[:, :, 0]\n",
    "\n",
    "        ## set keypoints to 0 when were not visible initially (so heatmap all 0s)\n",
    "            for i in range(np.shape(orig_keypoints)[1]):\n",
    "                if kptmp[0,i,0] == 0 and kptmp[0,i,1] == 0:\n",
    "                    keypoints[0,i,0] = 0\n",
    "                    keypoints[0,i,1] = 0\n",
    "                    orig_keypoints[0,i,0] = 0\n",
    "                    orig_keypoints[0,i,1] = 0\n",
    "\n",
    "        heatmaps = generateHeatmap(keypoints,out_res[0],16)\n",
    "        return inp,heatmaps\n",
    "####################################################################àà\n",
    "\n",
    "\n",
    "def get_normalize(input_shape):\n",
    "    \"\"\"\n",
    "    rescale keypoint distance normalize coefficient\n",
    "    based on input shape, used for PCK evaluation\n",
    "    NOTE: 6.4 is standard normalize coefficient under\n",
    "          input shape (256,256)\n",
    "    # Arguments\n",
    "        input_shape: input image shape as (height, width)\n",
    "    # Returns\n",
    "        scale: normalize coefficient\n",
    "    \"\"\"\n",
    "    #assert input_shape[0] == input_shape[1], 'only support square input shape.'\n",
    "\n",
    "    # use averaged scale factor for non square input shape\n",
    "    scale = float((input_shape[0] + input_shape[1]) / 2) / 256.0\n",
    "\n",
    "    return 6.4*scale\n",
    "\n",
    "global_best_acc = 0.0\n",
    "\n",
    "class EvalCallBack(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model_input_shape, fileName, imgs,hms):\n",
    "        self.normalize = get_normalize(model_input_shape)\n",
    "        self.model_input_shape = model_input_shape\n",
    "        self.best_acc = 0.0\n",
    "        self.fileName = fileName\n",
    "        self.eval_images = imgs#np.load(\"../imgs_val_mpii128.npz\")['arr_0']\n",
    "        self.eval_hms = hms#np.load(\"../hms_val_mpii128.npz\")['arr_0']\n",
    "        self.listAcc = []\n",
    "        #self.eval_hms = np.reshape(self.eval_hms,(self.eval_hms.shape[0],64,64,16))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global global_best_acc\n",
    "        output = self.model.predict(self.eval_images)\n",
    "        #output = output.reshape( (output.shape[0],)+(16,64,64) )\n",
    "        output = np.transpose(output,(0,3,1,2))\n",
    "        val_acc = accuracy(output,self.eval_hms)\n",
    "        self.listAcc.append(val_acc[0])\n",
    "        print('\\nvalidate accuracy:\\n', val_acc, '@epoch', epoch)\n",
    "        f = open(self.fileName, \"a\")\n",
    "        if val_acc[0] > self.best_acc:\n",
    "            # Save best accuray value and model checkpoint\n",
    "            #checkpoint_dir = os.path.join(self.log_dir, 'ep{epoch:03d}-loss{loss:.3f}-val_acc{val_acc:.3f}.h5'.format(epoch=(epoch+1), loss=logs.get('loss'), val_acc=val_acc))\n",
    "            #self.model.save(f\"../modelsave/ep{epoch}_acc{val_acc[0]}.h5\")\n",
    "            self.model.save_weights(f\"../cunet{nUNet}_{m}{n}_best_sigmoidAdamQuasiDef3sx12_restartsig_henormal.h5\")\n",
    "            print('Epoch {epoch:03d}: val_acc improved from {best_acc:.3f} to {val_acc:.3f}'.format(epoch=epoch+1, best_acc=self.best_acc, val_acc=val_acc[0]))# checkpoint_dir=checkpoint_dir))\n",
    "            message = 'Epoch {epoch:03d}: val_acc improved from {best_acc:.3f} to {val_acc:.3f}. Train loss is {loss:.3f}\\n'.format(epoch=epoch+1, best_acc=self.best_acc, val_acc=val_acc[0],loss=logs.get('loss'))\n",
    "            self.best_acc = val_acc[0]\n",
    "            global_best_acc = val_acc[0]\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            message = 'Epoch {epoch:03d}: val_acc did not improve from {best_acc:.3f}. Train loss is {loss:.3f}\\n'.format(epoch=epoch+1, best_acc=self.best_acc,loss=logs.get('loss'))\n",
    "            print('Epoch {epoch:03d}: val_acc did not improve from {best_acc:.3f}'.format(epoch=epoch+1, best_acc=self.best_acc))\n",
    "        f.write(message)\n",
    "        f.close()\n",
    "        if epoch == 150:\n",
    "            np.savez_compressed(f'../cunet{nUNet}_{m}{n}_best_sigmoidAdamQuasiDef3sx12_restartsig_henormal_accs', self.listAcc)\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "class CustomLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience,factor,fileName=None):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.fileName = fileName\n",
    "        self.factor = factor\n",
    "        self.best_acc = 0.0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        global global_best_acc\n",
    "        print(f\"Counter: {self.counter}, Global: {global_best_acc}, MyBest: {self.best_acc}\\n\")\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "    \n",
    "    def schedule(self,epoch,lr):\n",
    "        global global_best_acc\n",
    "        if self.counter == self.patience:\n",
    "            self.counter = 0 \n",
    "            print(\"Epoch %03d: Updating Learning rate.. New value is %f\" % (epoch, lr*self.factor))\n",
    "            message = \"Epoch %03d: Updating Learning rate.. New value is %f\" % (epoch, lr*self.factor)\n",
    "            f = open(self.fileName, \"a\")\n",
    "            f.write(message)\n",
    "            f.close()\n",
    "            return lr*self.factor\n",
    "        if self.best_acc == global_best_acc:\n",
    "            self.counter = self.counter + 1\n",
    "        elif self.best_acc < global_best_acc:\n",
    "            self.counter = 0\n",
    "            self.best_acc = global_best_acc\n",
    "        return lr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finito 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndataset = Dataset(\"datasets.json\",32)\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "with open('../train_imgssx12.pickle', 'rb') as handle:\n",
    "    train_images = pickle.load(handle)\n",
    "with open('../train_hmssx12.pickle', 'rb') as handle:\n",
    "    train_hms = pickle.load(handle)\n",
    "\n",
    "eval_images = np.load(\"../imgs_val_mpii128sx12.npz\")['arr_0']\n",
    "eval_hms = np.load(\"../hms_val_mpii128sx12.npz\")['arr_0']\n",
    "\n",
    "#train_images = np.load(\"../imgs_train_mpii128.npz\")['arr_0']\n",
    "#train_hms = np.load(\"../hms_train_mpii128.npz\")['arr_0']\n",
    "train_hms = np.transpose(train_hms,(0,2,3,1))#np.reshape(train_hms,(train_hms.shape[0],64,64,16))\n",
    "eval_hms2 = np.transpose(eval_hms,(0,2,3,1))\n",
    "print(\"Finito 1\")\n",
    "\"\"\"\n",
    "dataset = Dataset(\"datasets.json\",32)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finito 2\n"
     ]
    }
   ],
   "source": [
    "evalcallback = EvalCallBack((128,128),\"../sigmoidLayersAdamSchedulingQuasiDef3sx12_restartsig_henormal.txt\",eval_images,eval_hms)\n",
    "updateLR = CustomLearningRateScheduler(7,0.2,fileName=\"../sigmoidLayersAdamSchedulingQuasiDef3sx12_restartsig_henormal.txt\")\n",
    "print(\"Finito 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 0, Global: 0.0, MyBest: 0.0\n",
      "\n",
      "Epoch 1/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0994WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0134s vs `on_test_batch_end` time: 0.0367s). Check your callbacks.\n",
      "\n",
      "validate accuracy:\n",
      " [0.01092268 0.01506471 0.00802676 0.02031993 0.00388993 0.\n",
      " 0.00297746 0.0254372  0.00977927 0.00013976 0.01229745 0.00522894\n",
      " 0.00042088 0.0287108  0.01589072 0.00014043 0.02643857] @epoch 0\n",
      "Epoch 001: val_acc improved from 0.000 to 0.011\n",
      "576/576 [==============================] - 131s 227ms/step - loss: 0.0994 - val_loss: 0.0315\n",
      "Counter: 1, Global: 0.010922675459369202, MyBest: 0.0\n",
      "\n",
      "Epoch 2/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0419\n",
      "validate accuracy:\n",
      " [0.0182665  0.00891152 0.01237458 0.02435509 0.02305143 0.00520134\n",
      " 0.01424926 0.00621477 0.0219335  0.02459818 0.03790509 0.00056529\n",
      " 0.01220539 0.03191638 0.03652077 0.016992   0.01526933] @epoch 1\n",
      "Epoch 002: val_acc improved from 0.011 to 0.018\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0419 - val_loss: 0.0444\n",
      "Counter: 0, Global: 0.018266495651914738, MyBest: 0.010922675459369202\n",
      "\n",
      "Epoch 3/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0382\n",
      "validate accuracy:\n",
      " [0.03519374 0.02440059 0.02107023 0.02954316 0.05748451 0.02130873\n",
      " 0.02148022 0.06315941 0.04833752 0.05213138 0.05627894 0.01540418\n",
      " 0.01557239 0.03526133 0.05631447 0.02541778 0.01993496] @epoch 2\n",
      "Epoch 003: val_acc improved from 0.018 to 0.035\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0382 - val_loss: 0.0341\n",
      "Counter: 0, Global: 0.03519373800372705, MyBest: 0.018266495651914738\n",
      "\n",
      "Epoch 4/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0368\n",
      "validate accuracy:\n",
      " [0.06957939 0.03988967 0.03862876 0.10707594 0.10819767 0.02734899\n",
      " 0.02615908 0.12747507 0.08102822 0.09951083 0.0697338  0.02331826\n",
      " 0.04938272 0.09783972 0.10189573 0.06347423 0.05231161] @epoch 3\n",
      "Epoch 004: val_acc improved from 0.035 to 0.070\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0368 - val_loss: 0.0291\n",
      "Counter: 0, Global: 0.06957939406856894, MyBest: 0.03519373800372705\n",
      "\n",
      "Epoch 5/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0360\n",
      "validate accuracy:\n",
      " [0.0341681  0.01570125 0.02608696 0.07061537 0.06036594 0.01409396\n",
      " 0.01467461 0.04393699 0.05923442 0.04472397 0.03848379 0.01144715\n",
      " 0.04559484 0.03108014 0.01756342 0.03159669 0.02149017] @epoch 4\n",
      "Epoch 005: val_acc did not improve from 0.070\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0360 - val_loss: 0.1464\n",
      "Counter: 0, Global: 0.06957939406856894, MyBest: 0.06957939406856894\n",
      "\n",
      "Epoch 6/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0355\n",
      "validate accuracy:\n",
      " [0.14128971 0.06662423 0.08929766 0.20752271 0.20357297 0.06157718\n",
      " 0.05295619 0.24107531 0.2113719  0.21928722 0.15755208 0.06938948\n",
      " 0.10844557 0.17672473 0.17368275 0.12371858 0.09783684] @epoch 5\n",
      "Epoch 006: val_acc improved from 0.070 to 0.141\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0355 - val_loss: 0.0313\n",
      "Counter: 1, Global: 0.1412897128611803, MyBest: 0.06957939406856894\n",
      "\n",
      "Epoch 7/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0350\n",
      "validate accuracy:\n",
      " [0.13381052 0.06704859 0.09983277 0.22092521 0.16741103 0.08003356\n",
      " 0.03913229 0.24483307 0.19223246 0.19315164 0.14048032 0.07885811\n",
      " 0.11321549 0.15860628 0.15193756 0.10419885 0.08907112] @epoch 6\n",
      "Epoch 007: val_acc did not improve from 0.141\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0350 - val_loss: 0.0363\n",
      "Counter: 0, Global: 0.1412897128611803, MyBest: 0.1412897128611803\n",
      "\n",
      "Epoch 8/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0346\n",
      "validate accuracy:\n",
      " [0.16665626 0.09038829 0.11404682 0.22568093 0.23008212 0.08389262\n",
      " 0.09208848 0.29108253 0.25062868 0.2271139  0.22439235 0.10599209\n",
      " 0.1095679  0.20125435 0.18497351 0.13776155 0.09755408] @epoch 7\n",
      "Epoch 008: val_acc improved from 0.141 to 0.167\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0346 - val_loss: 0.0297\n",
      "Counter: 1, Global: 0.1666562631726265, MyBest: 0.1412897128611803\n",
      "\n",
      "Epoch 9/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0342\n",
      "validate accuracy:\n",
      " [0.17193225 0.08953957 0.12056856 0.21775472 0.21826826 0.10973154\n",
      " 0.08634624 0.28833646 0.24545963 0.2438854  0.19632523 0.13694178\n",
      " 0.16287878 0.20404181 0.17730694 0.14113186 0.11239927] @epoch 8\n",
      "Epoch 009: val_acc improved from 0.167 to 0.172\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0342 - val_loss: 0.0315\n",
      "Counter: 0, Global: 0.1719322521239519, MyBest: 0.1666562631726265\n",
      "\n",
      "Epoch 10/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0339\n",
      "validate accuracy:\n",
      " [0.1559299  0.09823892 0.09431438 0.19757891 0.1983864  0.08422819\n",
      " 0.08102935 0.24382137 0.26320201 0.24835779 0.18287037 0.09313171\n",
      " 0.12261504 0.16418119 0.16699192 0.14169358 0.11423724] @epoch 9\n",
      "Epoch 010: val_acc did not improve from 0.172\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0339 - val_loss: 0.0548\n",
      "Counter: 0, Global: 0.1719322521239519, MyBest: 0.1719322521239519\n",
      "\n",
      "Epoch 11/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0336\n",
      "validate accuracy:\n",
      " [0.17452777 0.12327605 0.09983277 0.20997262 0.19478461 0.10218121\n",
      " 0.10293492 0.28775835 0.2524448  0.24472398 0.2452257  0.13934426\n",
      " 0.14506173 0.19581881 0.17410092 0.16753265 0.10745087] @epoch 10\n",
      "Epoch 011: val_acc improved from 0.172 to 0.175\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0336 - val_loss: 0.0301\n",
      "Counter: 1, Global: 0.17452776618301868, MyBest: 0.1719322521239519\n",
      "\n",
      "Epoch 12/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0333\n",
      "validate accuracy:\n",
      " [0.1692095  0.0678973  0.10033445 0.2596916  0.20299669 0.10167785\n",
      " 0.1014462  0.25451654 0.32243642 0.23801537 0.24450232 0.10387225\n",
      " 0.12626262 0.20167248 0.15779203 0.1242803  0.09995759] @epoch 11\n",
      "Epoch 012: val_acc did not improve from 0.175\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0333 - val_loss: 0.0430\n",
      "Counter: 0, Global: 0.17452776618301868, MyBest: 0.17452776618301868\n",
      "\n",
      "Epoch 13/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0330\n",
      "validate accuracy:\n",
      " [0.18740561 0.10163378 0.12826087 0.20694624 0.23296355 0.11124161\n",
      " 0.09421523 0.27200463 0.33403185 0.30118799 0.26229745 0.14160542\n",
      " 0.12780583 0.2036237  0.20602174 0.15419182 0.12045808] @epoch 12\n",
      "Epoch 013: val_acc improved from 0.175 to 0.187\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0330 - val_loss: 0.0519\n",
      "Counter: 1, Global: 0.18740561185404658, MyBest: 0.17452776618301868\n",
      "\n",
      "Epoch 14/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0327\n",
      "validate accuracy:\n",
      " [0.21569519 0.13261193 0.14314382 0.25940338 0.26970178 0.13053691\n",
      " 0.1282433  0.28327793 0.37091365 0.35457721 0.29456019 0.1438666\n",
      " 0.15333894 0.23763067 0.19682185 0.18340121 0.16909374] @epoch 13\n",
      "Epoch 014: val_acc improved from 0.187 to 0.216\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0327 - val_loss: 0.0284\n",
      "Counter: 0, Global: 0.21569519490003586, MyBest: 0.18740561185404658\n",
      "\n",
      "Epoch 15/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0325\n",
      "validate accuracy:\n",
      " [0.22135555 0.12221515 0.15986621 0.27986741 0.2185564  0.1271812\n",
      " 0.11675882 0.28269982 0.39703828 0.33319357 0.33304399 0.1271905\n",
      " 0.18167789 0.28794426 0.2337608  0.19309086 0.14760356] @epoch 14\n",
      "Epoch 015: val_acc improved from 0.216 to 0.221\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0325 - val_loss: 0.0289\n",
      "Counter: 0, Global: 0.22135554626584053, MyBest: 0.21569519490003586\n",
      "\n",
      "Epoch 16/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0322\n",
      "validate accuracy:\n",
      " [0.25024411 0.13133885 0.16939799 0.27784982 0.27704942 0.14714766\n",
      " 0.15227562 0.3319844  0.41254541 0.4985325  0.38556135 0.16506501\n",
      " 0.17746913 0.26815331 0.23599108 0.20671254 0.16683161] @epoch 15\n",
      "Epoch 016: val_acc improved from 0.221 to 0.250\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0322 - val_loss: 0.0274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: 0, Global: 0.2502441070973873, MyBest: 0.22135554626584053\n",
      "\n",
      "Epoch 17/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0320\n",
      "validate accuracy:\n",
      " [0.25208204 0.14916189 0.12709031 0.28692895 0.23382798 0.14681208\n",
      " 0.12335177 0.3506287  0.41939089 0.47435361 0.30555555 0.17990389\n",
      " 0.19682941 0.33142856 0.29871759 0.2249684  0.18436307] @epoch 16\n",
      "Epoch 017: val_acc improved from 0.250 to 0.252\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0320 - val_loss: 0.0374\n",
      "Counter: 0, Global: 0.2520820405334234, MyBest: 0.2502441070973873\n",
      "\n",
      "Epoch 18/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0318\n",
      "validate accuracy:\n",
      " [0.27812093 0.17080416 0.14648829 0.25666523 0.28093934 0.14077181\n",
      " 0.1750319  0.37303078 0.57823414 0.58350801 0.36675346 0.15163934\n",
      " 0.21001683 0.2949129  0.31865069 0.23339419 0.16909374] @epoch 17\n",
      "Epoch 018: val_acc improved from 0.252 to 0.278\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0318 - val_loss: 0.0281\n",
      "Counter: 0, Global: 0.27812092658132315, MyBest: 0.2520820405334234\n",
      "\n",
      "Epoch 19/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0316\n",
      "validate accuracy:\n",
      " [0.26547174 0.12285168 0.13361204 0.26358265 0.28021899 0.13909397\n",
      " 0.12611654 0.32056656 0.50810283 0.54367578 0.43171296 0.18329565\n",
      " 0.2055275  0.31567943 0.32143852 0.18312036 0.16895236] @epoch 18\n",
      "Epoch 019: val_acc did not improve from 0.278\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0316 - val_loss: 0.0486\n",
      "Counter: 0, Global: 0.27812092658132315, MyBest: 0.27812092658132315\n",
      "\n",
      "Epoch 20/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0313\n",
      "validate accuracy:\n",
      " [0.27480727 0.1166985  0.1645485  0.29889032 0.24708255 0.1602349\n",
      " 0.13845173 0.35901141 0.50125736 0.59203357 0.42578125 0.19177501\n",
      " 0.2361111  0.32724738 0.28840256 0.18255863 0.16683161] @epoch 19\n",
      "Epoch 020: val_acc did not improve from 0.278\n",
      "576/576 [==============================] - 125s 216ms/step - loss: 0.0313 - val_loss: 0.0726\n",
      "Counter: 1, Global: 0.27812092658132315, MyBest: 0.27812092658132315\n",
      "\n",
      "Epoch 21/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0312\n",
      "validate accuracy:\n",
      " [0.29452972 0.14237216 0.12892976 0.27050006 0.29088026 0.13087249\n",
      " 0.13717566 0.34701547 0.61092484 0.63116699 0.4807581  0.20251554\n",
      " 0.20089787 0.34327525 0.3649289  0.24490942 0.18535274] @epoch 20\n",
      "Epoch 021: val_acc improved from 0.278 to 0.295\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0312 - val_loss: 0.0329\n",
      "Counter: 2, Global: 0.2945297211408615, MyBest: 0.27812092658132315\n",
      "\n",
      "Epoch 22/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0308\n",
      "validate accuracy:\n",
      " [0.28715944 0.148101   0.13996656 0.32569534 0.23498055 0.15469798\n",
      " 0.17375585 0.32851568 0.62461579 0.61425579 0.40755209 0.15912946\n",
      " 0.19149831 0.36348432 0.33300808 0.21008286 0.18521136] @epoch 21\n",
      "Epoch 022: val_acc did not improve from 0.295\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0308 - val_loss: 0.0281\n",
      "Counter: 0, Global: 0.2945297211408615, MyBest: 0.2945297211408615\n",
      "\n",
      "Epoch 23/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0307\n",
      "validate accuracy:\n",
      " [0.26036956 0.13155103 0.11153846 0.19325551 0.27589685 0.13137583\n",
      " 0.1378137  0.32822663 0.55895501 0.51558352 0.41001156 0.19022046\n",
      " 0.16442199 0.25142857 0.36395317 0.2273557  0.1743249 ] @epoch 22\n",
      "Epoch 023: val_acc did not improve from 0.295\n",
      "576/576 [==============================] - 125s 216ms/step - loss: 0.0307 - val_loss: 0.0934\n",
      "Counter: 1, Global: 0.2945297211408615, MyBest: 0.2945297211408615\n",
      "\n",
      "Epoch 24/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0304\n",
      "validate accuracy:\n",
      " [0.25483753 0.12815617 0.13595317 0.28981122 0.2251837  0.13825503\n",
      " 0.1169715  0.25827432 0.55001396 0.5296995  0.3582176  0.15757491\n",
      " 0.13678451 0.35317072 0.35963202 0.21429574 0.12540647] @epoch 23\n",
      "Epoch 024: val_acc did not improve from 0.295\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0304 - val_loss: 0.1321\n",
      "Counter: 2, Global: 0.2945297211408615, MyBest: 0.2945297211408615\n",
      "\n",
      "Epoch 25/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0303\n",
      "validate accuracy:\n",
      " [0.30197191 0.13155103 0.13946488 0.24744199 0.30888921 0.15637584\n",
      " 0.16397278 0.40916318 0.62601286 0.63270438 0.46440971 0.18908988\n",
      " 0.25561166 0.31400695 0.39336494 0.23746665 0.1620246 ] @epoch 24\n",
      "Epoch 025: val_acc improved from 0.295 to 0.302\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0303 - val_loss: 0.0319\n",
      "Counter: 3, Global: 0.3019719086587429, MyBest: 0.2945297211408615\n",
      "\n",
      "Epoch 26/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0301\n",
      "validate accuracy:\n",
      " [0.3161622  0.15892214 0.1535117  0.33722439 0.32228786 0.16191275\n",
      " 0.16652489 0.41653419 0.61413801 0.62278128 0.50969326 0.19799322\n",
      " 0.23063973 0.42202091 0.35698354 0.20264007 0.18478721] @epoch 25\n",
      "Epoch 026: val_acc improved from 0.302 to 0.316\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0301 - val_loss: 0.0279\n",
      "Counter: 0, Global: 0.316162196919322, MyBest: 0.3019719086587429\n",
      "\n",
      "Epoch 27/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0299\n",
      "validate accuracy:\n",
      " [0.26797482 0.14767663 0.11755853 0.25378296 0.264083   0.13791946\n",
      " 0.14844747 0.36117935 0.55909473 0.54339623 0.40436921 0.12775579\n",
      " 0.20566779 0.31874564 0.35503206 0.16149417 0.18139404] @epoch 26\n",
      "Epoch 027: val_acc did not improve from 0.316\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0299 - val_loss: 0.1094\n",
      "Counter: 0, Global: 0.316162196919322, MyBest: 0.316162196919322\n",
      "\n",
      "Epoch 28/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0296\n",
      "validate accuracy:\n",
      " [0.33077143 0.17716953 0.1687291  0.34140366 0.30730441 0.15587248\n",
      " 0.19268396 0.40078047 0.67644596 0.64793849 0.51649308 0.20604861\n",
      " 0.23526937 0.33574912 0.4318372  0.27664652 0.22197087] @epoch 27\n",
      "Epoch 028: val_acc improved from 0.316 to 0.331\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0296 - val_loss: 0.0292\n",
      "Counter: 1, Global: 0.33077142760157585, MyBest: 0.316162196919322\n",
      "\n",
      "Epoch 29/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0295\n",
      "validate accuracy:\n",
      " [0.32318223 0.18523234 0.19347826 0.28981122 0.3356865  0.14261745\n",
      " 0.19544874 0.43691283 0.67630625 0.58448637 0.44632524 0.16873939\n",
      " 0.27034232 0.32780486 0.43127963 0.27875298 0.20769122] @epoch 28\n",
      "Epoch 029: val_acc did not improve from 0.331\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0295 - val_loss: 0.0340\n",
      "Counter: 0, Global: 0.33077142760157585, MyBest: 0.33077142760157585\n",
      "\n",
      "Epoch 30/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0293\n",
      "validate accuracy:\n",
      " [0.3430213  0.19435604 0.19397993 0.30782533 0.32992363 0.18489933\n",
      " 0.1941727  0.42260441 0.66429168 0.71153039 0.54369211 0.16704352\n",
      " 0.27258697 0.45770034 0.37357122 0.28198287 0.1881804 ] @epoch 29\n",
      "Epoch 030: val_acc improved from 0.331 to 0.343\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0293 - val_loss: 0.0296\n",
      "Counter: 1, Global: 0.34302130434662104, MyBest: 0.33077142760157585\n",
      "\n",
      "Epoch 31/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0291\n",
      "validate accuracy:\n",
      " [0.35321384 0.21557395 0.18043478 0.36359707 0.32329637 0.20318791\n",
      " 0.21288814 0.41031942 0.67532831 0.71614254 0.58912039 0.24024872\n",
      " 0.24214366 0.46508712 0.38179538 0.24252212 0.18973562] @epoch 30\n",
      "Epoch 031: val_acc improved from 0.343 to 0.353\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0291 - val_loss: 0.0291\n",
      "Counter: 0, Global: 0.3532138429582119, MyBest: 0.34302130434662104\n",
      "\n",
      "Epoch 32/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0290\n",
      "validate accuracy:\n",
      " [0.35786869 0.21833228 0.20936455 0.36460584 0.32344043 0.16728188\n",
      " 0.14440663 0.45338923 0.69935739 0.71460515 0.60228586 0.16845675\n",
      " 0.27427047 0.45797908 0.46557012 0.2783317  0.18422168] @epoch 31\n",
      "Epoch 032: val_acc improved from 0.353 to 0.358\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0290 - val_loss: 0.0305\n",
      "Counter: 0, Global: 0.3578686909750104, MyBest: 0.3532138429582119\n",
      "\n",
      "Epoch 33/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0288\n",
      "validate accuracy:\n",
      " [0.33310327 0.20199448 0.2013378  0.3497622  0.25255728 0.16392617\n",
      " 0.16992769 0.33314064 0.66652697 0.68022364 0.57118058 0.2177784\n",
      " 0.21689114 0.45226482 0.44103709 0.27636567 0.13473773] @epoch 32\n",
      "Epoch 033: val_acc did not improve from 0.358\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0288 - val_loss: 0.0730\n",
      "Counter: 0, Global: 0.3578686909750104, MyBest: 0.3578686909750104\n",
      "\n",
      "Epoch 34/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0287\n",
      "validate accuracy:\n",
      " [0.36397999 0.20157012 0.21488294 0.27064419 0.36637372 0.16426174\n",
      " 0.2235219  0.45931494 0.70773959 0.72858143 0.60532409 0.21961561\n",
      " 0.2721661  0.44682926 0.45511571 0.2755231  0.21221547] @epoch 33\n",
      "Epoch 034: val_acc improved from 0.358 to 0.364\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0287 - val_loss: 0.0274\n",
      "Counter: 1, Global: 0.36397999431937933, MyBest: 0.3578686909750104\n",
      "\n",
      "Epoch 35/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0286\n",
      "validate accuracy:\n",
      " [0.3223297  0.19456822 0.15886287 0.36691165 0.31306729 0.16040269\n",
      " 0.17715865 0.39687815 0.6651299  0.63326347 0.56047451 0.15149802\n",
      " 0.23232323 0.41212544 0.37719542 0.18747367 0.16994204] @epoch 34\n",
      "Epoch 035: val_acc did not improve from 0.364\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0286 - val_loss: 0.0308\n",
      "Counter: 0, Global: 0.36397999431937933, MyBest: 0.36397999431937933\n",
      "\n",
      "Epoch 36/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0285\n",
      "validate accuracy:\n",
      " [0.28238733 0.1258222  0.16705686 0.29687274 0.29361764 0.12298658\n",
      " 0.13717566 0.2484463  0.57865328 0.60209644 0.50245947 0.16407575\n",
      " 0.16722783 0.37742162 0.37928632 0.2311473  0.12385126] @epoch 35\n",
      "Epoch 036: val_acc did not improve from 0.364\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0285 - val_loss: 0.1697\n",
      "Counter: 1, Global: 0.36397999431937933, MyBest: 0.36397999431937933\n",
      "\n",
      "Epoch 37/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0284\n",
      "validate accuracy:\n",
      " [0.35686838 0.2312752  0.1680602  0.32569534 0.27978677 0.17567115\n",
      " 0.19863887 0.46379533 0.63313776 0.7274633  0.6171875  0.19756925\n",
      " 0.27693602 0.48390242 0.48466685 0.22244067 0.22366747] @epoch 36\n",
      "Epoch 037: val_acc did not improve from 0.364\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0284 - val_loss: 0.0290\n",
      "Counter: 2, Global: 0.36397999431937933, MyBest: 0.36397999431937933\n",
      "\n",
      "Epoch 38/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0284\n",
      "validate accuracy:\n",
      " [0.35714149 0.21472523 0.21421404 0.32973051 0.3513903  0.20453019\n",
      " 0.22586134 0.42000288 0.70033526 0.67141861 0.54976851 0.22611645\n",
      " 0.27427047 0.40264809 0.45344299 0.28338715 0.19242188] @epoch 37\n",
      "Epoch 038: val_acc did not improve from 0.364\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0284 - val_loss: 0.0304\n",
      "Counter: 3, Global: 0.36397999431937933, MyBest: 0.36397999431937933\n",
      "\n",
      "Epoch 39/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0283\n",
      "validate accuracy:\n",
      " [0.31977037 0.15276894 0.20016722 0.22294278 0.26667628 0.14177853\n",
      " 0.1482348  0.37808931 0.63593185 0.66722572 0.56756365 0.23798756\n",
      " 0.26599327 0.31331012 0.4333705  0.28437018 0.19991517] @epoch 38\n",
      "Epoch 039: val_acc did not improve from 0.364\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0283 - val_loss: 0.1355\n",
      "Counter: 4, Global: 0.36397999431937933, MyBest: 0.36397999431937933\n",
      "\n",
      "Epoch 40/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0282\n",
      "validate accuracy:\n",
      " [0.3696154  0.25185657 0.19598663 0.3827641  0.25904047 0.21140939\n",
      " 0.21437687 0.4850412  0.73651856 0.64416492 0.59230322 0.18513285\n",
      " 0.27342874 0.48250872 0.49567884 0.26639518 0.23724021] @epoch 39\n",
      "Epoch 040: val_acc improved from 0.364 to 0.370\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0282 - val_loss: 0.0359\n",
      "Counter: 5, Global: 0.3696154039353132, MyBest: 0.36397999431937933\n",
      "\n",
      "Epoch 41/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0281\n",
      "validate accuracy:\n",
      " [0.38904727 0.24167196 0.18160535 0.3550944  0.39432359 0.1580537\n",
      " 0.25074437 0.43792456 0.75160658 0.7505241  0.65379053 0.24491239\n",
      " 0.30780023 0.47331011 0.5036242  0.32635865 0.19341156] @epoch 40\n",
      "Epoch 041: val_acc improved from 0.370 to 0.389\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0281 - val_loss: 0.0284\n",
      "Counter: 0, Global: 0.3890472687780857, MyBest: 0.3696154039353132\n",
      "\n",
      "Epoch 42/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0281\n",
      "validate accuracy:\n",
      " [0.39532159 0.25206876 0.1909699  0.38348466 0.39086586 0.18657719\n",
      " 0.24415143 0.49067783 0.75523889 0.75653392 0.66218174 0.20675524\n",
      " 0.25280583 0.5144251  0.50557572 0.2972897  0.23554362] @epoch 41\n",
      "Epoch 042: val_acc improved from 0.389 to 0.395\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0281 - val_loss: 0.0269\n",
      "Counter: 0, Global: 0.39532158616930246, MyBest: 0.3890472687780857\n",
      "\n",
      "Epoch 43/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0280\n",
      "validate accuracy:\n",
      " [0.39896418 0.26479951 0.21956521 0.35120335 0.36522114 0.21845637\n",
      " 0.24904296 0.49096692 0.7588712  0.74926627 0.60011572 0.22498587\n",
      " 0.30373177 0.50494772 0.50822413 0.33113328 0.24289551] @epoch 42\n",
      "Epoch 043: val_acc improved from 0.395 to 0.399\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0280 - val_loss: 0.0355\n",
      "Counter: 0, Global: 0.3989641824737191, MyBest: 0.39532158616930246\n",
      "\n",
      "Epoch 44/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0279\n",
      "validate accuracy:\n",
      " [0.35689065 0.24400595 0.15133779 0.35797665 0.36133122 0.2033557\n",
      " 0.20927265 0.45338923 0.69195306 0.6974144  0.60561341 0.14457321\n",
      " 0.19023569 0.4296864  0.46027321 0.29082993 0.21900184] @epoch 43\n",
      "Epoch 044: val_acc did not improve from 0.399\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0279 - val_loss: 0.1070\n",
      "Counter: 0, Global: 0.3989641824737191, MyBest: 0.3989641824737191\n",
      "\n",
      "Epoch 45/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0279\n",
      "validate accuracy:\n",
      " [0.39413191 0.25143221 0.22424749 0.34241244 0.38467079 0.2159396\n",
      " 0.25776264 0.48157248 0.74601847 0.75513625 0.64858216 0.22470322\n",
      " 0.31509539 0.51219511 0.45288542 0.32874596 0.16471088] @epoch 44\n",
      "Epoch 045: val_acc did not improve from 0.399\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0279 - val_loss: 0.0641\n",
      "Counter: 1, Global: 0.3989641824737191, MyBest: 0.3989641824737191\n",
      "\n",
      "Epoch 46/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0278\n",
      "validate accuracy:\n",
      " [0.40591381 0.24167196 0.18779264 0.39328432 0.39576429 0.21895973\n",
      " 0.25542322 0.49645904 0.74993014 0.76422083 0.66030091 0.2741662\n",
      " 0.31579685 0.52599305 0.50822413 0.30374947 0.20288421] @epoch 45\n",
      "Epoch 046: val_acc improved from 0.399 to 0.406\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0278 - val_loss: 0.0376\n",
      "Counter: 2, Global: 0.40591381303966045, MyBest: 0.3989641824737191\n",
      "\n",
      "Epoch 47/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0277\n",
      "validate accuracy:\n",
      " [0.39629687 0.25100785 0.22943144 0.39025795 0.39432359 0.2159396\n",
      " 0.25669926 0.50426364 0.7453199  0.74633121 0.66608799 0.21933296\n",
      " 0.24943884 0.45881534 0.52216339 0.26611432 0.22522268] @epoch 46\n",
      "Epoch 047: val_acc did not improve from 0.406\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0277 - val_loss: 0.0297\n",
      "Counter: 0, Global: 0.40591381303966045, MyBest: 0.40591381303966045\n",
      "\n",
      "Epoch 48/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0277\n",
      "validate accuracy:\n",
      " [0.41441894 0.28983662 0.25033444 0.40063411 0.36666188 0.2169463\n",
      " 0.2790302  0.51279086 0.77340037 0.73081762 0.60778356 0.281091\n",
      " 0.32575756 0.53839719 0.52230275 0.34419322 0.1907253 ] @epoch 47\n",
      "Epoch 048: val_acc improved from 0.406 to 0.414\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0277 - val_loss: 0.0281\n",
      "Counter: 1, Global: 0.41441893577575684, MyBest: 0.40591381303966045\n",
      "\n",
      "Epoch 49/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0277\n",
      "validate accuracy:\n",
      " [0.37043709 0.23212391 0.16471572 0.37930536 0.35844979 0.21241611\n",
      " 0.1767333  0.40930772 0.73093045 0.74758911 0.65335649 0.26158845\n",
      " 0.21212122 0.51470381 0.44201282 0.27116978 0.1604694 ] @epoch 48\n",
      "Epoch 049: val_acc did not improve from 0.414\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0277 - val_loss: 0.0743\n",
      "Counter: 0, Global: 0.41441893577575684, MyBest: 0.41441893577575684\n",
      "\n",
      "Epoch 50/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0276\n",
      "validate accuracy:\n",
      " [0.39546431 0.23318481 0.20685619 0.38535812 0.38942516 0.19966443\n",
      " 0.25627393 0.49443561 0.73595977 0.76226413 0.6791088  0.26554552\n",
      " 0.21296297 0.52752614 0.481879   0.24645415 0.25053018] @epoch 49\n",
      "Epoch 050: val_acc did not improve from 0.414\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0276 - val_loss: 0.0660\n",
      "Counter: 1, Global: 0.41441893577575684, MyBest: 0.41441893577575684\n",
      "\n",
      "Epoch 51/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0276\n",
      "validate accuracy:\n",
      " [0.40028094 0.26204115 0.23528428 0.34385359 0.36003458 0.21862416\n",
      " 0.27307528 0.50339645 0.67700475 0.75849056 0.67838544 0.27303562\n",
      " 0.28549382 0.52891988 0.5193755  0.29279596 0.19468401] @epoch 50\n",
      "Epoch 051: val_acc did not improve from 0.414\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0276 - val_loss: 0.0426\n",
      "Counter: 2, Global: 0.41441893577575684, MyBest: 0.41441893577575684\n",
      "\n",
      "Epoch 52/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0275\n",
      "validate accuracy:\n",
      " [0.31200047 0.13749205 0.22240803 0.3251189  0.34303415 0.20771812\n",
      " 0.1499362  0.39283133 0.62671137 0.71893781 0.31741899 0.14641041\n",
      " 0.0805275  0.41449478 0.48801225 0.17876703 0.2421886 ] @epoch 51\n",
      "Epoch 052: val_acc did not improve from 0.414\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0275 - val_loss: 0.0519\n",
      "Counter: 3, Global: 0.41441893577575684, MyBest: 0.41441893577575684\n",
      "\n",
      "Epoch 53/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0275\n",
      "validate accuracy:\n",
      " [0.42270252 0.28325906 0.24715719 0.34443003 0.41002738 0.22667785\n",
      " 0.2860485  0.51452523 0.78038555 0.78658283 0.69777197 0.27148107\n",
      " 0.2843715  0.54815328 0.45678839 0.35978094 0.26579952] @epoch 52\n",
      "Epoch 053: val_acc improved from 0.414 to 0.423\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0275 - val_loss: 0.0301\n",
      "Counter: 4, Global: 0.4227025182917714, MyBest: 0.41441893577575684\n",
      "\n",
      "Epoch 54/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0274\n",
      "validate accuracy:\n",
      " [0.35111097 0.23233609 0.20936455 0.33160397 0.33727127 0.20536913\n",
      " 0.21203743 0.43879172 0.63285834 0.66708595 0.60083914 0.21905032\n",
      " 0.27539283 0.4420906  0.38974074 0.28015727 0.14378624] @epoch 53\n",
      "Epoch 054: val_acc did not improve from 0.423\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0274 - val_loss: 0.2062\n",
      "Counter: 0, Global: 0.4227025182917714, MyBest: 0.4227025182917714\n",
      "\n",
      "Epoch 55/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0274\n",
      "validate accuracy:\n",
      " [0.36161306 0.23148738 0.20050167 0.35163569 0.23555684 0.20721476\n",
      " 0.25265846 0.36233559 0.6865046  0.6954577  0.62644678 0.16266252\n",
      " 0.30765992 0.46703833 0.46459436 0.3048729  0.22918139] @epoch 54\n",
      "Epoch 055: val_acc did not improve from 0.423\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0274 - val_loss: 0.1068\n",
      "Counter: 1, Global: 0.4227025182917714, MyBest: 0.4227025182917714\n",
      "\n",
      "Epoch 56/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0273\n",
      "validate accuracy:\n",
      " [0.42824031 0.27328664 0.25384617 0.37022626 0.40858665 0.23137584\n",
      " 0.28094429 0.51958376 0.77284157 0.77735847 0.67447919 0.27868852\n",
      " 0.33964646 0.52919859 0.53359354 0.33913776 0.26905131] @epoch 55\n",
      "Epoch 056: val_acc improved from 0.423 to 0.428\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0273 - val_loss: 0.0406\n",
      "Counter: 2, Global: 0.4282403141260147, MyBest: 0.4227025182917714\n",
      "\n",
      "Epoch 57/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0272\n",
      "validate accuracy:\n",
      " [0.42592665 0.2516444  0.24347825 0.39587837 0.40469673 0.23976511\n",
      " 0.27711612 0.52334154 0.77116513 0.7811321  0.69458914 0.27741662\n",
      " 0.34343433 0.53825784 0.50961804 0.35065299 0.21263961] @epoch 56\n",
      "Epoch 057: val_acc did not improve from 0.428\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0272 - val_loss: 0.0382\n",
      "Counter: 0, Global: 0.4282403141260147, MyBest: 0.4282403141260147\n",
      "\n",
      "Epoch 58/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0272\n",
      "validate accuracy:\n",
      " [0.41321415 0.2403989  0.25284281 0.32699236 0.32344043 0.24379195\n",
      " 0.24925564 0.44702992 0.75481981 0.74926627 0.70558447 0.27119842\n",
      " 0.34960717 0.54620206 0.52606636 0.35149556 0.27343419] @epoch 57\n",
      "Epoch 058: val_acc did not improve from 0.428\n",
      "576/576 [==============================] - 125s 216ms/step - loss: 0.0272 - val_loss: 0.0621\n",
      "Counter: 1, Global: 0.4282403141260147, MyBest: 0.4282403141260147\n",
      "\n",
      "Epoch 59/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0271\n",
      "validate accuracy:\n",
      " [0.43742071 0.28453216 0.2617057  0.33679205 0.42573115 0.24815436\n",
      " 0.29817098 0.47954908 0.8100028  0.8134172  0.73220485 0.23841153\n",
      " 0.32856342 0.57114983 0.53861165 0.369611   0.26212355] @epoch 58\n",
      "Epoch 059: val_acc improved from 0.428 to 0.437\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0271 - val_loss: 0.0270\n",
      "Counter: 2, Global: 0.43742070719599724, MyBest: 0.4282403141260147\n",
      "\n",
      "Epoch 60/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0271\n",
      "validate accuracy:\n",
      " [0.40367642 0.26628473 0.25668895 0.35134745 0.32113528 0.19161074\n",
      " 0.2677584  0.4601821  0.74839342 0.74507338 0.68778938 0.18781798\n",
      " 0.32070708 0.49477351 0.52955115 0.35556805 0.2741411 ] @epoch 59\n",
      "Epoch 060: val_acc did not improve from 0.437\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0271 - val_loss: 0.0761\n",
      "Counter: 0, Global: 0.43742070719599724, MyBest: 0.43742070719599724\n",
      "\n",
      "Epoch 61/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0270\n",
      "validate accuracy:\n",
      " [0.4359931  0.2951411  0.27240804 0.42225105 0.42947701 0.25469798\n",
      " 0.28328371 0.54473191 0.74993014 0.77260655 0.67100692 0.22979084\n",
      " 0.35465768 0.57533103 0.56161135 0.30094087 0.25802347] @epoch 60\n",
      "Epoch 061: val_acc did not improve from 0.437\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0270 - val_loss: 0.0293\n",
      "Counter: 1, Global: 0.43742070719599724, MyBest: 0.43742070719599724\n",
      "\n",
      "Epoch 62/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0269\n",
      "validate accuracy:\n",
      " [0.4227315  0.25122002 0.2396321  0.32252485 0.37645873 0.23104027\n",
      " 0.28987664 0.52088451 0.76753283 0.77470303 0.70399308 0.23403053\n",
      " 0.34217173 0.55972123 0.49526066 0.36736414 0.28728968] @epoch 61\n",
      "Epoch 062: val_acc did not improve from 0.437\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0269 - val_loss: 0.0563\n",
      "Counter: 2, Global: 0.43742070719599724, MyBest: 0.43742070719599724\n",
      "\n",
      "Epoch 63/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0269\n",
      "validate accuracy:\n",
      " [0.43619036 0.29641417 0.24180602 0.33996254 0.43034145 0.24228188\n",
      " 0.29795831 0.46813124 0.80064261 0.80670857 0.73466438 0.26215374\n",
      " 0.32407406 0.56780487 0.50864232 0.37059402 0.28686553] @epoch 62\n",
      "Epoch 063: val_acc did not improve from 0.437\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0269 - val_loss: 0.0298\n",
      "Counter: 3, Global: 0.43742070719599724, MyBest: 0.43742070719599724\n",
      "\n",
      "Epoch 64/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0269\n",
      "validate accuracy:\n",
      " [0.42955242 0.28325906 0.2381271  0.3840611  0.40671372 0.25755033\n",
      " 0.26882178 0.49819338 0.71626151 0.79874212 0.6808449  0.21481062\n",
      " 0.38103256 0.56864113 0.57276273 0.30964753 0.29336914] @epoch 63\n",
      "Epoch 064: val_acc did not improve from 0.437\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0269 - val_loss: 0.0279\n",
      "Counter: 4, Global: 0.43742070719599724, MyBest: 0.43742070719599724\n",
      "\n",
      "Epoch 65/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0269\n",
      "validate accuracy:\n",
      " [0.40973179 0.24846171 0.22658862 0.39818418 0.33467799 0.20436242\n",
      " 0.24351341 0.48952159 0.71486449 0.76519918 0.70341438 0.24618429\n",
      " 0.34147027 0.55066204 0.54641759 0.29405981 0.24812669] @epoch 64\n",
      "Epoch 065: val_acc did not improve from 0.437\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0269 - val_loss: 0.0460\n",
      "Counter: 5, Global: 0.43742070719599724, MyBest: 0.43742070719599724\n",
      "\n",
      "Epoch 66/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0267\n",
      "validate accuracy:\n",
      " [0.39766727 0.25588796 0.20936455 0.33880964 0.36709407 0.22298658\n",
      " 0.26733306 0.48663101 0.71542329 0.72578615 0.6646412  0.22117016\n",
      " 0.33978677 0.47777003 0.52522999 0.35205731 0.19270465] @epoch 65\n",
      "Epoch 066: val_acc did not improve from 0.437\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0267 - val_loss: 0.1983\n",
      "Counter: 6, Global: 0.43742070719599724, MyBest: 0.43742070719599724\n",
      "\n",
      "Epoch 67/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0267\n",
      "validate accuracy:\n",
      " [0.4124437  0.28177381 0.25083613 0.33001873 0.39936608 0.23406041\n",
      " 0.27754146 0.47304523 0.72659963 0.72354996 0.62905091 0.24533635\n",
      " 0.30218855 0.55888504 0.51547253 0.3714366  0.2799378 ] @epoch 66\n",
      "Epoch 067: val_acc did not improve from 0.437\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0267 - val_loss: 0.1259\n",
      "Counter: 7, Global: 0.43742070719599724, MyBest: 0.43742070719599724\n",
      "\n",
      "Epoch 067: Updating Learning rate.. New value is 0.000050\n",
      "Epoch 68/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0243\n",
      "validate accuracy:\n",
      " [0.45922855 0.29768726 0.2750836  0.40495747 0.4185276  0.26426175\n",
      " 0.29476818 0.53851712 0.80483377 0.80209643 0.73943865 0.27685133\n",
      " 0.3869248  0.58048779 0.57652634 0.3960118  0.29068288] @epoch 67\n",
      "Epoch 068: val_acc improved from 0.437 to 0.459\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0243 - val_loss: 0.0385\n",
      "Counter: 0, Global: 0.4592285491526127, MyBest: 0.43742070719599724\n",
      "\n",
      "Epoch 69/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0243\n",
      "validate accuracy:\n",
      " [0.46957971 0.30214301 0.27608696 0.42239517 0.42702779 0.26761746\n",
      " 0.30816674 0.54444283 0.81531155 0.81495458 0.7511574  0.2877332\n",
      " 0.39744669 0.60097563 0.59590185 0.40331414 0.29860032] @epoch 68\n",
      "Epoch 069: val_acc improved from 0.459 to 0.470\n",
      "576/576 [==============================] - 126s 219ms/step - loss: 0.0243 - val_loss: 0.0318\n",
      "Counter: 0, Global: 0.46957970783114433, MyBest: 0.4592285491526127\n",
      "\n",
      "Epoch 70/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0243\n",
      "validate accuracy:\n",
      " [0.46016677 0.25694886 0.27625418 0.42974493 0.37861979 0.26795301\n",
      " 0.27839217 0.55094665 0.80916458 0.81313765 0.7505787  0.28886378\n",
      " 0.39113355 0.58522648 0.59604126 0.39615223 0.29351053] @epoch 69\n",
      "Epoch 070: val_acc did not improve from 0.470\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0243 - val_loss: 0.0298\n",
      "Counter: 0, Global: 0.46957970783114433, MyBest: 0.46957970783114433\n",
      "\n",
      "Epoch 71/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0243\n",
      "validate accuracy:\n",
      " [0.46883005 0.30193081 0.27709031 0.43594179 0.42832446 0.27919462\n",
      " 0.29965973 0.52594304 0.80259848 0.82348007 0.76345485 0.25593555\n",
      " 0.40670595 0.59108013 0.61039865 0.39867997 0.30086243] @epoch 70\n",
      "Epoch 071: val_acc did not improve from 0.470\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0243 - val_loss: 0.0269\n",
      "Counter: 1, Global: 0.46957970783114433, MyBest: 0.46957970783114433\n",
      "\n",
      "Epoch 72/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0243\n",
      "validate accuracy:\n",
      " [0.47120135 0.30256736 0.28093645 0.41576597 0.43221438 0.26946309\n",
      " 0.30518928 0.55022401 0.81265718 0.8177498  0.74956596 0.29013568\n",
      " 0.40235689 0.60919863 0.59464735 0.40780789 0.2987417 ] @epoch 71\n",
      "Epoch 072: val_acc improved from 0.470 to 0.471\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0243 - val_loss: 0.0361\n",
      "Counter: 2, Global: 0.47120135091245174, MyBest: 0.46957970783114433\n",
      "\n",
      "Epoch 73/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0243\n",
      "validate accuracy:\n",
      " [0.46867254 0.30426481 0.27274248 0.42138636 0.42414638 0.26543623\n",
      " 0.30412591 0.53967339 0.80762786 0.81187981 0.75303817 0.28815717\n",
      " 0.3901515  0.60947734 0.59687763 0.41159949 0.29817617] @epoch 72\n",
      "Epoch 073: val_acc did not improve from 0.471\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0243 - val_loss: 0.0317\n",
      "Counter: 0, Global: 0.47120135091245174, MyBest: 0.47120135091245174\n",
      "\n",
      "Epoch 74/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0243\n",
      "validate accuracy:\n",
      " [0.4765928  0.31084237 0.28645486 0.43522123 0.44143495 0.26510066\n",
      " 0.31008083 0.56756759 0.82145852 0.82278126 0.75853586 0.27882984\n",
      " 0.40123457 0.61491287 0.60858655 0.40879089 0.29365191] @epoch 73\n",
      "Epoch 074: val_acc improved from 0.471 to 0.477\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0243 - val_loss: 0.0317\n",
      "Counter: 1, Global: 0.47659279592335224, MyBest: 0.47120135091245174\n",
      "\n",
      "Epoch 75/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0242\n",
      "validate accuracy:\n",
      " [0.47176235 0.3082962  0.2729097  0.41360426 0.41636652 0.26627517\n",
      " 0.30986813 0.53967339 0.81531155 0.8216632  0.75752312 0.29098362\n",
      " 0.40726712 0.61714286 0.60593814 0.41426766 0.29110703] @epoch 74\n",
      "Epoch 075: val_acc did not improve from 0.477\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0242 - val_loss: 0.0284\n",
      "Counter: 0, Global: 0.47659279592335224, MyBest: 0.47659279592335224\n",
      "\n",
      "Epoch 76/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0242\n",
      "validate accuracy:\n",
      " [0.47873372 0.31275195 0.28745818 0.43219483 0.43538395 0.27500001\n",
      " 0.31071883 0.55961841 0.82215703 0.82697415 0.7638889  0.28306952\n",
      " 0.40824917 0.6188153  0.60747141 0.41286337 0.30312455] @epoch 75\n",
      "Epoch 076: val_acc improved from 0.477 to 0.479\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0242 - val_loss: 0.0259\n",
      "Counter: 1, Global: 0.47873372212052345, MyBest: 0.47659279592335224\n",
      "\n",
      "Epoch 77/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0242\n",
      "validate accuracy:\n",
      " [0.46804427 0.29492891 0.27324414 0.41648653 0.42976516 0.26795301\n",
      " 0.30412591 0.5476225  0.80790722 0.81174004 0.74537039 0.27345958\n",
      " 0.40460157 0.60891986 0.59576249 0.40949306 0.29732788] @epoch 76\n",
      "Epoch 077: val_acc did not improve from 0.479\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0242 - val_loss: 0.0315\n",
      "Counter: 0, Global: 0.47873372212052345, MyBest: 0.47873372212052345\n",
      "\n",
      "Epoch 78/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0242\n",
      "validate accuracy:\n",
      " [0.47764132 0.30914491 0.27491638 0.42916846 0.44157901 0.27567115\n",
      " 0.30306253 0.5599075  0.81866443 0.81998605 0.75679976 0.28631994\n",
      " 0.40979236 0.61797911 0.61736828 0.41651455 0.30538669] @epoch 77\n",
      "Epoch 078: val_acc did not improve from 0.479\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0242 - val_loss: 0.0264\n",
      "Counter: 1, Global: 0.47873372212052345, MyBest: 0.47873372212052345\n",
      "\n",
      "Epoch 79/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0242\n",
      "validate accuracy:\n",
      " [0.47220066 0.30256736 0.27976587 0.42974493 0.43797722 0.26895973\n",
      " 0.30582732 0.55311459 0.81014252 0.81872815 0.75434029 0.26992652\n",
      " 0.40432099 0.62104529 0.60370785 0.39516923 0.29987276] @epoch 78\n",
      "Epoch 079: val_acc did not improve from 0.479\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0242 - val_loss: 0.0282\n",
      "Counter: 2, Global: 0.47873372212052345, MyBest: 0.47873372212052345\n",
      "\n",
      "Epoch 80/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0242\n",
      "validate accuracy:\n",
      " [0.47828711 0.30999362 0.28946489 0.42225105 0.40613744 0.27701342\n",
      " 0.30986813 0.56771207 0.82243645 0.83004892 0.76258683 0.2952233\n",
      " 0.40923122 0.61324042 0.61695009 0.42409775 0.2963382 ] @epoch 79\n",
      "Epoch 080: val_acc did not improve from 0.479\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0242 - val_loss: 0.0274\n",
      "Counter: 3, Global: 0.47873372212052345, MyBest: 0.47873372212052345\n",
      "\n",
      "Epoch 81/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0242\n",
      "validate accuracy:\n",
      " [0.4641782  0.29132187 0.26672241 0.42052168 0.40901887 0.27701342\n",
      " 0.29200339 0.52334154 0.79267955 0.82725364 0.75260419 0.25833803\n",
      " 0.41035354 0.60306621 0.60886532 0.39839911 0.2953485 ] @epoch 80\n",
      "Epoch 081: val_acc did not improve from 0.479\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0242 - val_loss: 0.0294\n",
      "Counter: 4, Global: 0.47873372212052345, MyBest: 0.47873372212052345\n",
      "\n",
      "Epoch 82/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0242\n",
      "validate accuracy:\n",
      " [0.47790881 0.30214301 0.2832776  0.43147427 0.44143495 0.27684563\n",
      " 0.31603572 0.56221998 0.81503212 0.82236201 0.75622106 0.28575465\n",
      " 0.41049382 0.6163066  0.60914415 0.41735712 0.30043828] @epoch 81\n",
      "Epoch 082: val_acc did not improve from 0.479\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0242 - val_loss: 0.0273\n",
      "Counter: 5, Global: 0.47873372212052345, MyBest: 0.47873372212052345\n",
      "\n",
      "Epoch 83/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0241\n",
      "validate accuracy:\n",
      " [0.47553577 0.30362827 0.26923078 0.43017727 0.44057053 0.2738255\n",
      " 0.31050617 0.5544154  0.81112045 0.81984627 0.75405091 0.28787452\n",
      " 0.40909091 0.61839724 0.60900474 0.41328466 0.30354869] @epoch 82\n",
      "Epoch 083: val_acc did not improve from 0.479\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0241 - val_loss: 0.0293\n",
      "Counter: 6, Global: 0.47873372212052345, MyBest: 0.47873372212052345\n",
      "\n",
      "Epoch 84/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0241\n",
      "validate accuracy:\n",
      " [0.48137092 0.30575004 0.29163879 0.43291542 0.44604525 0.27550337\n",
      " 0.31008083 0.56250906 0.81796592 0.82627535 0.75882524 0.29324478\n",
      " 0.41610551 0.62257838 0.61708951 0.42072743 0.30467978] @epoch 83\n",
      "Epoch 084: val_acc improved from 0.479 to 0.481\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0241 - val_loss: 0.0277\n",
      "Counter: 7, Global: 0.48137091659009457, MyBest: 0.47873372212052345\n",
      "\n",
      "Epoch 084: Updating Learning rate.. New value is 0.000010\n",
      "Epoch 85/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.4794863  0.30638659 0.28846154 0.43147427 0.44258752 0.27348992\n",
      " 0.30837941 0.56265354 0.81195867 0.82278126 0.7572338  0.29154891\n",
      " 0.41372055 0.62550521 0.61513799 0.42030615 0.30015552] @epoch 84\n",
      "Epoch 085: val_acc did not improve from 0.481\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0240 - val_loss: 0.0274\n",
      "Counter: 0, Global: 0.48137091659009457, MyBest: 0.47873372212052345\n",
      "\n",
      "Epoch 86/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.47370322 0.29280713 0.28812709 0.42988902 0.42933294 0.27298659\n",
      " 0.30284986 0.55701691 0.80678958 0.82082462 0.7505787  0.27388355\n",
      " 0.40993267 0.6144948  0.61011988 0.41960397 0.30001414] @epoch 85\n",
      "Epoch 086: val_acc did not improve from 0.481\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0240 - val_loss: 0.0285\n",
      "Counter: 0, Global: 0.48137091659009457, MyBest: 0.48137091659009457\n",
      "\n",
      "Epoch 87/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.47795259 0.30744749 0.28628764 0.42830378 0.44100273 0.277349\n",
      " 0.30646533 0.5599075  0.80804694 0.82096434 0.75448495 0.28872246\n",
      " 0.41077441 0.6220209  0.61499858 0.41974443 0.30072105] @epoch 86\n",
      "Epoch 087: val_acc did not improve from 0.481\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0240 - val_loss: 0.0272\n",
      "Counter: 1, Global: 0.48137091659009457, MyBest: 0.48137091659009457\n",
      "\n",
      "Epoch 88/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48023653 0.30935711 0.28511706 0.43334773 0.44474858 0.27516779\n",
      " 0.3075287  0.56380981 0.81545126 0.82445842 0.7589699  0.29197288\n",
      " 0.41315937 0.6252265  0.61402285 0.42100829 0.30043828] @epoch 87\n",
      "Epoch 088: val_acc did not improve from 0.481\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0240 - val_loss: 0.0271\n",
      "Counter: 2, Global: 0.48137091659009457, MyBest: 0.48137091659009457\n",
      "\n",
      "Epoch 89/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48242685 0.31063017 0.28779265 0.43320364 0.44791818 0.27902684\n",
      " 0.3111442  0.56525511 0.81754678 0.82921034 0.76316553 0.29253817\n",
      " 0.41526374 0.62634146 0.61931974 0.41862097 0.30185211] @epoch 88\n",
      "Epoch 089: val_acc improved from 0.481 to 0.482\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0240 - val_loss: 0.0262\n",
      "Counter: 3, Global: 0.48242685198783875, MyBest: 0.48137091659009457\n",
      "\n",
      "Epoch 90/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48124095 0.30935711 0.28561872 0.43262717 0.44633338 0.27583891\n",
      " 0.31390896 0.56337619 0.81377482 0.82739341 0.75766784 0.29282081\n",
      " 0.41470259 0.62466902 0.61820465 0.42185086 0.30171072] @epoch 89\n",
      "Epoch 090: val_acc did not improve from 0.482\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0267\n",
      "Counter: 0, Global: 0.48242685198783875, MyBest: 0.48242685198783875\n",
      "\n",
      "Epoch 91/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48164811 0.31063017 0.28762543 0.43406832 0.44777408 0.27802014\n",
      " 0.31284559 0.56698942 0.81545126 0.82669461 0.7578125  0.29013568\n",
      " 0.41456228 0.62439024 0.61862278 0.41988486 0.30086243] @epoch 90\n",
      "Epoch 091: val_acc did not improve from 0.482\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0240 - val_loss: 0.0267\n",
      "Counter: 1, Global: 0.48242685198783875, MyBest: 0.48242685198783875\n",
      "\n",
      "Epoch 92/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.47872982 0.3023552  0.28143811 0.43320364 0.44532487 0.27751678\n",
      " 0.31220758 0.56511056 0.80986309 0.82026553 0.75405091 0.2842001\n",
      " 0.41442201 0.62188154 0.61639255 0.42128915 0.30015552] @epoch 91\n",
      "Epoch 092: val_acc did not improve from 0.482\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0240 - val_loss: 0.0287\n",
      "Counter: 2, Global: 0.48242685198783875, MyBest: 0.48242685198783875\n",
      "\n",
      "Epoch 93/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48097636 0.31020582 0.28963211 0.43133017 0.44561303 0.27634227\n",
      " 0.3111442  0.56597775 0.81377482 0.82361984 0.75752312 0.29310346\n",
      " 0.41554433 0.62397212 0.61695009 0.41988486 0.30100381] @epoch 92\n",
      "Epoch 093: val_acc did not improve from 0.482\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0240 - val_loss: 0.0274\n",
      "Counter: 3, Global: 0.48242685198783875, MyBest: 0.48242685198783875\n",
      "\n",
      "Epoch 94/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48319766 0.30914491 0.28879598 0.43161839 0.44820631 0.2817114\n",
      " 0.314547   0.56583321 0.8171277  0.82711393 0.76157409 0.2952233\n",
      " 0.41708755 0.62634146 0.61904097 0.42283386 0.30496255] @epoch 93\n",
      "Epoch 094: val_acc improved from 0.482 to 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0267\n",
      "Counter: 4, Global: 0.4831976629793644, MyBest: 0.48242685198783875\n",
      "\n",
      "Epoch 95/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48149096 0.31020582 0.28795987 0.43089783 0.44719782 0.27667785\n",
      " 0.31242025 0.56438792 0.81307626 0.8259958  0.76027197 0.28957039\n",
      " 0.41610551 0.62689894 0.6187622  0.42086786 0.30255902] @epoch 94\n",
      "Epoch 095: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 216ms/step - loss: 0.0240 - val_loss: 0.0270\n",
      "Counter: 0, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 96/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48240296 0.30893275 0.29080269 0.43032137 0.44633338 0.27751678\n",
      " 0.31390896 0.56511056 0.81601006 0.82473797 0.76142937 0.29253817\n",
      " 0.41820988 0.62592334 0.62071371 0.42283386 0.30312455] @epoch 95\n",
      "Epoch 096: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0268\n",
      "Counter: 1, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 97/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.46616984 0.30447698 0.27190635 0.41922468 0.41377324 0.26661074\n",
      " 0.29561889 0.53129065 0.79226041 0.81900769 0.75448495 0.25904465\n",
      " 0.41217732 0.6220209  0.60064119 0.39432663 0.30185211] @epoch 96\n",
      "Epoch 097: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0299\n",
      "Counter: 2, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 98/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48109416 0.31084237 0.28729096 0.42700678 0.44532487 0.27701342\n",
      " 0.31305829 0.56352073 0.81391448 0.82501745 0.75983799 0.29267949\n",
      " 0.4159652  0.62578398 0.61597437 0.42072743 0.30354869] @epoch 97\n",
      "Epoch 098: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0278\n",
      "Counter: 3, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 99/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.4808777  0.30893275 0.29013377 0.43190661 0.44964704 0.27768457\n",
      " 0.3075287  0.56221998 0.81042188 0.82431865 0.75969326 0.28759187\n",
      " 0.41526374 0.6252265  0.61722887 0.42227215 0.30397284] @epoch 98\n",
      "Epoch 099: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0273\n",
      "Counter: 4, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 100/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48218025 0.30405262 0.29280937 0.43147427 0.44446045 0.28020135\n",
      " 0.31050617 0.56221998 0.81545126 0.82753319 0.76099539 0.29409271\n",
      " 0.41736811 0.6270383  0.6205743  0.42185086 0.30425563] @epoch 99\n",
      "Epoch 100: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0240 - val_loss: 0.0265\n",
      "Counter: 5, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 101/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48220919 0.31041798 0.29096991 0.43305951 0.44690967 0.2785235\n",
      " 0.31220758 0.56496602 0.81628948 0.82487768 0.76012731 0.28914642\n",
      " 0.41386083 0.6277352  0.62001675 0.42311472 0.30312455] @epoch 100\n",
      "Epoch 101: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 126s 218ms/step - loss: 0.0240 - val_loss: 0.0274\n",
      "Counter: 6, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 102/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.47186244 0.31169108 0.2742475  0.41663063 0.41506988 0.26191276\n",
      " 0.31412166 0.53952885 0.81307626 0.82292104 0.76229745 0.26243639\n",
      " 0.41372055 0.62397212 0.61681068 0.40134811 0.30001414] @epoch 101\n",
      "Epoch 102: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0299\n",
      "Counter: 7, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 102: Updating Learning rate.. New value is 0.000002\n",
      "Epoch 103/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48213784 0.31211543 0.29063544 0.43176252 0.44647744 0.28036913\n",
      " 0.31242025 0.56193089 0.81628948 0.82389939 0.76085067 0.29282081\n",
      " 0.41470259 0.6270383  0.6187622  0.42128915 0.30284178] @epoch 102\n",
      "Epoch 103: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0267\n",
      "Counter: 0, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 104/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.4825755  0.31147888 0.29163879 0.43349186 0.44719782 0.2817114\n",
      " 0.31305829 0.56380981 0.81559092 0.82459819 0.76142937 0.2933861\n",
      " 0.41470259 0.62564462 0.61834401 0.420587   0.3045384 ] @epoch 103\n",
      "Epoch 104: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0268\n",
      "Counter: 1, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 105/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48279429 0.31169108 0.2889632  0.43334773 0.44763002 0.28271812\n",
      " 0.30965546 0.56511056 0.81614977 0.82739341 0.76359951 0.29225552\n",
      " 0.41624579 0.62592334 0.6187622  0.42114872 0.30411422] @epoch 104\n",
      "Epoch 105: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0240 - val_loss: 0.0265\n",
      "Counter: 2, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 106/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48231015 0.30978146 0.29180601 0.43435654 0.44618931 0.28087249\n",
      " 0.31305829 0.56424338 0.8164292  0.82557654 0.75969326 0.29423404\n",
      " 0.41512346 0.62480837 0.61792582 0.42044657 0.30241764] @epoch 105\n",
      "Epoch 106: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0266\n",
      "Counter: 3, Global: 0.4831976629793644, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 107/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48342817 0.31084237 0.29180601 0.43248308 0.44892666 0.28154361\n",
      " 0.31284559 0.56554413 0.81796592 0.82711393 0.76128471 0.29607123\n",
      " 0.41554433 0.62689894 0.61959857 0.42297429 0.30340731] @epoch 106\n",
      "Epoch 107: val_acc improved from 0.483 to 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0263\n",
      "Counter: 4, Global: 0.4834281671792269, MyBest: 0.4831976629793644\n",
      "\n",
      "Epoch 108/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48230847 0.30765966 0.29113713 0.43262717 0.44719782 0.28087249\n",
      " 0.31008083 0.56395435 0.8164292  0.82767296 0.76128471 0.29282081\n",
      " 0.41358024 0.62606269 0.62043488 0.42128915 0.30383146] @epoch 107\n",
      "Epoch 108: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0265\n",
      "Counter: 0, Global: 0.4834281671792269, MyBest: 0.4834281671792269\n",
      "\n",
      "Epoch 109/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0239\n",
      "validate accuracy:\n",
      " [0.48122329 0.30978146 0.2889632  0.43075371 0.44575709 0.27986577\n",
      " 0.31178221 0.56221998 0.81391448 0.82306081 0.75868058 0.29310346\n",
      " 0.41568461 0.62592334 0.61834401 0.41974443 0.30199349] @epoch 108\n",
      "Epoch 109: val_acc did not improve from 0.483\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0239 - val_loss: 0.0270\n",
      "Counter: 1, Global: 0.4834281671792269, MyBest: 0.4834281671792269\n",
      "\n",
      "Epoch 110/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0239\n",
      "validate accuracy:\n",
      " [0.48386707 0.31063017 0.29180601 0.43435654 0.45065552 0.28238255\n",
      " 0.31284559 0.56438792 0.81782621 0.8279525  0.76273149 0.29423404\n",
      " 0.41540405 0.6270383  0.62210763 0.42269343 0.30482116] @epoch 109\n",
      "Epoch 110: val_acc improved from 0.483 to 0.484\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0239 - val_loss: 0.0261\n",
      "Counter: 2, Global: 0.48386706970632076, MyBest: 0.4834281671792269\n",
      "\n",
      "Epoch 111/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0239\n",
      "validate accuracy:\n",
      " [0.48272346 0.31041798 0.29214045 0.4350771  0.44734189 0.28020135\n",
      " 0.31284559 0.56438792 0.81559092 0.82571626 0.76085067 0.2933861\n",
      " 0.41540405 0.62620211 0.62001675 0.42030615 0.30369008] @epoch 110\n",
      "Epoch 111: val_acc did not improve from 0.484\n",
      "576/576 [==============================] - 125s 218ms/step - loss: 0.0239 - val_loss: 0.0267\n",
      "Counter: 0, Global: 0.48386706970632076, MyBest: 0.48386706970632076\n",
      "\n",
      "Epoch 112/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48349946 0.31317633 0.29063544 0.43305951 0.44878259 0.27936241\n",
      " 0.31220758 0.56482148 0.81782621 0.82781273 0.76157409 0.29578859\n",
      " 0.41610551 0.62731707 0.62155002 0.42086786 0.30510393] @epoch 111\n",
      "Epoch 112: val_acc did not improve from 0.484\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0265\n",
      "Counter: 1, Global: 0.48386706970632076, MyBest: 0.48386706970632076\n",
      "\n",
      "Epoch 113/151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - ETA: 0s - loss: 0.0239\n",
      "validate accuracy:\n",
      " [0.48196284 0.30893275 0.29046822 0.43378007 0.4467656  0.27969798\n",
      " 0.31135687 0.56323171 0.81545126 0.82389939 0.75868058 0.2933861\n",
      " 0.41652638 0.62689894 0.61862278 0.42128915 0.30241764] @epoch 112\n",
      "Epoch 113: val_acc did not improve from 0.484\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0239 - val_loss: 0.0269\n",
      "Counter: 2, Global: 0.48386706970632076, MyBest: 0.48386706970632076\n",
      "\n",
      "Epoch 114/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0240\n",
      "validate accuracy:\n",
      " [0.48071845 0.31084237 0.28963211 0.43161839 0.44460452 0.277349\n",
      " 0.3109315  0.56279808 0.81279689 0.82236201 0.75752312 0.29352742\n",
      " 0.41400114 0.62411147 0.61639255 0.420587   0.30241764] @epoch 113\n",
      "Epoch 114: val_acc did not improve from 0.484\n",
      "576/576 [==============================] - 125s 217ms/step - loss: 0.0240 - val_loss: 0.0273\n",
      "Counter: 3, Global: 0.48386706970632076, MyBest: 0.48386706970632076\n",
      "\n",
      "Epoch 115/151\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.0239\n",
      "validate accuracy:\n",
      " [0.48180623 0.31084237 0.29046822 0.43334773 0.44705373 0.27869126\n",
      " 0.31327096 0.56366527 0.81447333 0.82417887 0.75810188 0.29381007\n",
      " 0.41470259 0.62675959 0.61639255 0.42128915 0.30185211] @epoch 114\n",
      "Epoch 115: val_acc did not improve from 0.484\n",
      "576/576 [==============================] - 127s 221ms/step - loss: 0.0239 - val_loss: 0.0272\n",
      "Counter: 4, Global: 0.48386706970632076, MyBest: 0.48386706970632076\n",
      "\n",
      "Epoch 116/151\n",
      "414/576 [====================>.........] - ETA: 27s - loss: 0.0239"
     ]
    }
   ],
   "source": [
    "\n",
    "net.compile(optimizer=optimizer, loss=weighted_mse_loss, metrics=[])\n",
    "#checkpointer = ModelCheckpoint(filepath=f'../cunet{nUNet}_{m}{n}_best.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True)\n",
    "history = net.fit(train_images,train_hms,validation_data=(eval_images,eval_hms2),epochs=151, batch_size=32,shuffle=True,verbose=1, callbacks=[evalcallback,updateLR])#,use_multiprocessing=True,workers=20)#[checkpointer,updateLR])\n",
    "#np.savez_compressed(\"../history\",history)\n",
    "net.save_weights(f'../cunet{nUNet}_{m}{n}_last_sigmoidAdamQuasiDef3sx12_restartsig_henormal.h5', overwrite=True)\n",
    "np.save('../history_sigmoidAdamQuasiDef3sx12_restartsig_henormal.npy',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "net.load_weights(\"../cunet2_6416_last_sigmoidAdamQuasiDef3sx12.h5\")\n",
    "output = net.predict(train_images)\n",
    "output = np.transpose(output,(0,3,1,2))\n",
    "print(accuracy(output,np.transpose(train_hms,(0,3,1,2))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ho aumentato solo patience \n",
    "#Provato con dropout ma andato male"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
